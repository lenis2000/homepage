\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}
\usepackage{comment}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{example}[proposition]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newenvironment{lnotes}{\section*{Notes for the lecturer}}{}
% \excludecomment{lnotes}


\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 3: Gaussian and tridiagonal matrices}


\date{Wednesday, January 22, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l03.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}

\author{Leonid Petrov}


\maketitle


\tableofcontents




\section{Recap}

We have established the semicircle law for
real Wigner random matrices.
If $W$ is an $n\times n$ real symmetric matrix with
independent entries $X_{ij}$ above the main diagonal
(mean zero, variance~$1$), and mean zero diagonal entries,
then the empirical spectral distribution of $W/\sqrt{n}$
converges to the semicircle law as $n\to\infty$:
\begin{equation}
	\label{eq:semicircle_conv}
	\lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i/\sqrt n} =
	\mu_{\mathrm{sc}},
\end{equation}
where
\begin{equation*}
	\mu_{\mathrm{sc}}(dx) = \begin{cases}
		\frac{1}{2\pi} \sqrt{4-x^2} \, dx, & \text{if } |x|\le 2, \\
		0, & \text{otherwise}.
	\end{cases}
\end{equation*}
The convergence
in \eqref{eq:semicircle_conv} is weakly almost sure.
The way we got the result is by expanding
$\operatorname{\mathbb{E}}\operatorname{Tr} (W^k)$ and counting
trees, plus analytic lemmas which ensure that
the convergence of expected powers of traces is enough
to conclude the convergence
\eqref{eq:semicircle_conv}
of the empirical spectral measures.

\medskip

Today, we are going to focus on Gaussian ensembles. The plan is:
\begin{enumerate}[$\bullet$]
	\item Definition and spectral density for real symmetric Gaussian matrices (GOE).
	\item Other random matrix ensembles with explicit eigenvalue densities:
		Wishart (Laguerre) and Jacobi (MANOVA/CCA) ensembles.
	\item Tridiagonalization and general beta ensemble.
	\item (next week, not today) Wigner's semicircle law via tridiagonalization.
\end{enumerate}

\section{Gaussian ensembles}

\subsection{Definitions}
\label{sub:GOE_GUE_definitons}

Recall that a real Wigner matrix $W$ can be modeled as
\begin{equation*}
	W=\frac{Y+Y^\top}{\sqrt{2}},
\end{equation*}
where $Y$ is an $n\times n$ matrix with independent entries $Y_{ij}$,
$1\le i,j\le n$, such that $Y_{ij}$ are mean zero, variance~$1$.
Then for $1\le i<j\le n$, we have for the matrix
$W=(X_{ij})$:
\begin{equation*}
	\operatorname{\mathrm{Var}}\left( X_{ii} \right)=
	\operatorname{\mathrm{Var}}( \sqrt 2\ssp Y_{ii} )=2,\qquad
	\operatorname{\mathrm{Var}}\left( X_{ij} \right)=
	\operatorname{\mathrm{Var}}\left( \frac{Y_{ij}+Y_{ji}}{\sqrt 2} \right)=1.
\end{equation*}

If, in addition, we assume that $Y_{ij}$ are standard Gaussian
$\mathcal{N}(0,1)$, then the distribution of $W$ is called
the \emph{Gaussian Orthogonal Ensemble} (GOE).

For the complex case, we
have the \emph{standard complex Gaussian random variable}
\begin{equation*}
	Z=\frac{1}{\sqrt 2}\left( Z^R+\mathbf{i}\ssp Z^I \right),
	\qquad
	\operatorname{\mathbb{E}} (Z)=0,
	\qquad
	\operatorname{\mathrm{Var}}_{\mathbb{C}}(Z)\coloneqq
	\operatorname{\mathbb{E}} (|Z|^2)=
	\frac{
	\operatorname{\mathbb{E}} (|Z^R|^2)+
	\operatorname{\mathbb{E}} (|Z^I|^2)}{2}=1
	,
\end{equation*}
where $Z^R$ and $Z^I$ are independent
standard Gaussian real random variables $\mathcal{N}(0,1)$.

If we take $Y$ to be an $n\times n$ matrix with independent
entries $Y_{ij}$, $1\le i,j\le n$
distributed as $Z$, then the random matrix\footnote{$Y^\dagger$ denotes the transpose of $Y$ combined with complex conjugation.}
\begin{equation*}
	W=\frac{Y+Y^\dagger}{\sqrt 2}
\end{equation*}
is said to have the \emph{Gaussian Unitary Ensemble} (GUE) distribution.
For the GUE matrix $W=(X_{ij})$,
we have for $1\le i<j\le n$:
\begin{equation*}
	\operatorname{\mathrm{Var}}_{\mathbb{C}}(X_{ii})=2,
	\qquad
	\operatorname{\mathrm{Var}}_{\mathbb{C}}(X_{ij})
	=\frac{1}{4}
	\Bigl[ \operatorname{\mathbb{E}}(Z_{ij}^R
			+
		Z_{ji}^R)^2
		+
		\operatorname{\mathbb{E}}(Z_{ij}^I
			+
		Z_{ji}^I)^2
	\Bigr]
	=1.
\end{equation*}

Both GOE and GUE have real eigenvalues $\lambda_1 \ge \ldots \ge \lambda_n$.
We are going to describe the joint distribution of these eigenvalues.
Despite the fact that the map from a matrix to its eigenvalues
is quite complicated and nonlinear (you need to solve an equation of degree $n$),
the distribution of eigenvalues in the Gaussian cases is fully explicit.

See Problem \ref{prob:invariance_GOE_GUE}
for invariance of GOE/GUE under orthogonal/unitary conjugation
(this is where the names ``orthogonal'' and ``unitary'' come from).

\begin{remark}
	There is a third player in the game, the \emph{Gaussian
	Symplectic Ensemble} (GSE),which we will mainly ignore in
	this course
	due to its less intuitive quaternionic nature.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Joint eigenvalue distribution for GOE}
\label{sub:GOE-derivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we give a derivation of the joint probability density for the GOE.

\begin{theorem}[GOE Joint Eigenvalue Density]
\label{thm:GOE-joint-eigs-detailed}
Let \(W\) be an \(n\times n\) real symmetric matrix with
the GOE distribution (\Cref{sub:GOE_GUE_definitons}).
Then its ordered real eigenvalues \(\lambda_1 \le \cdots \le
\lambda_n\)
of $W/\sqrt 2$
have a joint probability density function
on $\mathbb{R}^n$
given by:
\[
  p(\lambda_1,\ldots,\lambda_n)
  \;=\;\frac{1}{Z_{n}}
  \,\prod_{1 \le i < j \le n}
  \!\!\bigl|\lambda_i - \lambda_j\bigr|\,
  \exp\Bigl(
    -\frac{1}{2}\sum_{k=1}^n \lambda_k^2
  \Bigr),
\]
where \(Z_{n}\) is a constant (depending on \(n\) but not on \(\lambda_i\)) ensuring the density integrates to 1:
\begin{equation*}
	Z_n=Z_n^{GOE}=\frac{(2\pi)^{n/2}}{n!}
	\prod_{j=0}^{n-1}\frac{\Gamma(1+(j+1)\beta/2)}{\Gamma(1+\beta/2)}, \qquad
	\beta=1.
\end{equation*}
\end{theorem}
\begin{remark}
	We renormalized the GOE by a factor of $\sqrt 2$ to make the
	Gaussian part of the density, $\exp(-\frac{1}{2}\sum_{k=1}^n \lambda_k^2)$,
	standard. In the GUE case, no normalization is required.
\end{remark}

We break the proof into four major steps,
considered in
\Cref{subsec:density-entries,subsec:spectral,subsec:jacobian,subsec:final-form}
below.

\subsection{Step A. Joint density of matrix entries}
\label{subsec:density-entries}

Let us label all independent entries of \(W/\sqrt 2\):
\[
	\{\underbrace{X_{12}, X_{13},\dots, X_{23},\ldots }_{\text{above diag}},
	\underbrace{X_{22}, X_{33},\dots}_{\text{diag}}\}.
\]
There are \(\frac{n(n-1)}{2}\) off-diagonal entries
with variance $1/2$,
and \(n\) diagonal entries with variance $1$.
The joint density of these entries (ignoring normalization for a moment) is
proportional to
\begin{equation}
	\label{eq:GOE-density-entries-first}
	f(x_{12},x_{13},\ldots,x_{22},x_{33},\ldots )
  \propto\exp\Bigl(
    - \sum_{i<j} x_{ij}^2
    -\frac{1}{2} \sum_{i=1}^n x_{ii}^2
  \Bigr)=
	\exp
	\Bigl( -\frac{1}{2}\sum_{i,j=1}^n x_{ij}^2 \Bigr),
\end{equation}
where in the right-hand side, we have
$x_{ij}=x_{ji}$ for $i\ne j$.
We then recognize
\[
	\sum_{i,j=1}^n x_{ij}^2=\operatorname{Tr}(W^2)=\sum_{k=1}^n \lambda_k^2.
\]
Including the normalization for Gaussians, one arrives at
the density on $\mathbb{R}^{n(n+1)/2}$:
\[
  f(W)\,dW
  \;=\;
  \pi^{-\tfrac{n(n-1)}{4}}
  \,\bigl(2\pi\bigr)^{-\tfrac{n}{4}}
  \,\exp\Bigl(-\tfrac{1}{2}\operatorname{Tr}(W^2)\Bigr)\; dW,
\]
where \(dW\) is the product measure over the \(\tfrac{n(n+1)}{2}\) independent entries.

\subsection{Step B. Spectral decomposition}
\label{subsec:spectral}

Since \(W\) is real symmetric, it can be orthogonally diagonalized:
\[
  W = Q\,\Lambda\,Q^\top,\quad
  Q \in O(n),
\]
where \(\Lambda = \mathrm{diag}(\lambda_1,\ldots,\lambda_n)\) has the eigenvalues.  Then, as we saw before, we have
\[
  \operatorname{Tr}(W^2)
  = \operatorname{Tr}\bigl(Q\,\Lambda\,Q^\top Q\,\Lambda\,Q^\top\bigr)
  = \operatorname{Tr}(\Lambda^2)
  = \sum_{k=1}^n \lambda_k^2.
\]
The map
from $W$ to $(\Lambda,Q)$ is not one-to one,
but in case $W$ has distinct eigenvalues,
the preimage of $(\Lambda,Q)$
contains $2^n$ elements.
See Problems~\ref{prob:GOE-preimage} and \ref{prob:distinct-eigenvalues}.
\medskip

It remains to make the change of variables from \(W\) to \(\Lambda\), which involves the Jacobian.

\subsection{Step C. Jacobian}
\label{subsec:jacobian}

We now examine how the measure \(dW\) in the space of real symmetric matrices factors into a piece depending on \(\{\lambda_i\}\) and a piece depending on \(Q\).  Formally,
\[
  dW
  = \Bigl|\det\bigl(\tfrac{\partial W}{\partial(\Lambda,Q)}\bigr)\Bigr|
    \,d\Lambda\,dQ,
\]
where \(dQ\) is the Haar measure\footnote{Recall that the
	Haar measure on \(O(n)\) is the unique
	(up to a constant factor) measure that is invariant under
	group shifts (in this situation, both left and right shifts
	work). In probabilistic terms,
	if a random orthogonal matrix $Q$ is Haar-distributed,
	then $QR$ and $RQ$ are also Haar-distributed for any fixed orthogonal
matrix $R$.}
on \(O(n)\), and
\(d\Lambda\) is the Lebesgue measure on \(\mathbb{R}^n\).
The Lebesgue measure later needs to be restricted
to the ``Weyl chamber''
\(\lambda_1\le \cdots\le \lambda_n\) if we want an ordering,
this introduces the simple factor \(n!\) in the final density.


\begin{lemma}[Jacobian for Spectral Decomposition]
\label{lemma:Jacobian-GOE}
For real symmetric \(W=Q\Lambda Q^\top\), one has
\[
  \bigl|\det\bigl(\tfrac{\partial W}{\partial(\Lambda,Q)}\bigr)\bigr|
  \;=\;
	\mathrm{const}
  \prod_{1\le i<j\le n}
	\!\!
	\bigl|\lambda_i - \lambda_j\bigr|,
\]
where the constant is independent of the \(\lambda_i\)'s and
depends only on \(n\).
\end{lemma}

\begin{remark}
Equivalently, one often writes
\[
  dW
  \;=\;
  \bigl|\Delta(\lambda_1,\dots,\lambda_n)\bigr|\;
  d\Lambda\,dQ,
  \quad\text{where }
  \Delta(\lambda_1,\dots,\lambda_n)
  = \prod_{i<j}(\lambda_j-\lambda_i)
\]
is the \emph{Vandermonde determinant}.
\end{remark}

We prove \Cref{lemma:Jacobian-GOE} in the rest of this subsection.
\medskip

Consider small perturbations of \(\Lambda\) and \(Q\).  Write
\[
  W = Q\,\Lambda\,Q^\top,
  \quad
  \Lambda = \mathrm{diag}(\lambda_1,\dots,\lambda_n).
\]
Let \(\delta W\) be an infinitesimal change in \(W\). We want to see how \(\delta W\) depends on \(\delta\Lambda\) and \(\delta Q\).

\paragraph{Parametrizing \(\delta Q\).}
Since \( Q \in O(n) \), any small variation of \( Q \) can be expressed as
\[ Q \exp(B) \approx Q(I + B), \] where \( B \) is an infinitesimal
skew-symmetric matrix (\( B^\top = -B \)). Indeed,
$\exp(B)$ must be orthogonal, so $\exp(B)^\top \exp(B) = I$.
Thus, we have
\begin{equation*}
	(I+B)^\top(I+B)=I,\qquad \text{or}\qquad B^\top + B = 0.
\end{equation*}
Note that $\exp(B)$ is the matrix exponential of $B$,
which is defined by the usual power series.
Note also that the dimension of \( O(n) \)
is \( \dim(O(n)) = \frac{n(n-1)}{2} \), which matches the dimension of the
space of skew-symmetric matrices.

\paragraph{Computing \(\delta W\).}
Under an infinitesimal change, say,
\[
  Q \mapsto Q\,(I + B),
  \quad
  \Lambda \mapsto \Lambda + \delta\Lambda,
\]
we have
\[
  W
  = Q\ssp\Lambda\ssp Q^\top
  \;\;\Longrightarrow\;\;
  Q^\top \delta W Q
	=
	\delta\Lambda
	+
	B\Lambda-\Lambda B,
\]
to first order in small quantities.
Here we used the orthogonality of $Q$ and
the skew-symmetry of~$B$.

\paragraph{Local structure of the map.}

We see that the map
$W\mapsto(\Lambda,Q)$
in a neighborhood of $(\Lambda,Q)$ determined by
$\delta \Lambda$ and $B$
locally translates by $Q^\top\ssp\delta \Lambda\ssp Q$,
which implies the Lebesgue factor
$d\lambda_1 \ldots d\lambda_n $
in $\delta W$. Indeed, the Lebesgue measure
on $\mathbb{R}^n$ is invariant under orthogonal transformations.

The next terms, the commutator $[B,\Lambda]$, has the form
(recall that $B$ is infinitesimally small and $\Lambda$ is diagonal):
\begin{align*}
B\Lambda-\Lambda B&=
\begin{pmatrix}
	0 & b_{12} & \cdots \\
	-b_{12} & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
	\lambda_1 & 0 & \cdots \\
	0 & \lambda_2 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}-
\begin{pmatrix}
	\lambda_1 & 0 & \cdots \\
	0 & \lambda_2 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
	0 & b_{12} & \cdots \\
	-b_{12} & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}\\&
=
\begin{pmatrix}
	0 & b_{12}\lambda_2 & \cdots \\
	-b_{12}\lambda_1 & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
-
\begin{pmatrix}
	0 & b_{12}\lambda_1 & \cdots \\
	b_{12}\lambda_2 & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
\\&=
\begin{pmatrix}
	0 &b_{12}(\lambda_2-\lambda_1) & \cdots \\
	b_{12}(\lambda_1-\lambda_2) & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}.
\end{align*}
Thus, this action locally means that the
infinitesimal $b_{ij}$ is multiplied by $\lambda_i-\lambda_j$,
for all $1\le i<j\le n$.
This is a scalar factor that does not depend on
the orthogonal component $Q$, but only on the eigenvalues.
Therefore, this factor is the same in
$Q^\top \ssp \delta W \ssp Q$.

This completes the proof of
\Cref{lemma:Jacobian-GOE}.
See also Problem~\ref{prob:Jacobian-GUE} for the GUE Jacobian.

\subsection{Step D. Final Form of the density}
\label{subsec:final-form}

Putting Steps A--C together, we find:
\[
  dW
  \;=\;
	\mathrm{const}\cdot
  \prod_{i<j}|\lambda_i-\lambda_j|\ssp
  d\Lambda
  \,\bigl(\underbrace{\text{Haar measure on }O(n)}_{\text{does not depend on }\lambda_i}\bigr).
\]
Hence, the joint density of \(\{\lambda_1,\dots,\lambda_n\}\) is,
up to normalization depending only on \(n\), equal to
\begin{equation}
	\label{eq:GOE_proportional}
  \prod_{i<j}|\lambda_i-\lambda_j|\;
  \exp\Bigl(-\tfrac{1}{2}\sum_{k=1}^n \lambda_k^2\Bigr).
\end{equation}
We leave the computation of the normalization constant in
\Cref{thm:GOE-joint-eigs-detailed} as Problem~\ref{prob:GOE-normalization}.

\begin{remark}
	We emphasize that in the GOE case, the normalization $W/\sqrt 2$ for
	\eqref{eq:GOE_proportional}
	is so that the variance is $1$ on the diagonal and
	$\frac{1}{2}$ off the diagonal.
\end{remark}

\section{Other classical ensembles with explicit eigenvalue densities}
\label{sec:other-ensembles}

Let us briefly discuss other classical ensembles with explicit eigenvalue densities,
which are not necessarily Gaussian,
but are related to other classical structures
like orthogonal polynomials. These ensembles
also have
a built-in parameter $\beta$ (and in the cases $\beta=1,2,4$, they have
invariance under orthogonal/unitary/symplectic conjugation).


\subsection{Wishart (Laguerre) ensemble}
\label{sec:Wishart}

In this subsection, we describe another classical family of random matrices whose eigenvalues form a fundamental example of a $\beta$-ensemble with a ``logarithmic'' pairwise interaction. These are called the \emph{Wishart} or \emph{Laguerre} ensembles. Their importance arises in statistics (covariance estimation, principal component analysis), signal processing, and many other areas.

\subsubsection{Definition via SVD}

Let $X$ be an $n\times m$ random matrix with iid\ entries drawn from a real/complex/quaternionic normal distribution. We assume $n \leq m$.
We can perform the \emph{singular value decomposition} (SVD) of $X$:
\[
    X = U
    \begin{pmatrix}
        s_1 &        & 0 \\
             &\ddots &    \\
         0  &        & s_n
    \end{pmatrix}
    V^\dagger,
\]
where $U,V$ are orthogonal/unitary/symplectic matrices (depending on $\beta$), $s_1,\dots,s_n\geq 0$ are the singular values of $X$, and $\dagger$
means the corresponding conjugation.
For example, in the real case, $s_1,\ldots,s_n $ are
the square roots of the eigenvalues of $X X^\top$.

Moreover, let $W=XX^\dagger$; this is called the Wishart
random matrix ensemble. We have
\[
    \lambda_i = s_i^2,\qquad i=1,\ldots,n;
		\qquad
		\lambda_1\ge \cdots \ge \lambda_n\ge 0.
\]
These eigenvalues admit a closed-form joint probability density function (pdf) in complete analogy with the GOE/GUE calculations from previous subsections.

\subsubsection{Joint density of eigenvalues}

\begin{theorem}[Wishart eigenvalue density]
\label{thm:Wishart-Distribution}
The ordered eigenvalues $\lambda_1,\dots,\lambda_n \geq 0$ of
the $n\times n$ Wishart matrix
$W$ have the joint density on $\{\lambda_i\geq 0\}$ proportional to
\[
    \prod_{1 \leq i < j \leq n}
    (\lambda_i - \lambda_j)^\beta
    \prod_{i=1}^n
    \lambda_i^{\frac{\beta}{2}(m - n + 1)-1}
    \exp\Bigl(-\tfrac{\lambda_i}{2}\Bigr),
\]
where $\beta=1,2,4$ corresponds to the real, complex, or
quaternionic case, respectively.
\end{theorem}



\begin{proof}[Idea of proof (sketch)]
The proof is a variant of the derivation for the joint eigenvalue density in the GOE/GUE case (see \Cref{sub:GOE-derivation}).  One writes down the joint distribution of all entries of \(X\), changes variables to singular values and orthogonal/unitary transformations, and identifies the Jacobian factor as
\(\prod_{i<j}|s_i^2 - s_j^2|^\beta = \prod_{i<j}| \lambda_i - \lambda_j |^\beta\).
The extra factors in front arise from the powers of \(\lambda_i\) (i.e.\ from \(\prod_i s_i\)) and the Gaussian exponential \(\exp\bigl(-\frac12\sum s_i^2\bigr)\) when reshaped to \(\exp\bigl(-\frac12\sum \lambda_i\bigr)\).
\end{proof}
\begin{remark}
The exponent of \(\lambda_i\) in the product is often written as
\(\alpha = \frac{\beta}{2}(m-n+1)-1\).  One also sees the name \emph{multivariate Gamma distribution} in statistics.  For \(\beta=1\) the ensemble is sometimes called the \emph{real Wishart} (or \emph{Laguerre Orthogonal}) ensemble; for \(\beta=2\) it is the \emph{complex Wishart} (or \emph{Laguerre Unitary}) ensemble; and \(\beta=4\) (not discussed in detail here) is the \emph{symplectic version}.
In point processes, the case $\beta=2$ is
also referred to as the \emph{Laguerre orthogonal polynomial ensemble}.
\end{remark}


\subsection{Jacobi (MANOVA/CCA) ensemble}
\label{sec:Jacobi_MANOVA_CCA}

The \emph{Jacobi} (sometimes called \emph{MANOVA} or \emph{CCA}) ensemble arises when one looks at the interaction between two independent rectangular Gaussian matrices that share the same number of columns.  Statistically, this corresponds to questions of canonical correlations or multivariate Beta distributions.  In random matrix theory, it appears as yet another fundamental example of a \(\beta\)-ensemble with an explicit eigenvalue density.

\subsubsection{Setup}

Let \(X\) be an \(n\times t\) real (or complex) matrix and \(Y\) be a \(k\times t\) matrix, with \(n\le k \le t\).  Assume \(X\) and \(Y\) have iid\ Gaussian entries (real or complex) of mean 0 and variance 1 and are independent of each other.

\begin{definition}[Projectors and canonical correlations]
Denote by
\[
  P_X \;=\; X^\top\!(X\,X^\top)^{-1}X
  \quad\bigl(\text{or }X^\dagger(X\,X^\dagger)^{-1}X\bigr),
\]
the orthogonal (unitary) projector onto the row span of \(X\).
Similarly, define
\[
  P_Y \;=\; Y^\top\!(Y\,Y^\top)^{-1}Y.
\]
These are \(t\times t\) projection matrices of ranks \(n\) and \(k\), respectively, embedded in a space of dimension~\(t\).  One checks that \(P_X\) and \(P_Y\) commute if and only if the row spaces of \(X\) and \(Y\) are aligned in a certain way.  The \emph{canonical correlations} between these two subspaces are the singular values of \(P_X P_Y\).  Equivalently, the \emph{squared} canonical correlations are the nonzero eigenvalues of \(P_X P_Y\).
\end{definition}

Since \(\mathrm{rank}(P_X P_Y)\le \min(n,k)\), there are at most \(\min(n,k)\) nonzero eigenvalues of \(P_X P_Y\).  In fact, generically
(when the subspaces are in ``general position''), there are exactly \(\min(n,k)\) nonzero eigenvalues.

\begin{example}
	For $n=k=1$, we have
	\begin{equation*}
		P_XP_Y=\frac{\langle X,Y \rangle }{\langle X,X \rangle \langle Y,X \rangle }\ssp X^\top Y,
	\end{equation*}
	which is a rank one matrix with the only nonzero singular
	eigenvalue $\langle X,Y \rangle $.
	Therefore, the singular value is exactly the sample correlation
	coefficient between $X$ and $Y$.
\end{example}

\subsubsection{Jacobi ensemble}

\begin{theorem}[Jacobi/MANOVA/CCA Distribution]
\label{thm:Jacobi_distribution}
Let \(X\) and \(Y\) be as above, each having iid\ (real
or complex) Gaussian entries of size \(n\times t\) and
\(k\times t\), respectively, with \(n\le k \le t\).  Assume
further that \(X\) and \(Y\) are independent of each other
(this
is the null hypothesis in statistics).

Then the nonzero eigenvalues \(\lambda_1,\ldots,\lambda_n\) of the matrix \(P_X P_Y\) lie in the interval \([0,1]\) and have the joint density function
of the form
\[
  \prod_{i<j} |\lambda_i - \lambda_j|^\beta
  \;\prod_{i=1}^n
  \lambda_i^{\,\frac{\beta}{2}(k-n+1)\,-1}
  \,\bigl(1-\lambda_i\bigr)^{\,\frac{\beta}{2}(t-n-k+1)\,-1},
\]
up to a normalization constant that depends on \(n,k,t\) (but not on \(\{\lambda_i\}\)).
Here again \(\beta=1\) for the real case and \(\beta=2\) for the complex case.
\end{theorem}
This distribution is called the \emph{Jacobi} (or \emph{MANOVA}, or \emph{CCA}) ensemble, and it is also sometimes called the
\emph{multivariate Beta distribution}.
In point processes, the $\beta=2$ case is often referred to as the
\emph{Jacobi orthogonal polynomial ensemble}.

\begin{remark}
The derivation is again parallel to that in the GOE/GUE context, but one now keeps track of the row spaces and the relevant rectangular dimensions.  The matrix \((X\,X^\top)\) (or \((X\,X^\dagger)\)) is invertible with high probability whenever \(n\le t\) and \(X\) is in general position.  The distribution above reflects the geometry of overlapping projectors in a higher-dimensional space \(\mathbb{R}^t\) (or \(\mathbb{C}^t\)).
\end{remark}


\subsection{General Pattern and \texorpdfstring{\(\beta\)}{beta}-Ensembles}
\label{sec:general_pattern_log_gas}

We have now seen three classical examples:
\begin{enumerate}[\(\bullet\)]
\item \emph{Wigner (Gaussian) ensembles} (real/complex/quaternionic),
\item \emph{Wishart/Laguerre ensembles} \(W = X X^\top\),
\item \emph{Jacobi/MANOVA/CCA ensembles}.
\end{enumerate}
Their eigenvalue densities (ordered or unordered) always display the same building blocks:
\[
  \prod_{1\le i<j\le n}
  |\lambda_i - \lambda_j|^{\beta}
  \;\times\;
  \prod_{i=1}^n V(\lambda_i),
\]
where \(\beta\) indicates the real (\(\beta=1\)), complex (\(\beta=2\)), or symplectic (\(\beta=4\)) symmetry class, and \(V(\lambda)\) is a single-variable potential function.
Such distributions are often referred to as \emph{\(\beta\)-ensembles} or \emph{log-gases}, reflecting that the factor \(\prod_{i<j}|\lambda_i - \lambda_j|^\beta\) can be interpreted as the Boltzmann weight for charges with a logarithmic pairwise repulsion.

\begin{remark}
	Beyond these three classical families, there are many other \emph{matrix models}
and \emph{discrete distributions}
whose eigenvalues produce similar log-gas structures but with different potentials \(V(\lambda)\).  These share many of the same techniques and phenomena (e.g.\ local eigenvalue statistics, largest-eigenvalue asymptotics, etc.) that appear throughout modern random matrix theory.
\end{remark}

\begin{remark}
For $\beta=2$, the connection to orthogonal polynomials
suggests discrete models of log-gases, which are powered by most
known orthogonal polynomials in one variable from the
(q-)\emph{Askey scheme} \cite{Koekoek1996}. For example,
the model of (uniformly random) lozenge tilings of the hexagon is
connected to Hahn orthogonal polynomials \cite{gorin2021lectures}
whose orthogonality weight is the classical
hypergeometric distribution from probability theory.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tridiagonal form for real symmetric matrices}
\label{sec:householder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Any real symmetric matrix can be orthogonally transformed into a tridiagonal matrix. This fact is standard in numerical linear algebra (the ``Householder reduction'') and also central in random matrix theory---notably in the Dumitriu--Edelman approach \cite{dumitriu2002matrix} for Gaussian ensembles.

\begin{theorem}
\label{thm:tridiagonal}
Any real symmetric matrix \(W \in \mathbb{R}^{n\times n}\) can be represented as
\[
  W \;=\; Q^\top\, T\, Q,
  \quad
  Q \in O(n),
\]
where \(T\) is real symmetric tridiagonal.  Concretely, \(T\) has nonzero entries only on the main diagonal and the first super-/sub-diagonals:
\[
  T \;=\;
  \begin{pmatrix}
         d_1 & \alpha_1 & 0 & \cdots & 0\\
         \alpha_1 & d_2 & \alpha_2 & \cdots & 0\\
         0 & \alpha_2 & d_3 & \ddots & \vdots\\
         \vdots & \vdots & \ddots & \ddots & \alpha_{n-1}\\
         0 & 0 & \cdots & \alpha_{n-1} & d_n
  \end{pmatrix}.
\]
\end{theorem}

\begin{definition}[Householder reflection]
A \emph{Householder reflection} in \(\mathbb{R}^n\) is a matrix \(H\) of the form
\[
  H \;=\; I \;-\; 2\,\frac{v\,v^\top}{\|v\|^2},
  \quad
  v\in \mathbb{R}^n \text{ nonzero column vector}.
\]
One checks that \(H^\top = H\), \(H^2 = I\), and \(H\) is orthogonal (i.e.\ \(H^\top H = I\)).  Geometrically, \(H\) is the reflection across the hyperplane orthogonal to~\(v\).
\end{definition}

\begin{proof}[Proof of \Cref{thm:tridiagonal}]
We want to apply successive Householder reflections to ``zero out'' all entries below the first subdiagonal (and by symmetry, above the first superdiagonal), leaving only the tridiagonal part possibly nonzero.

\smallskip\noindent\textbf{Start with }\(W^{(0)} = W\).

\smallskip\noindent\textbf{Step \(k=1\).}
   We aim to zero out the entries \(W^{(0)}_{2,1}, W^{(0)}_{3,1}, \dots, W^{(0)}_{n,1}\).  Define the vector
   \[
     x \;=\; \bigl(W^{(0)}_{2,1},\,W^{(0)}_{3,1},\,\dots,\,W^{(0)}_{n,1}\bigr)^\top
     \;\in\;\mathbb{R}^{\,n-1}.
   \]
   We then embed \(x\) into a vector \(\tilde{x} \in \mathbb{R}^{n}\) by placing \(0\) in the top coordinate:
   \[
     \tilde x \;=\; \bigl(0,\;W^{(0)}_{2,1},\;W^{(0)}_{3,1},\;\dots,\;W^{(0)}_{n,1}\bigr)^\top.
   \]
   To construct a Householder reflection that annihilates all but the first subdiagonal entry, choose
   \[
     v \;=\; \tilde x \;+\;\alpha\,e_1,
     \quad
     \alpha \;=\;\pm\,\|\tilde x\|,
   \]
   picking the sign of \(\alpha\) to avoid cancellation.  Then define
   \[
     H_1 \;=\; I \;-\; 2\,\frac{v\,v^\top}{\|v\|^2}.
   \]
   By construction, \(H_1\) is orthogonal and symmetric, and it will force all sub-subdiagonal entries below row~2 in column~1 to vanish when we do \(H_1\,W^{(0)}\,H_1\).

	 \colorbox{yellow}{\parbox{.7\textwidth}{MORE DETAILS ON THE HOUSEHOLDER REFLECTIONS NEEDED HERE.}}

\smallskip\noindent\textbf{Step \(k=2,\dots,n-2\).}
   Inductively, we zero out the \((k+2)\)-th through \(n\)-th entries in the \(k\)-th column (and by symmetry, the same row).  Each step uses a smaller Householder matrix \(H_k\) acting nontrivially in the lower-right \((n-k+1)\times(n-k+1)\) sub-block.  We set
   \[
     W^{(k)} \;=\; H_k \, W^{(k-1)} \, H_k.
   \]

\smallskip\noindent\textbf{End result.}
   After \(n-2\) steps, we obtain \(W^{(n-2)}\), which is tridiagonal.  Define
   \[
     Q \;=\; H_1 \,H_2\,\cdots\,H_{n-2}.
   \]
   Since each \(H_k\) is orthogonal, \(Q\in O(n)\).  We see
   \[
     W^{(n-2)}
     \;=\;
     (H_{n-2}\!\cdots H_1)\,\bigl[\,W\,\bigr]\,(H_1\!\cdots H_{n-2}),
   \]
   so
   \[
     W^{(n-2)}
     \;=\;
     Q \,W \,Q^\top
   \]
   has the desired tridiagonal form.
\end{proof}

\begin{remark}
This Householder procedure is also used in practical numerical methods for eigenvalue computations: once a real symmetric matrix is reduced to tridiagonal form, specialized algorithms (such as the QR algorithm) can then be applied more efficiently.
Overall, computations with tridiagonal matrices are much simpler and with better numerical stability than with general dense matrices.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tridiagonalization of random matrices}
\label{sec:Wigner-SC-detailed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here we discuss the tridiagonal form of the GOE random matrices,
and extend it to the general beta case.

\subsection{Dumitriu–Edelman tridiagonal model for GOE}

\begin{theorem}
\label{thm:DE-model}
Let \(W\) be an \(n\times n\) GOE matrix (real symmetric) with variances chosen so that each off-diagonal entry has variance \(1/2\) and each diagonal entry has variance \(1\).  Then there exists an orthogonal matrix \(Q\) such that
\[
   W \;=\; Q^\top\,T\,Q,
\]
where \(T\) is a real symmetric tridiagonal matrix of the special form
\[
   T \;=\; \begin{pmatrix}
         d_1 & \alpha_1 & 0 & \cdots \\
         \alpha_1 & d_2 & \alpha_2 & \ddots \\
         0 & \alpha_2 & d_3 & \ddots \\
         \vdots & \ddots & \ddots & \ddots
       \end{pmatrix},
\]
and the random variables \(\{d_i,\alpha_j\}_{1 \le i \le n,\;1\le j\le n-1}\) are mutually independent, with
\[
  d_i \,\sim\, \mathcal{N}(0,1),
  \qquad
  \alpha_j \;=\; \sqrt{\frac{\chi^2_{\,n-j}}{2}},
\]
where \(\chi^2_{\nu}\) is a chi-square distribution with \(\nu\) degrees of freedom.
\end{theorem}

\begin{remark}[Chi-square distributions]
The \emph{chi-square distribution} with \(\nu\) degrees of
freedom, denoted by \(\chi^2_{\nu}\), is a fundamental
distribution in statistics and probability theory. It arises
naturally as the distribution of the sum of the squares of
\(\nu\) independent standard normal random variables.
Formally, if \(Z_1, Z_2, \ldots, Z_{\nu}\) are independent
random variables with \(Z_i \sim \mathcal{N}(0,1)\), then
the random variable
\[
  Q = \sum_{i=1}^{\nu} Z_i^2
\]
follows a chi-square distribution with \(\nu\) degrees of
freedom, i.e., \(Q \sim \chi^2_{\nu}\). In the context of
the Dumitriu–Edelman tridiagonal model
(\Cref{thm:DE-model}), the subdiagonal entries \(\alpha_j\)
are defined as \(\alpha_j = \sqrt{\frac{\chi^2_{n-j}}{2}}\).
One can call this a \emph{chi random variable},
as this is a square root of a chi-square variable.

The parameter $\nu$ does not need to be an integer, and the
chi-square distribution is well defined for any positive
real $\nu$, by continuation of the density formula.
\end{remark}

\begin{proof}[Idea of proof of \Cref{thm:DE-model}]
	This construction is essentially a specialized version of
	the Householder reduction in \Cref{sec:householder}, set up
	so that each step matches precisely the distributions
	\(\alpha_j\sim \sqrt{\tfrac{\chi^2_{n-j}}{2}}\) and
	\(d_i\sim\mathcal{N}(0,1)\).  One uses the rotational
	invariance of Gaussian matrices to ensure at each step that
	the ``residual vector'' is isotropic (i.e., its distribution
	is invariant under orthogonal transformations).  The norm
	of that vector yields the \(\chi^2\)-type variables.
\end{proof}

Thus, to study the eigenvalues of a GOE matrix \(W\), one can equivalently study the (much sparser) random tridiagonal matrix \(T\).

\subsection{Generalization to \(\beta\)-ensembles}

The tridiagonal GOE construction (\Cref{thm:DE-model})
extends to a whole family of ensembles, parametrized by
\(\beta>0\).  In particular, for \(\beta = 1, 2, 4\) we get
the classical Orthogonal, Unitary, and Symplectic
(GOE/GUE/GSE) ensembles, respectively.  The general
$\beta$ case is known as the \(\beta\)-ensemble; 
outside of the classical cases $\beta=1,2,4$, there
is no matrix ensemble interpretation with iid entries,
but the tridiagonal form model still works.

We saw that
the \(\beta\)-ensembles arise naturally as
\emph{log-gases} in physics, with density 
proportional to
\[
  \exp\Bigl(-\,\sum_{i=1}^n V(\lambda_i)\Bigr)
  \;\prod_{1\le i<j\le n}\bigl|\lambda_i - \lambda_j\bigr|^\beta
\]
for some potential \(V\).  The simplest choice, \(V(\lambda)=\tfrac12\,\lambda^2\), corresponds to Gaussian \(\beta\)-ensembles, which in the classical cases reproduce GOE/GUE/GSE.

\begin{remark}[Tridiagonal Construction for General \(\beta\)]
	A breakthrough \cite{dumitriu2002matrix} showed that the
	Gaussian \(\beta\)-ensembles (for \emph{any} \(\beta>0\))
	can be represented 
	as eigenvalues of real symmetric
	\emph{tridiagonal} matrices
	whose entries
	are independent 
	(but not identically distributed), and have 
	Gaussian and chi distributions: 
\begin{itemize}
\item The diagonal entries are iid\ 
	standard normal random variables $\mathcal{N}(0,1)$.
\item The subdiagonal entries are
	$\alpha_j = \sqrt{\frac{\chi^2_{(n-j)\beta}}{2}}$,
	where $\chi^2_{\nu}$ is a chi-square distribution with $\nu$ degrees of freedom.
	Here we use the fact that the parameter $\nu$ in the chi-square distribution does not need to be an integer.
\item The superdiagonal entries are
	determined by symmetry.
\end{itemize}
\end{remark}

In the next lecture, we will see how the tridiagonal form
allows to prove the Wigner's semicircle law for the
Gaussian \(\beta\)-ensembles.



\appendix
\setcounter{section}{2}

\section{Problems (due 2025-02-22)}

\subsection{Invariance of GOE and GUE}
\label{prob:invariance_GOE_GUE}

Show that the distribution of the GOE and GUE is
invariant under, respectively, orthogonal and unitary conjugation.
For GOE, this means that if \(W\)
is a random GOE matrix and \(Q\) is a fixed orthogonal
matrix of order $n$, then the distribution
of \(QWQ^\top\) is the same as the distribution of \(W\).
(Similarly for GUE.)

\medskip
\noindent
Hint: write the joint density of all entries of GOE/GUE (for instance, GOE
is determined by $n(n+1)/2$ real random independent variables)
in a coordinate-free way.


\subsection{Preimage size for spectral decomposition}
\label{prob:GOE-preimage}

Show that for a real symmetric matrix $W$ with distinct eigenvalues,
if $W=Q\Lambda Q^\top$ is its spectral decomposition
where $Q$ is orthogonal and $\Lambda=\mathrm{diag}(\lambda_1,\ldots,\lambda_n)$
is diagonal with $(\lambda_1\ge \cdots\ge \lambda_n)$,
then there are exactly $2^n$ different choices of $Q$
that give the same matrix $W$.

\subsection{Distinct eigenvalues}
\label{prob:distinct-eigenvalues}

Show that under GOE and GUE, almost surely,
all eigenvalues are distinct.


\subsection{Testing distinctness of eigenvalues via rank-1 perturbations}
\label{prob:distinct-eigs-rank1}
  Suppose \(\lambda\) is an eigenvalue of a fixed matrix
	\(W\) with multiplicity \(\ell\). Consider the rank-1 perturbation
  \[
     W_{\varepsilon} \;=\; W \;+\;\alpha \, u \, u^\top,
     \quad \alpha \sim \mathcal{N}(0,\varepsilon),
  \]
  where \(u \in \mathbb{R}^n\) is fixed. Prove that with probability one (in \(\alpha\)), the eigenvalue \(\lambda\) \emph{splits} into \(\ell\) distinct eigenvalues of \(W_{\varepsilon}\). 

\medskip
\noindent
\emph{Hint:}
Write the characteristic polynomial of \(W_{\varepsilon}\) as \(\det(W_{\varepsilon}-\mu I)\). Show that the infinitesimal change in \(\alpha\) moves the roots in a non-degenerate way, splitting a repeated root.



\subsection{Jacobian for GUE}
\label{prob:Jacobian-GUE}

Arguing similarly to
\Cref{subsec:jacobian},
show that the Jacobian for the spectral decomposition
of a complex Hermitian matrix is proportional to
\begin{equation*}
	\prod_{1\le i<j\le n}|\lambda_i-\lambda_j|^2.
\end{equation*}
In particular, make sure you understand
where the factor $2$ comes from in the complex case.

\subsection{Normalization for GOE}
\label{prob:GOE-normalization}

Compute the $n$-dimensional integral
(in the ordered on unordered form):
\begin{multline*}
	\int_{\lambda_1<\ldots<\lambda_n } \prod_{i<j}(\lambda_i-\lambda_j)
	\ssp
	\exp\Bigl(-\tfrac{1}{2}\sum_{k=1}^n \lambda_k^2\Bigr)\,d\lambda_1\cdots d\lambda_n.\\=
	\frac{1}{n!}
	\int_{\mathbb{R}^n}
	\prod_{i<j}|\lambda_i-\lambda_j|\ssp
	\exp\Bigl(-\tfrac{1}{2}\sum_{k=1}^n \lambda_k^2\Bigr)\,d\lambda_1\cdots d\lambda_n.
\end{multline*}

\medskip
\noindent
Hint: The following identity might be useful:
\[
\int_{-\infty}^{\infty} x^{2m} e^{-x^2/2} \, dx = 2^{m+1/2} \Gamma \left( m + \frac{1}{2} \right).
\]


\subsection{Wishart eigenvalue density}

Prove \Cref{thm:Wishart-Distribution} (in the real case $\beta=1$)
by using the singular
value
decomposition
of \(X\) and the properties of the Wishart ensemble.


\subsection{Householder reflection properties}
\label{prob:householder-properties}

Show that the Householder reflection \(H = I - 2\,v\,v^\top/\|v\|^2\) has the following properties:
\begin{enumerate}
	\item \(H\) is orthogonal, i.e., \(H^\top H = I\).
	\item \(H\) is symmetric, i.e., \(H^\top = H\).
	\item \(H\) is idempotent, i.e., \(H^2 = I\).
	\item \(H\) is a reflection across the hyperplane orthogonal to \(v\).
\end{enumerate}

\subsection{Distribution of the Householder vector in random tridiagonalization}
\label{prob:householder-vector}

Consider the first step of the Householder tridiagonalization of a GOE matrix \(W\). Denote the first column by \(x \in \mathbb{R}^n\), and let
\[
   v \;=\; x \;+\;\alpha \,e_1,\quad 
   \alpha \;=\; \pm\,\|x\|.
\]
Then the first Householder reflection is given by 
\[
   H_1 \;=\; I \;-\; 2 \,\frac{v\,v^\top}{\langle v,v\rangle}.
\]
Prove that:
\begin{enumerate}
  \item \(\|v\|^2\) follows a \(\chi^2_{\,\nu}\) distribution with \(\nu\) degrees of freedom (determine \(\nu\) in terms of \(n\)).
  \item The direction \(v / \|v\|\) is uniformly distributed on the unit sphere \(\mathbb{S}^{n-1}\) and is independent of \(\|v\|\).
\end{enumerate}

\medskip
\noindent
\emph{Hint:}
View \(x\) as a Gaussian vector in \(\mathbb{R}^n\), using the fact that the first column of a GOE matrix (including its diagonal entry) is an isotropic normal vector (up to small adjustments for the diagonal). Orthogonal invariance of the underlying distribution ensures the direction is uniform on \(\mathbb{S}^{n-1}\).

\subsection{Householder reflection for GUE}

Modify the tridiagonalization procedure which was discussed 
for the GOE case, and show that the 
GUE random matrix can be transformed (by a unitary conjugation)
into
\begin{equation*}
	\begin{pmatrix}
		\mathcal{N}(0,1) & \chi_{2(n-1)}/\sqrt{2} & 0 & 0 & \cdots \\
		\chi_{2(n-1)}/\sqrt{2} & \mathcal{N}(0,1) & \chi_{2(n-2)}/\sqrt{2} & 0 & \cdots \\
		0 & \chi_{2(n-2)}/\sqrt{2} & \mathcal{N}(0,1) & \chi_{2(n-3)}/\sqrt{2} & \cdots \\
		0 & 0 & \chi_{2(n-3)}/\sqrt{2} & \mathcal{N}(0,1) & \cdots \\
		\vdots & \vdots & \vdots & \vdots & \ddots
	\end{pmatrix}
\end{equation*}
(this matrix is symmetric, and in the entries, we list the
distributions).






\subsection{Jacobi ensemble is related to two Wisharts}
\label{prob:jacobi-two-wisharts}

Let \(X\) be an \(n\times m\) and \(Y\) be a \(k\times m\) real Gaussian matrices with iid \(\mathcal{N}(0,1)\) entries, independent of each other, and assume \(n \le k \le m\). Consider the matrix
\[
  \bigl(X\,X^\top + Y\,Y^\top \bigr)^{-1}\,
  \bigl(X\,X^\top \bigr)
  \quad \in \mathbb{R}^{n\times n}.
\]
\begin{enumerate}
  \item Prove that it is well-defined (invertible denominator) with probability 1, and that it is symmetric and diagonalizable in \(\mathbb{R}^n\).  
  \item Show that its eigenvalues lie in \([0,1]\) and follow a Jacobi (MANOVA) distribution of parameters \(\beta=1\) and \(\bigl(n,k,m\bigr)\).  
  \item Identify explicitly how these parameters match the shape parameters in the standard multivariate Beta / Jacobi pdf
  \[
     \prod_{i<j} |\lambda_i - \lambda_j| \,\prod_{i=1}^n 
     \lambda_i^{\,\alpha}\,(1-\lambda_i)^{\,\gamma},
  \]
  with appropriate \(\alpha,\gamma\) in terms of \(n,k,m\).
\end{enumerate}

\noindent
\emph{Hint:}
Use that \(X\,X^\top\) and \(Y\,Y^\top\) are (independent) Wishart matrices. Rewrite 
\[\bigl(X\,X^\top + Y\,Y^\top\bigr)^{-1}X\,X^\top\] via block-inversion or projector-based arguments to see it is related to the product of two orthogonal projectors in \(\mathbb{R}^{m}\). The Jacobi distribution then emerges from the overlapping subspace geometry.
























\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}
\usepackage{comment}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{example}[proposition]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newenvironment{lnotes}{\section*{Notes for the lecturer}}{}
% \excludecomment{lnotes}


\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 3: Gaussian and tridiagonal matrices}


\date{Wednesday, January 22, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l03.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}

\author{Leonid Petrov}


\maketitle


\tableofcontents




\section{Recap}

We have established the semicircle law for
real Wigner random matrices.
If $W$ is an $n\times n$ real symmetric matrix with
independent entries $X_{ij}$ above the main diagonal
(mean zero, variance~$1$), and mean zero diagonal entries,
then the empirical spectral distribution of $W/\sqrt{n}$
converges to the semicircle law as $n\to\infty$:
\begin{equation}
	\label{eq:semicircle_conv}
	\lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i/\sqrt n} =
	\mu_{\mathrm{sc}},
\end{equation}
where
\begin{equation*}
	\mu_{\mathrm{sc}}(dx) = \begin{cases}
		\frac{1}{2\pi} \sqrt{4-x^2} \, dx, & \text{if } |x|\le 2, \\
		0, & \text{otherwise}.
	\end{cases}
\end{equation*}
The convergence
in \eqref{eq:semicircle_conv} is weakly almost sure.
The way we got the result is by expanding
$\operatorname{\mathbb{E}}\operatorname{Tr} (W^k)$ and counting
trees, plus analytic lemmas which ensure that
the convergence of expected powers of traces is enough
to conclude the convergence
\eqref{eq:semicircle_conv}
of the empirical spectral measures.

\medskip

Today, we are going to focus on Gaussian ensembles. The plan is:
\begin{enumerate}[$\bullet$]
	\item Definition and spectral density for real symmetric Gaussian matrices (GOE).
	\item Tridiagonalization and general beta ensemble.
	\item Wigner's semicircle law via tridiagonalization.
\end{enumerate}

\section{Gaussian ensembles}

\subsection{Definitions}
\label{sub:GOE_GUE_definitons}

Recall that a real Wigner matrix $W$ can be modeled as
\begin{equation*}
	W=\frac{Y+Y^\top}{\sqrt{2}},
\end{equation*}
where $Y$ is an $n\times n$ matrix with independent entries $Y_{ij}$,
$1\le i,j\le n$, such that $Y_{ij}$ are mean zero, variance~$1$.
Then for $1\le i<j\le n$, we have for the matrix
$W=(X_{ij})$:
\begin{equation*}
	\operatorname{\mathrm{Var}}\left( X_{ii} \right)=
	\operatorname{\mathrm{Var}}( \sqrt 2\ssp Y_{ii} )=2,\qquad
	\operatorname{\mathrm{Var}}\left( X_{ij} \right)=
	\operatorname{\mathrm{Var}}\left( \frac{Y_{ij}+Y_{ji}}{\sqrt 2} \right)=1.
\end{equation*}

If, in addition, we assume that $Y_{ij}$ are standard Gaussian
$\mathcal{N}(0,1)$, then the distribution of $W$ is called
the \emph{Gaussian Orthogonal Ensemble} (GOE).

For the complex case, we
have the \emph{standard complex Gaussian random variable}
\begin{equation*}
	Z=\frac{1}{\sqrt 2}\left( Z^R+\mathbf{i}\ssp Z^I \right),
	\qquad
	\operatorname{\mathbb{E}} (Z)=0,
	\qquad
	\operatorname{\mathrm{Var}}_{\mathbb{C}}(Z)\coloneqq
	\operatorname{\mathbb{E}} (|Z|^2)=
	\frac{
	\operatorname{\mathbb{E}} (|Z^R|^2)+
	\operatorname{\mathbb{E}} (|Z^I|^2)}{2}=1
	,
\end{equation*}
where $Z^R$ and $Z^I$ are independent
standard Gaussian real random variables $\mathcal{N}(0,1)$.

If we take $Y$ to be an $n\times n$ matrix with independent
entries $Y_{ij}$, $1\le i,j\le n$
distributed as $Z$, then the random matrix\footnote{$Y^\dagger$ denotes the transpose of $Y$ combined with complex conjugation.}
\begin{equation*}
	W=\frac{Y+Y^\dagger}{\sqrt 2}
\end{equation*}
is said to have the \emph{Gaussian Unitary Ensemble} (GUE) distribution.
For the GUE matrix $W=(X_{ij})$,
we have for $1\le i<j\le n$:
\begin{equation*}
	\operatorname{\mathrm{Var}}_{\mathbb{C}}(X_{ii})=2,
	\qquad
	\operatorname{\mathrm{Var}}_{\mathbb{C}}(X_{ij})
	=\frac{1}{4}
	\Bigl[ \operatorname{\mathbb{E}}(Z_{ij}^R
			+
		Z_{ji}^R)^2
		+
		\operatorname{\mathbb{E}}(Z_{ij}^I
			+
		Z_{ji}^I)^2
	\Bigr]
	=1.
\end{equation*}

Both GOE and GUE have real eigenvalues $\lambda_1 \ge \ldots \ge \lambda_n$.
We are going to describe the joint distribution of these eigenvalues.
Despite the fact that the map from a matrix to its eigenvalues
is quite complicated and nonlinear (you need to solve an equation of degree $n$),
the distribution of eigenvalues in the Gaussian cases is fully explicit.

See Problem \ref{prob:invariance_GOE_GUE}
for invariance of GOE/GUE under orthogonal/unitary conjugation
(this is where the names ``orthogonal'' and ``unitary'' come from).

\begin{remark}
	There is a third player in the game, the \emph{Gaussian
	Symplectic Ensemble} (GSE),which we will mainly ignore in
	this course
	due to its less intuitive quaternionic nature.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Joint eigenvalue distribution for GOE}
\label{sub:GOE-derivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we give a derivation of the joint probability density for the GOE.

\begin{theorem}[GOE Joint Eigenvalue Density]
\label{thm:GOE-joint-eigs-detailed}
Let \(W\) be an \(n\times n\) real symmetric matrix with
the GOE distribution (\Cref{sub:GOE_GUE_definitons}).
Then its ordered real eigenvalues \(\lambda_1 \le \cdots \le
\lambda_n\)
of $W/\sqrt 2$
have a joint probability density function
on $\mathbb{R}^n$
given by:
\[
  p(\lambda_1,\ldots,\lambda_n)
  \;=\;\frac{1}{Z_{n}}
  \,\prod_{1 \le i < j \le n}
  \!\!\bigl|\lambda_i - \lambda_j\bigr|\,
  \exp\Bigl(
    -\frac{1}{2}\sum_{k=1}^n \lambda_k^2
  \Bigr),
\]
where \(Z_{n}\) is a constant (depending on \(n\) but not on \(\lambda_i\)) ensuring the density integrates to 1:
\begin{equation*}
	Z_n=Z_n^{GOE}=\frac{(2\pi)^{n/2}}{n!}
	\prod_{j=0}^{n-1}\frac{\Gamma(1+(j+1)\beta/2)}{\Gamma(1+\beta/2)}, \qquad
	\beta=1.
\end{equation*}
\end{theorem}
\begin{remark}
	We renormalized the GOE by a factor of $\sqrt 2$ to make the
	Gaussian part of the density, $\exp(-\frac{1}{2}\sum_{k=1}^n \lambda_k^2)$,
	standard. In the GUE case, no normalization is required.
\end{remark}

We break the proof into four major steps,
considered in
\Cref{subsec:density-entries,subsec:spectral,subsec:jacobian,subsec:final-form}
below.

\subsection{Step A. Joint density of matrix entries}
\label{subsec:density-entries}

Let us label all independent entries of \(W/\sqrt 2\):
\[
	\{\underbrace{X_{12}, X_{13},\dots, X_{23},\ldots }_{\text{above diag}},
	\underbrace{X_{22}, X_{33},\dots}_{\text{diag}}\}.
\]
There are \(\frac{n(n-1)}{2}\) off-diagonal entries
with variance $1/2$,
and \(n\) diagonal entries with variance $1$.
The joint density of these entries (ignoring normalization for a moment) is
proportional to
\begin{equation}
	\label{eq:GOE-density-entries-first}
	f(x_{12},x_{13},\ldots,x_{22},x_{33},\ldots )
  \propto\exp\Bigl(
    - \sum_{i<j} x_{ij}^2
    -\frac{1}{2} \sum_{i=1}^n x_{ii}^2
  \Bigr)=
	\exp
	\Bigl( -\frac{1}{2}\sum_{i,j=1}^n x_{ij}^2 \Bigr),
\end{equation}
where in the right-hand side, we have
$x_{ij}=x_{ji}$ for $i\ne j$.
We then recognize
\[
	\sum_{i,j=1}^n x_{ij}^2=\operatorname{Tr}(W^2)=\sum_{k=1}^n \lambda_k^2.
\]
Including the normalization for Gaussians, one arrives at
the density on $\mathbb{R}^{n(n+1)/2}$:
\[
  f(W)\,dW
  \;=\;
  \pi^{-\tfrac{n(n-1)}{4}}
  \,\bigl(2\pi\bigr)^{-\tfrac{n}{4}}
  \,\exp\Bigl(-\tfrac{1}{2}\operatorname{Tr}(W^2)\Bigr)\; dW,
\]
where \(dW\) is the product measure over the \(\tfrac{n(n+1)}{2}\) independent entries.

\subsection{Step B. Spectral decomposition}
\label{subsec:spectral}

Since \(W\) is real symmetric, it can be orthogonally diagonalized:
\[
  W = Q\,\Lambda\,Q^\top,\quad
  Q \in O(n),
\]
where \(\Lambda = \mathrm{diag}(\lambda_1,\ldots,\lambda_n)\) has the eigenvalues.  Then, as we saw before, we have
\[
  \operatorname{Tr}(W^2)
  = \operatorname{Tr}\bigl(Q\,\Lambda\,Q^\top Q\,\Lambda\,Q^\top\bigr)
  = \operatorname{Tr}(\Lambda^2)
  = \sum_{k=1}^n \lambda_k^2.
\]
The map
from $W$ to $(\Lambda,Q)$ is not one-to one,
but in case $W$ has distinct eigenvalues,
the preimage of $(\Lambda,Q)$
contains $2^n$ elements.
See Problems~\ref{prob:GOE-preimage} and \ref{prob:distinct-eigenvalues}.
\medskip

It remains to make the change of variables from \(W\) to \(\Lambda\), which involves the Jacobian.

\subsection{Step C. Jacobian}
\label{subsec:jacobian}

We now examine how the measure \(dW\) in the space of real symmetric matrices factors into a piece depending on \(\{\lambda_i\}\) and a piece depending on \(Q\).  Formally,
\[
  dW
  = \Bigl|\det\bigl(\tfrac{\partial W}{\partial(\Lambda,Q)}\bigr)\Bigr|
    \,d\Lambda\,dQ,
\]
where \(dQ\) is the Haar measure\footnote{Recall that the
	Haar measure on \(O(n)\) is the unique
	(up to a constant factor) measure that is invariant under
	group shifts (in this situation, both left and right shifts
	work). In probabilistic terms,
	if a random orthogonal matrix $Q$ is Haar-distributed,
	then $QR$ and $RQ$ are also Haar-distributed for any fixed orthogonal
matrix $R$.}
on \(O(n)\), and
\(d\Lambda\) is the Lebesgue measure on \(\mathbb{R}^n\).
The Lebesgue measure later needs to be restricted
to the ``Weyl chamber''
\(\lambda_1\le \cdots\le \lambda_n\) if we want an ordering,
this introduces the simple factor \(n!\) in the final density.


\begin{lemma}[Jacobian for Spectral Decomposition]
\label{lemma:Jacobian-GOE}
For real symmetric \(W=Q\Lambda Q^\top\), one has
\[
  \bigl|\det\bigl(\tfrac{\partial W}{\partial(\Lambda,Q)}\bigr)\bigr|
  \;=\;
	\mathrm{const}
  \prod_{1\le i<j\le n}
	\!\!
	\bigl|\lambda_i - \lambda_j\bigr|,
\]
where the constant is independent of the \(\lambda_i\)'s and
depends only on \(n\).
\end{lemma}

\begin{remark}
Equivalently, one often writes
\[
  dW
  \;=\;
  \bigl|\Delta(\lambda_1,\dots,\lambda_n)\bigr|\;
  d\Lambda\,dQ,
  \quad\text{where }
  \Delta(\lambda_1,\dots,\lambda_n)
  = \prod_{i<j}(\lambda_j-\lambda_i)
\]
is the \emph{Vandermonde determinant}.
\end{remark}

We prove \Cref{lemma:Jacobian-GOE} in the rest of this subsection.
\medskip

Consider small perturbations of \(\Lambda\) and \(Q\).  Write
\[
  W = Q\,\Lambda\,Q^\top,
  \quad
  \Lambda = \mathrm{diag}(\lambda_1,\dots,\lambda_n).
\]
Let \(\delta W\) be an infinitesimal change in \(W\). We want to see how \(\delta W\) depends on \(\delta\Lambda\) and \(\delta Q\).

\paragraph{Parametrizing \(\delta Q\).}
Since \( Q \in O(n) \), any small variation of \( Q \) can be expressed as
\[ Q \exp(B) \approx Q(I + B), \] where \( B \) is an infinitesimal
skew-symmetric matrix (\( B^\top = -B \)). Indeed,
$\exp(B)$ must be orthogonal, so $\exp(B)^\top \exp(B) = I$.
Thus, we have
\begin{equation*}
	(I+B)^\top(I+B)=I,\qquad \text{or}\qquad B^\top + B = 0.
\end{equation*}
Note that $\exp(B)$ is the matrix exponential of $B$,
which is defined by the usual power series.
Note also that the dimension of \( O(n) \)
is \( \dim(O(n)) = \frac{n(n-1)}{2} \), which matches the dimension of the
space of skew-symmetric matrices.

\paragraph{Computing \(\delta W\).}
Under an infinitesimal change, say,
\[
  Q \mapsto Q\,(I + B),
  \quad
  \Lambda \mapsto \Lambda + \delta\Lambda,
\]
we have
\[
  W
  = Q\ssp\Lambda\ssp Q^\top
  \;\;\Longrightarrow\;\;
  Q^\top \delta W Q
	=
	\delta\Lambda
	+
	B\Lambda-\Lambda B,
\]
to first order in small quantities.
Here we used the orthogonality of $Q$ and
the skew-symmetry of~$B$.

\paragraph{Local structure of the map.}

We see that the map
$W\mapsto(\Lambda,Q)$
in a neighborhood of $(\Lambda,Q)$ determined by
$\delta \Lambda$ and $B$
locally translates by $Q^\top\ssp\delta \Lambda\ssp Q$,
which implies the Lebesgue factor
$d\lambda_1 \ldots d\lambda_n $
in $\delta W$. Indeed, the Lebesgue measure
on $\mathbb{R}^n$ is invariant under orthogonal transformations.

The next terms, the commutator $[B,\Lambda]$, has the form
(recall that $B$ is infinitesimally small and $\Lambda$ is diagonal):
\begin{align*}
B\Lambda-\Lambda B&=
\begin{pmatrix}
	0 & b_{12} & \cdots \\
	-b_{12} & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
	\lambda_1 & 0 & \cdots \\
	0 & \lambda_2 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}-
\begin{pmatrix}
	\lambda_1 & 0 & \cdots \\
	0 & \lambda_2 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
	0 & b_{12} & \cdots \\
	-b_{12} & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}\\&
=
\begin{pmatrix}
	0 & b_{12}\lambda_2 & \cdots \\
	-b_{12}\lambda_1 & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
-
\begin{pmatrix}
	0 & b_{12}\lambda_1 & \cdots \\
	b_{12}\lambda_2 & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}
\\&=
\begin{pmatrix}
	0 &b_{12}(\lambda_2-\lambda_1) & \cdots \\
	b_{12}(\lambda_1-\lambda_2) & 0 & \cdots \\
	\vdots & \vdots & \ddots
\end{pmatrix}.
\end{align*}
Thus, this action locally means that the
infinitesimal $b_{ij}$ is multiplied by $\lambda_i-\lambda_j$,
for all $1\le i<j\le n$.
This is a scalar factor that does not depend on
the orthogonal component $Q$, but only on the eigenvalues.
Therefore, this factor is the same in
$Q^\top \ssp \delta W \ssp Q$.

This completes the proof of
\Cref{lemma:Jacobian-GOE}.
See also Problem~\ref{prob:Jacobian-GUE} for the GUE Jacobian.

\subsection{Step D. Final Form of the density}
\label{subsec:final-form}

Putting Steps A--C together, we find:
\[
  dW
  \;=\;
	\mathrm{const}\cdot
  \prod_{i<j}|\lambda_i-\lambda_j|\ssp
  d\Lambda
  \,\bigl(\underbrace{\text{Haar measure on }O(n)}_{\text{does not depend on }\lambda_i}\bigr).
\]
Hence, the joint density of \(\{\lambda_1,\dots,\lambda_n\}\) is,
up to normalization depending only on \(n\), equal to
\begin{equation}
	\label{eq:GOE_proportional}
  \prod_{i<j}|\lambda_i-\lambda_j|\;
  \exp\Bigl(-\tfrac{1}{2}\sum_{k=1}^n \lambda_k^2\Bigr).
\end{equation}
We leave the computation of the normalization constant in
\Cref{thm:GOE-joint-eigs-detailed} as Problem~\ref{prob:GOE-normalization}.

\begin{remark}
	We emphasize that in the GOE case, the normalization $W/\sqrt 2$ for
	\eqref{eq:GOE_proportional}
	is so that the variance is $1$ on the diagonal and
	$\frac{1}{2}$ off the diagonal.
\end{remark}

\section{Other classical ensembles with explicit eigenvalue densities}
\label{sec:other-ensembles}

Let us briefly discuss other classical ensembles with explicit eigenvalue densities,
which are not necessarily Gaussian,
but are related to other classical structures
like orthogonal polynomials. These ensembles
also have
a built-in parameter $\beta$ (and in the cases $\beta=1,2,4$, they have
invariance under orthogonal/unitary/symplectic conjugation).


\subsection{Wishart (Laguerre) ensemble}
\label{sec:Wishart}

In this subsection, we describe another classical family of random matrices whose eigenvalues form a fundamental example of a $\beta$-ensemble with a ``logarithmic'' pairwise interaction. These are called the \emph{Wishart} or \emph{Laguerre} ensembles. Their importance arises in statistics (covariance estimation, principal component analysis), signal processing, and many other areas.

\subsubsection{Definition via SVD}

Let $X$ be an $n\times m$ random matrix with i.i.d.\ entries drawn from a real/complex/quaternionic normal distribution. We assume $n \leq m$.
We can perform the \emph{singular value decomposition} (SVD) of $X$:
\[
    X = U
    \begin{pmatrix}
        s_1 &        & 0 \\
             &\ddots &    \\
         0  &        & s_n
    \end{pmatrix}
    V^\dagger,
\]
where $U,V$ are orthogonal/unitary/symplectic matrices (depending on $\beta$), $s_1,\dots,s_n\geq 0$ are the singular values of $X$, and $\dagger$
means the corresponding conjugation.
For example, in the real case, $s_1,\ldots,s_n $ are
the square roots of the eigenvalues of $X X^\top$.

Moreover, let $W=XX^\dagger$; this is called the Wishart
random matrix ensemble. We have
\[
    \lambda_i = s_i^2,\qquad i=1,\ldots,n;
		\qquad
		\lambda_1\ge \cdots \ge \lambda_n\ge 0.
\]
These eigenvalues admit a closed-form joint probability density function (pdf) in complete analogy with the GOE/GUE calculations from previous subsections.

\subsubsection{Joint density of eigenvalues}

\begin{theorem}[Wishart eigenvalue density]
\label{thm:Wishart-Distribution}
The ordered eigenvalues $\lambda_1,\dots,\lambda_n \geq 0$ of
the $n\times n$ Wishart matrix
$W$ have the joint density on $\{\lambda_i\geq 0\}$ proportional to
\[
    \prod_{1 \leq i < j \leq n}
    (\lambda_i - \lambda_j)^\beta
    \prod_{i=1}^n
    \lambda_i^{\frac{\beta}{2}(m - n + 1)-1}
    \exp\Bigl(-\tfrac{\lambda_i}{2}\Bigr),
\]
where $\beta=1,2,4$ corresponds to the real, complex, or
quaternionic case, respectively.
\end{theorem}



\begin{proof}[Idea of proof (sketch)]
The proof is a variant of the derivation for the joint eigenvalue density in the GOE/GUE case (see \Cref{sub:GOE-derivation}).  One writes down the joint distribution of all entries of \(X\), changes variables to singular values and orthogonal/unitary transformations, and identifies the Jacobian factor as
\(\prod_{i<j}|s_i^2 - s_j^2|^\beta = \prod_{i<j}| \lambda_i - \lambda_j |^\beta\).
The extra factors in front arise from the powers of \(\lambda_i\) (i.e.\ from \(\prod_i s_i\)) and the Gaussian exponential \(\exp\bigl(-\frac12\sum s_i^2\bigr)\) when reshaped to \(\exp\bigl(-\frac12\sum \lambda_i\bigr)\).
\end{proof}
\begin{remark}
The exponent of \(\lambda_i\) in the product is often written as
\(\alpha = \frac{\beta}{2}(m-n+1)-1\).  One also sees the name \emph{multivariate Gamma distribution} in statistics.  For \(\beta=1\) the ensemble is sometimes called the \emph{real Wishart} (or \emph{Laguerre Orthogonal}) ensemble; for \(\beta=2\) it is the \emph{complex Wishart} (or \emph{Laguerre Unitary}) ensemble; and \(\beta=4\) (not discussed in detail here) is the \emph{symplectic version}.
In point processes, the case $\beta=2$ is
also referred to as the \emph{Laguerre orthogonal polynomial ensemble}.
\end{remark}


\subsection{Jacobi (MANOVA/CCA) ensemble}
\label{sec:Jacobi_MANOVA_CCA}

The \emph{Jacobi} (sometimes called \emph{MANOVA} or \emph{CCA}) ensemble arises when one looks at the interaction between two independent rectangular Gaussian matrices that share the same number of columns.  Statistically, this corresponds to questions of canonical correlations or multivariate Beta distributions.  In random matrix theory, it appears as yet another fundamental example of a \(\beta\)-ensemble with an explicit eigenvalue density.

\subsubsection{Setup}

Let \(X\) be an \(n\times t\) real (or complex) matrix and \(Y\) be a \(k\times t\) matrix, with \(n\le k \le t\).  Assume \(X\) and \(Y\) have i.i.d.\ Gaussian entries (real or complex) of mean 0 and variance 1 and are independent of each other.

\begin{definition}[Projectors and canonical correlations]
Denote by
\[
  P_X \;=\; X^\top\!(X\,X^\top)^{-1}X
  \quad\bigl(\text{or }X^\dagger(X\,X^\dagger)^{-1}X\bigr),
\]
the orthogonal (unitary) projector onto the row span of \(X\).
Similarly, define
\[
  P_Y \;=\; Y^\top\!(Y\,Y^\top)^{-1}Y.
\]
These are \(t\times t\) projection matrices of ranks \(n\) and \(k\), respectively, embedded in a space of dimension~\(t\).  One checks that \(P_X\) and \(P_Y\) commute if and only if the row spaces of \(X\) and \(Y\) are aligned in a certain way.  The \emph{canonical correlations} between these two subspaces are the singular values of \(P_X P_Y\).  Equivalently, the \emph{squared} canonical correlations are the nonzero eigenvalues of \(P_X P_Y\).
\end{definition}

Since \(\mathrm{rank}(P_X P_Y)\le \min(n,k)\), there are at most \(\min(n,k)\) nonzero eigenvalues of \(P_X P_Y\).  In fact, generically
(when the subspaces are in ``general position''), there are exactly \(\min(n,k)\) nonzero eigenvalues.

\begin{example}
	For $n=k=1$, we have
	\begin{equation*}
		P_XP_Y=\frac{\langle X,Y \rangle }{\langle X,X \rangle \langle Y,X \rangle }\ssp X^\top Y,
	\end{equation*}
	which is a rank one matrix with the only nonzero singular
	eigenvalue $\langle X,Y \rangle $.
	Therefore, the singular value is exactly the sample correlation
	coefficient between $X$ and $Y$.
\end{example}

\subsubsection{Jacobi ensemble}

\begin{theorem}[Jacobi/MANOVA/CCA Distribution]
\label{thm:Jacobi_distribution}
Let \(X\) and \(Y\) be as above, each having i.i.d.\ (real
or complex) Gaussian entries of size \(n\times t\) and
\(k\times t\), respectively, with \(n\le k \le t\).  Assume
further that \(X\) and \(Y\) are independent of each other
(this
is the null hypothesis in statistics).

Then the nonzero eigenvalues \(\lambda_1,\ldots,\lambda_n\) of the matrix \(P_X P_Y\) lie in the interval \([0,1]\) and have the joint density function
of the form
\[
  \prod_{i<j} |\lambda_i - \lambda_j|^\beta
  \;\prod_{i=1}^n
  \lambda_i^{\,\frac{\beta}{2}(k-n+1)\,-1}
  \,\bigl(1-\lambda_i\bigr)^{\,\frac{\beta}{2}(t-n-k+1)\,-1},
\]
up to a normalization constant that depends on \(n,k,t\) (but not on \(\{\lambda_i\}\)).
Here again \(\beta=1\) for the real case and \(\beta=2\) for the complex case.
\end{theorem}
This distribution is called the \emph{Jacobi} (or \emph{MANOVA}, or \emph{CCA}) ensemble, and it is also sometimes called the
\emph{multivariate Beta distribution}.
In point processes, the $\beta=2$ case is often referred to as the
\emph{Jacobi orthogonal polynomial ensemble}.

\begin{remark}
The derivation is again parallel to that in the GOE/GUE context, but one now keeps track of the row spaces and the relevant rectangular dimensions.  The matrix \((X\,X^\top)\) (or \((X\,X^\dagger)\)) is invertible with high probability whenever \(n\le t\) and \(X\) is in general position.  The distribution above reflects the geometry of overlapping projectors in a higher-dimensional space \(\mathbb{R}^t\) (or \(\mathbb{C}^t\)).
\end{remark}


\subsection{General Pattern and \texorpdfstring{\(\beta\)}{beta}-Ensembles}
\label{sec:general_pattern_log_gas}

We have now seen three classical examples:
\begin{enumerate}[\(\bullet\)]
\item \emph{Wigner (Gaussian) ensembles} (real/complex/quaternionic),
\item \emph{Wishart/Laguerre ensembles} \(W = X X^\top\),
\item \emph{Jacobi/MANOVA/CCA ensembles}.
\end{enumerate}
Their eigenvalue densities (ordered or unordered) always display the same building blocks:
\[
  \prod_{1\le i<j\le n}
  |\lambda_i - \lambda_j|^{\beta}
  \;\times\;
  \prod_{i=1}^n V(\lambda_i),
\]
where \(\beta\) indicates the real (\(\beta=1\)), complex (\(\beta=2\)), or symplectic (\(\beta=4\)) symmetry class, and \(V(\lambda)\) is a single-variable potential function.
Such distributions are often referred to as \emph{\(\beta\)-ensembles} or \emph{log-gases}, reflecting that the factor \(\prod_{i<j}|\lambda_i - \lambda_j|^\beta\) can be interpreted as the Boltzmann weight for charges with a logarithmic pairwise repulsion.

\begin{remark}
	Beyond these three classical families, there are many other \emph{matrix models}
and \emph{discrete distributions}
whose eigenvalues produce similar log-gas structures but with different potentials \(V(\lambda)\).  These share many of the same techniques and phenomena (e.g.\ local eigenvalue statistics, largest-eigenvalue asymptotics, etc.) that appear throughout modern random matrix theory.
\end{remark}

\begin{remark}
For $\beta=2$, the connection to orthogonal polynomials
suggests discrete models of log-gases, which are powered by most
known orthogonal polynomials in one variable from the
(q-)\emph{Askey scheme} \cite{Koekoek1996}. For example,
the model of (uniformly random) lozenge tilings of the hexagon is
connected to Hahn orthogonal polynomials \cite{gorin2021lectures}
whose orthogonality weight is the classical
hypergeometric distribution from probability theory.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tridiagonal form for real symmetric matrices}
\label{sec:householder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Any real symmetric matrix can be orthogonally transformed into a tridiagonal matrix. This fact is standard in numerical linear algebra (the ``Householder reduction'') and also central in random matrix theory---notably in the Dumitriu--Edelman approach \cite{dumitriu2002matrix} for Gaussian ensembles.

\subsection{Statement}

\begin{theorem}
\label{thm:tridiagonal}
Any real symmetric matrix \(W \in \mathbb{R}^{n\times n}\) can be represented as
\[
  W \;=\; Q^\top\, T\, Q,
  \quad
  Q \in O(n),
\]
where \(T\) is real symmetric tridiagonal.  Concretely, \(T\) has nonzero entries only on the main diagonal and the first super-/sub-diagonals:
\[
  T \;=\;
  \begin{pmatrix}
         d_1 & \alpha_1 & 0 & \cdots & 0\\
         \alpha_1 & d_2 & \alpha_2 & \cdots & 0\\
         0 & \alpha_2 & d_3 & \ddots & \vdots\\
         \vdots & \vdots & \ddots & \ddots & \alpha_{n-1}\\
         0 & 0 & \cdots & \alpha_{n-1} & d_n
  \end{pmatrix}.
\]
\end{theorem}

\subsection{Householder Reflections: A Detailed Algorithm}

\begin{definition}[Householder Reflection]
A \emph{Householder reflection} in \(\mathbb{R}^n\) is a matrix \(H\) of the form
\[
  H \;=\; I \;-\; 2\,\frac{v\,v^\top}{\|v\|^2},
  \quad
  v\in \mathbb{R}^n \text{ nonzero}.
\]
One checks that \(H^\top = H\), \(H^2 = I\), and \(H\) is orthogonal (i.e.\ \(H^\top H = I\)).  Geometrically, \(H\) is the reflection across the hyperplane orthogonal to~\(v\).
\end{definition}

\medskip
\noindent
\textbf{Goal.}
We want to apply successive Householder reflections to ``zero out'' all entries below the first subdiagonal (and by symmetry, above the first superdiagonal), leaving only the tridiagonal part possibly nonzero.

\begin{enumerate}[1.]
\item \textbf{Start with }\(W^{(0)} = W\).

\item \textbf{Step \(k=1\).}
   We aim to zero out the entries \(W^{(0)}_{2,1}, W^{(0)}_{3,1}, \dots, W^{(0)}_{n,1}\).  Define the vector
   \[
     x \;=\; \bigl(W^{(0)}_{2,1},\,W^{(0)}_{3,1},\,\dots,\,W^{(0)}_{n,1}\bigr)^\top
     \;\in\;\mathbb{R}^{\,n-1}.
   \]
   We then embed \(x\) into a vector \(\tilde{x} \in \mathbb{R}^{n}\) by placing \(0\) in the top coordinate:
   \[
     \tilde x \;=\; \bigl(0,\;W^{(0)}_{2,1},\;W^{(0)}_{3,1},\;\dots,\;W^{(0)}_{n,1}\bigr)^\top.
   \]
   To construct a Householder reflection that annihilates all but the first subdiagonal entry, choose
   \[
     v \;=\; \tilde x \;+\;\alpha\,e_1,
     \quad
     \alpha \;=\;\pm\,\|\tilde x\|,
   \]
   picking the sign of \(\alpha\) to avoid cancellation.  Then define
   \[
     H_1 \;=\; I \;-\; 2\,\frac{v\,v^\top}{\|v\|^2}.
   \]
   By construction, \(H_1\) is orthogonal and symmetric, and it will force all sub-subdiagonal entries below row~2 in column~1 to vanish when we do \(H_1\,W^{(0)}\,H_1\).

\item \textbf{Step \(k=2,\dots,n-2\).}
   Inductively, we zero out the \((k+2)\)-th through \(n\)-th entries in the \(k\)-th column (and by symmetry, the same row).  Each step uses a smaller Householder matrix \(H_k\) acting nontrivially in the lower-right \((n-k+1)\times(n-k+1)\) sub-block.  We set
   \[
     W^{(k)} \;=\; H_k \, W^{(k-1)} \, H_k.
   \]

\item \textbf{End result.}
   After \(n-2\) steps, we obtain \(W^{(n-2)}\), which is tridiagonal.  Define
   \[
     Q \;=\; H_1 \,H_2\,\cdots\,H_{n-2}.
   \]
   Since each \(H_k\) is orthogonal, \(Q\in O(n)\).  We see
   \[
     W^{(n-2)}
     \;=\;
     (H_{n-2}\!\cdots H_1)\,\bigl[\,W\,\bigr]\,(H_1\!\cdots H_{n-2}),
   \]
   so
   \[
     W^{(n-2)}
     \;=\;
     Q \,W \,Q^\top
   \]
   has the desired tridiagonal form.
\end{enumerate}

\begin{remark}
This Householder procedure is also used in practical numerical methods for eigenvalue computations: once a real symmetric matrix is reduced to tridiagonal form, specialized algorithms (such as the QR algorithm) can then be applied more efficiently.
\end{remark}

\begin{proof}[Proof of \Cref{thm:tridiagonal}]
The algorithmic construction above completes the argument.  At each step, note that
\(\bigl[H\,(\text{symmetric})\,H\bigr]\) remains symmetric because \(H\) itself is symmetric.  Moreover, each step enforces the vanishing of sub-subdiagonal (and super-subdiagonal) entries in the targeted row and column.  In the end, we get the desired real symmetric tridiagonal matrix
\(\,W^{(n-2)}\).  By construction, the product \(Q = \prod_{k=1}^{n-2}H_k\) is orthogonal, giving \(W^{(n-2)} = Q\,W\,Q^\top\).
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wigner’s Semicircle Law via Tridiagonalization}
\label{sec:Wigner-SC-detailed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now present a more detailed outline of how one proves the Wigner semicircle law for the GOE by using its \emph{random tridiagonal model}. This method, due to Dumitriu and Edelman \cite{dumitriu2002matrix}, is often viewed as more direct than Wigner’s original moment method.

\subsection{Dumitriu–Edelman Tridiagonal Model}

\begin{theorem}[Tridiagonal Representation of GOE]
\label{thm:DE-model}
Let \(W\) be an \(n\times n\) GOE matrix (real symmetric) with variances chosen so that each off-diagonal entry has variance \(1/2\) and each diagonal entry has variance \(1\).  Then there exists an orthogonal matrix \(Q\) such that
\[
   W \;=\; Q^\top\,T\,Q,
\]
where \(T\) is a real symmetric tridiagonal matrix of the special form
\[
   T \;=\; \begin{pmatrix}
         d_1 & \alpha_1 & 0 & \cdots \\
         \alpha_1 & d_2 & \alpha_2 & \ddots \\
         0 & \alpha_2 & d_3 & \ddots \\
         \vdots & \ddots & \ddots & \ddots
       \end{pmatrix},
\]
and the random variables \(\{d_i,\alpha_j\}_{1 \le i \le n,\;1\le j\le n-1}\) are mutually independent, with
\[
  d_i \,\sim\, \mathcal{N}(0,1),
  \qquad
  \alpha_j \;=\; \sqrt{\frac{\chi^2_{\,n-j}}{2}},
\]
where \(\chi^2_{\nu}\) is a chi-square distribution with \(\nu\) degrees of freedom.  Equivalently, \(\sqrt{\tfrac{\chi^2_\nu}{2}}\) is (up to a factor) the norm of a Gaussian vector in~\(\mathbb{R}^\nu\).
\end{theorem}

\begin{remark}
To restate succinctly:
\begin{itemize}
\item The diagonal entries \(d_1,\dots,d_n\) are i.i.d.\ \(\mathcal{N}(0,1)\).
\item The subdiagonal entries \(\alpha_1,\dots,\alpha_{n-1}\) are independent, each distributed as \(\sqrt{\tfrac{1}{2}\chi^2_{n-j}}\).
\item All higher off-diagonal entries above the first superdiagonal are zero, so \(T\) has only \(2n-1\) nontrivial entries.
\end{itemize}
\end{remark}

\begin{proof}[Sketch of Construction]
This construction is essentially a specialized version of the Householder reduction in \Cref{sec:householder}, set up so that each step matches precisely the distributions \(\alpha_j\sim \sqrt{\tfrac{\chi^2_{n-j}}{2}}\) and \(d_i\sim\mathcal{N}(0,1)\).  One uses the rotational invariance of Gaussian matrices to ensure at each step that the ``residual vector'' is isotropic.  The norm of that vector yields the \(\chi^2\)-type variables.  Full details appear in \cite{dumitriu2002matrix} and advanced RMT texts.
\end{proof}

Thus, to study the eigenvalues of a GOE matrix \(W\), one can equivalently study the (much sparser) random tridiagonal matrix \(T\).

\subsection{Characteristic Polynomial and Three-Term Recurrence}

Consider \(p_n(\lambda) = \det(T - \lambda I)\).  Because \(T\) is tridiagonal, we have the classical three-term recurrence for these characteristic polynomials:
\[
  p_0(\lambda) := 1,\quad
  p_1(\lambda) := d_1 - \lambda,
\]
\[
  p_{k+1}(\lambda)
  \;=\;
  (d_{k+1} - \lambda)\,p_k(\lambda)
  \;-\;\alpha_k^2\,p_{k-1}(\lambda),
  \quad
  (k=1,\dots,n-1).
\]
The eigenvalues of \(T\) are precisely the roots of \(p_n(\lambda)\).

\subsection{Sketch of the Semicircle Limit Proof}

We want to show that the empirical distribution
\[
  L_n
  \;=\;
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i}
\]
(where \(\lambda_1,\dots,\lambda_n\) are the eigenvalues of \(T\)) converges weakly to the semicircle law
\[
  \mu_{\mathrm{sc}}(dx)
  \;=\;
  \frac{1}{2\pi}\sqrt{4 - x^2}\,\mathbf{1}_{|x|\le 2}\,dx
\]
as \(n\to\infty\).  A typical outline:

\begin{enumerate}[1.]
\item \textbf{Law of Large Numbers for \(\alpha_j\).}
   Since \(\alpha_j^2 = \tfrac12\,\chi^2_{\,n-j}\) has mean \(\tfrac{n-j}{2}\), it is typically of order \(n/2\).  More precisely, for large \(n\), \(\alpha_j \approx \sqrt{\tfrac{n-j}{2}}\) with high probability.

\item \textbf{Scaling by \(\sqrt{n}\).}
   One rescales \(T\) by \(\tfrac{1}{\sqrt{n}}\).  This gives subdiagonal entries
   \[
     \frac{\alpha_j}{\sqrt{n}}
     \;\approx\;
     \sqrt{\frac{n-j}{2n}}
     \;\approx\;
     \sqrt{\frac{1 - j/n}{2}},
   \]
   while the diagonal entries become \(\tfrac{d_i}{\sqrt{n}}\), which vanish in the large-\(n\) limit.  So effectively, the subdiagonal structure drives the main spectral behavior in the bulk, producing the semicircle shape in the limit.

\item \textbf{Orthogonal Polynomial / Recurrence Analysis.}
   The polynomial \(p_n(\lambda)\) satisfies a discrete three-term recurrence whose ``continuum limit'' yields a certain integral equation (specifically the Stieltjes transform for the measure) whose solution is precisely the semicircle distribution.  In more detailed treatments, one shows that the moments or the Cauchy transform of \(L_n\) converge to that of \(\mu_{\mathrm{sc}}\).  The relevant PDE or integral equation is exactly solvable, producing the semicircle.

\end{enumerate}

Hence, with probability 1, as \(n\to\infty\), the empirical spectrum of \(\tfrac{1}{\sqrt{n}}\,W\) converges to the semicircle distribution on \([-2,2]\).  This precisely recovers \emph{Wigner’s semicircle law}.

\begin{remark}[Extensions]
A very similar approach works for the Gaussian Unitary Ensemble (\(\beta=2\)), leading to a random \emph{complex Hermitian} tridiagonal matrix.  For \(\beta=4\), there is a quaternionic block-tridiagonal model.  All of these point toward the same semicircle law for the global spectral distribution.
\end{remark}



\appendix
\setcounter{section}{2}

\section{Problems (due 2025-02-22)}

\subsection{Invariance of GOE and GUE}
\label{prob:invariance_GOE_GUE}

Show that the distribution of the GOE and GUE is
invariant under, respectively, orthogonal and unitary conjugation.
For GOE, this means that if \(W\)
is a random GOE matrix and \(Q\) is a fixed orthogonal
matrix of order $n$, then the distribution
of \(QWQ^\top\) is the same as the distribution of \(W\).
(Similarly for GUE.)

\medskip
\noindent
Hint: write the joint density of all entries of GOE/GUE (for instance, GOE
is determined by $n(n+1)/2$ real random independent variables)
in a coordinate-free way.


\subsection{Preimage size for spectral decomposition}
\label{prob:GOE-preimage}

Show that for a real symmetric matrix $W$ with distinct eigenvalues,
if $W=Q\Lambda Q^\top$ is its spectral decomposition
where $Q$ is orthogonal and $\Lambda=\mathrm{diag}(\lambda_1,\ldots,\lambda_n)$
is diagonal with $(\lambda_1\ge \cdots\ge \lambda_n)$,
then there are exactly $2^n$ different choices of $Q$
that give the same matrix $W$.

\subsection{Distinct eigenvalues}
\label{prob:distinct-eigenvalues}

Show that under GOE and GUE, almost surely,
all eigenvalues are distinct.

\subsection{Jacobian for GUE}
\label{prob:Jacobian-GUE}

Arguing similarly to
\Cref{subsec:jacobian},
show that the Jacobian for the spectral decomposition
of a complex Hermitian matrix is proportional to
\begin{equation*}
	\prod_{1\le i<j\le n}|\lambda_i-\lambda_j|^2.
\end{equation*}
In particular, make sure you understand
where the factor $2$ comes from in the complex case.

\subsection{Normalization for GOE}
\label{prob:GOE-normalization}

Compute the $n$-dimensional integral
(in the ordered on unordered form):
\begin{multline*}
	\int_{\lambda_1<\ldots<\lambda_n } \prod_{i<j}(\lambda_i-\lambda_j)
	\ssp
	\exp\Bigl(-\tfrac{1}{2}\sum_{k=1}^n \lambda_k^2\Bigr)\,d\lambda_1\cdots d\lambda_n.\\=
	\frac{1}{n!}
	\int_{\mathbb{R}^n}
	\prod_{i<j}|\lambda_i-\lambda_j|\ssp
	\exp\Bigl(-\tfrac{1}{2}\sum_{k=1}^n \lambda_k^2\Bigr)\,d\lambda_1\cdots d\lambda_n.
\end{multline*}



\subsection{Wishart eigenvalue density}

Prove \Cref{thm:Wishart-Distribution} (in the real case $\beta=1$)
by using the singular
value
decomposition
of \(X\) and the properties of the Wishart ensemble.


























\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

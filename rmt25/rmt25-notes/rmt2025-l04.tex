\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{example}[proposition]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newenvironment{lnotes}{\section*{Notes for the lecturer}}{}
% \excludecomment{lnotes}


\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 4: Title TBD}


\date{Wednesday, January 29, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l04.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle


\begin{lnotes}

	Add / finish up the discussion about tridiagonalization. The notes for L3 are updated, but
	mention it here.

	The reflection is mapping any vector into any other vector (unit vectors);
	Also, we apply it in $(n-1)$-dimensional space in the first pass.


\colorbox{yellow}{\parbox{.7\textwidth}{maybe make a simulation for Wishart and MANOVA in L4}}


\subsection{Characteristic Polynomial and Three-Term Recurrence}

Consider \(p_n(\lambda) = \det(T - \lambda I)\).  Because \(T\) is tridiagonal, we have the classical three-term recurrence for these characteristic polynomials:
\[
  p_0(\lambda) := 1,\quad
  p_1(\lambda) := d_1 - \lambda,
\]
\[
  p_{k+1}(\lambda)
  \;=\;
  (d_{k+1} - \lambda)\,p_k(\lambda)
  \;-\;\alpha_k^2\,p_{k-1}(\lambda),
  \quad
  (k=1,\dots,n-1).
\]
The eigenvalues of \(T\) are precisely the roots of \(p_n(\lambda)\).

\subsection{Sketch of the Semicircle Limit Proof}

We want to show that the empirical distribution
\[
  L_n
  \;=\;
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i}
\]
(where \(\lambda_1,\dots,\lambda_n\) are the eigenvalues of \(T\)) converges weakly to the semicircle law
\[
  \mu_{\mathrm{sc}}(dx)
  \;=\;
  \frac{1}{2\pi}\sqrt{4 - x^2}\,\mathbf{1}_{|x|\le 2}\,dx
\]
as \(n\to\infty\).  A typical outline:

\begin{enumerate}[1.]
\item \textbf{Law of Large Numbers for \(\alpha_j\).}
   Since \(\alpha_j^2 = \tfrac12\,\chi^2_{\,n-j}\) has mean \(\tfrac{n-j}{2}\), it is typically of order \(n/2\).  More precisely, for large \(n\), \(\alpha_j \approx \sqrt{\tfrac{n-j}{2}}\) with high probability.

\item \textbf{Scaling by \(\sqrt{n}\).}
   One rescales \(T\) by \(\tfrac{1}{\sqrt{n}}\).  This gives subdiagonal entries
   \[
     \frac{\alpha_j}{\sqrt{n}}
     \;\approx\;
     \sqrt{\frac{n-j}{2n}}
     \;\approx\;
     \sqrt{\frac{1 - j/n}{2}},
   \]
   while the diagonal entries become \(\tfrac{d_i}{\sqrt{n}}\), which vanish in the large-\(n\) limit.  So effectively, the subdiagonal structure drives the main spectral behavior in the bulk, producing the semicircle shape in the limit.

\item \textbf{Orthogonal Polynomial / Recurrence Analysis.}
   The polynomial \(p_n(\lambda)\) satisfies a discrete three-term recurrence whose ``continuum limit'' yields a certain integral equation (specifically the Stieltjes transform for the measure) whose solution is precisely the semicircle distribution.  In more detailed treatments, one shows that the moments or the Cauchy transform of \(L_n\) converge to that of \(\mu_{\mathrm{sc}}\).  The relevant PDE or integral equation is exactly solvable, producing the semicircle.

\end{enumerate}

Hence, with probability 1, as \(n\to\infty\), the empirical spectrum of \(\tfrac{1}{\sqrt{n}}\,W\) converges to the semicircle distribution on \([-2,2]\).  This precisely recovers \emph{Wigner’s semicircle law}.

\begin{remark}[Extensions]
A very similar approach works for the Gaussian Unitary Ensemble (\(\beta=2\)), leading to a random \emph{complex Hermitian} tridiagonal matrix.  For \(\beta=4\), there is a quaternionic block-tridiagonal model.  All of these point toward the same semicircle law for the global spectral distribution.
\end{remark}


\end{lnotes}





\section{Recap}

\subsection{Gaussian ensembles}

We introduced Gaussian ensembles,
and for GOE ($\beta=1$) we computed the joint eigenvalue density.
The normalization is so that 

\subsection{Tridiagonalization}

We showed that any real symmetric matrix \(A\) can be tridiagonalized by an orthogonal transformation \(Q\):
\[
	Q^\top\,A\,Q \;=\; T,
\]
where \(T\) is real symmetric tridiagonal, having nonzero entries only on the main diagonal and the first super-/subdiagonals:
\begin{equation*}
	T \;=\;
	\begin{pmatrix}
		 d_1 & \alpha_1 & 0 & \cdots & 0\\
		 \alpha_1 & d_2 & \alpha_2 & \cdots & 0\\
		 0 & \alpha_2 & d_3 & \ddots & \vdots\\
		 \vdots & \vdots & \ddots & \ddots & \alpha_{n-1}\\
		 0 & 0 & \cdots & \alpha_{n-1} & d_n
	\end{pmatrix}.
\end{equation*}
In the proof, each time we need to act in the orthogonal complement to the 
subspace $e_1,\ldots,e_{k-1}$ (starting from $e_1$),
and apply a Householder reflection to zero out everything strictly 
below the subdiagonal. (We apply the transformations like
$A\mapso H A H^{\top}$, so that the first row transforms 
in the same way as the first column of $A$).




























In this extended lecture, we continue our discussion from Lecture 3 on the role of tridiagonalization in random matrix theory. Our goals are:

\begin{itemize}
\item {\bf Householder reflections and real symmetric matrices.} We recall that any real symmetric (or Hermitian) matrix can be conjugated to a tridiagonal form using orthogonal (or unitary) transformations. This forms the starting point for efficient eigenvalue analysis in numerical linear algebra.
\item {\bf The three-term recurrence.} We derive the characteristic polynomial of a tridiagonal matrix and see how it satisfies a classical three-term recurrence relation.
\item {\bf The Dumitriu--Edelman approach.} We focus on how this tridiagonal representation is particularly useful for Gaussian (and more general) Wigner ensembles, giving a streamlined path to Wigner’s semicircle law.
\item {\bf Additional discussions: Stieltjes transform, PDE perspective, and simulations.} We outline how one can interpret the limiting spectral measure via the tridiagonal form in moment-based or Green’s function/Stieltjes transform approaches. We also briefly touch on simulating Wishart and Jacobi (MANOVA) ensembles.
\end{itemize}

This lecture is intended to be more detailed, matching the overall thoroughness of Lecture 3, and providing deeper insight into both the theoretical and computational aspects of random tridiagonal matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tridiagonalization: Householder Reflections Revisited}
\label{sec:householder}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We start with a recap of the Householder reflection procedure from Lecture 3, adding expanded discussion and examples.

\subsection{Definition and Properties of Householder Reflections}

\begin{definition}[Householder Reflection]
A \emph{Householder reflection} in $\R^n$ about a vector $v \in \R^n \setminus\{0\}$ is given by
\[
  H \;=\;
  I \;-\; 2\,\frac{v\,v^\top}{\|v\|^2}.
\]
\begin{enumerate}[(1)]
\item $H$ is orthogonal: $H^\top H = I$.
\item $H$ is symmetric: $H^\top = H$.
\item $H$ has determinant $-1$, representing a reflection in $\R^n$.
\item $H$ reflects vectors across the hyperplane orthogonal to $v$.
\end{enumerate}
\end{definition}

\begin{remark}
Using this construction, one can map any nonzero vector $x$ to a desired target vector, e.g.\ $e_1$, while preserving distances and angles in the subspace orthogonal to $v$.
\end{remark}

\subsection{Orthogonal Reduction to Tridiagonal}

\begin{theorem}[Tridiagonalization of Real Symmetric Matrices]
\label{thm:tridiagonal}
Let $A\in \R^{n\times n}$ be a real symmetric matrix. Then there exists an orthogonal matrix $Q$ such that
\[
  Q^\top\,A\,Q \;=\; T,
\]
where $T$ is real symmetric \emph{tridiagonal}, having nonzero entries only on the main diagonal and the first super-/subdiagonals:
\[
  T \;=\;
  \begin{pmatrix}
     d_1 & \alpha_1 & 0 & \cdots & 0\\
     \alpha_1 & d_2 & \alpha_2 & \cdots & 0\\
     0 & \alpha_2 & d_3 & \ddots & \vdots\\
     \vdots & \vdots & \ddots & \ddots & \alpha_{n-1}\\
     0 & 0 & \cdots & \alpha_{n-1} & d_n
  \end{pmatrix}.
\]
\end{theorem}

\begin{proof}[Sketch]
The proof proceeds by induction on $n$. For the $n\times n$ matrix $A$:

\noindent
{\bf Step 1.} Construct a Householder reflection $H_1$ that affects only the subspace spanned by $\{a_{21},a_{31},\dots,a_{n1}\}$. Conjugate $A$ by $H_1$, producing $H_1 A H_1^\top$. By symmetry, this zeros out all but one subdiagonal entry in the first column and, by the same token, the corresponding superdiagonal entries in the first row.

\noindent
{\bf Step 2.} Restrict attention to the trailing $(n-1)\times(n-1)$ principal submatrix. Repeat with $H_2$, $H_3,\dots,H_{n-2}$, each chosen to zero out subdiagonal entries outside the main tridiagonal band in the subsequent minor.

\noindent
{\bf Step 3.} The product $Q := H_1 H_2 \cdots H_{n-2}$ is orthogonal, and $T := Q^\top A Q$ is tridiagonal and real symmetric. This completes the construction.
\end{proof}

\begin{remark}[Complex Hermitian Case]
For complex Hermitian matrices, an analogous procedure using \emph{Householder unitary} transformations (or sometimes “Householder reflectors” in $\C^n$) reduces the matrix to a \emph{complex Hermitian} tridiagonal form. A quaternions-based approach exists for the symplectic case, though it is more intricate.
\end{remark}

\subsection{Further Discussion: Numerical Linear Algebra Perspective}

\begin{itemize}
\item \emph{QR Algorithm on $T$.} Once in tridiagonal form, standard eigenvalue algorithms (e.g.\ the QR algorithm with shifts) are much more stable and faster compared to working on the full dense matrix.
\item \emph{Floating Point Complexity.} Each Householder reflection in $n$ dimensions typically costs $O(n^2)$ operations, and reducing a matrix to tridiagonal form costs $O(n^3)$ in general.
\item \emph{Sparse / Banded Extensions.} If $A$ is originally banded or sparse, special care can reduce the complexity further, but the crux remains that tridiagonalization is a major step toward practical eigen-decomposition.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Characteristic Polynomial and Three-Term Recurrence}
\label{sec:3term}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Once we have $T = (t_{ij})$ tridiagonal, the characteristic polynomial of $T$ takes a well-known form governed by a three-term recurrence. Denote
\[
  T - \lambda I \;=\;
  \begin{pmatrix}
    d_1 - \lambda & \alpha_1 & 0 & \cdots & 0 \\
    \alpha_1 & d_2 - \lambda & \alpha_2 & \cdots & 0 \\
    0        & \alpha_2 & d_3 - \lambda & \ddots & \vdots \\
    \vdots   & \vdots   & \ddots & \ddots & \alpha_{n-1} \\
    0        & 0        & \cdots & \alpha_{n-1} & d_n - \lambda
  \end{pmatrix}.
\]

\begin{definition}[Characteristic Polynomials $p_k(\lambda)$]
For $1 \le k \le n$, let $T_k$ be the top-left $k \times k$ submatrix of $T$. Define
\[
  p_k(\lambda) \;=\; \det\bigl(T_k - \lambda I_k\bigr).
\]
Moreover, set $p_0(\lambda):=1$ by convention.
\end{definition}

\begin{lemma}[Three-Term Recurrence]
\label{lem:3term-recurrence}
Let $p_k(\lambda)$ be as above, with $p_1(\lambda) = d_1 - \lambda$. Then for $k \ge 1$,
\[
  p_{k+1}(\lambda)
  \;=\;
  (d_{k+1} - \lambda)\,p_k(\lambda)
  \;-\;\alpha_k^2\,p_{k-1}(\lambda).
\]
\end{lemma}

\begin{proof}[Idea of proof]
One checks the base case $k=1,2$ directly by computing determinants of $1\times1$ and $2\times2$ blocks. For the general step, expand $\det(T_{k+1}-\lambda I_{k+1})$ by minors along the last row or column. The block structure ensures exactly the claimed recurrence.
\end{proof}

\begin{remark}
This is analogous to recurrences in orthogonal polynomials, e.g.\ the three-term recursion for polynomials orthonormal with respect to certain measures. In fact, the polynomial $p_n(\lambda)$ can be viewed as an orthogonal polynomial if $\alpha_j>0$. This interplay is vital in random matrix theory.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dumitriu--Edelman Tridiagonal Model for Wigner Ensembles}
\label{sec:dumitriu-edelman}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An influential result by Dumitriu and Edelman (2002) shows that if you have a real Wigner matrix $W$ with the “right” Gaussian and $\chi$-distribution structure, then in the process of Householder reduction to a tridiagonal matrix, the diagonal and subdiagonal entries themselves become \emph{mutually independent} random variables. This leads to a direct handle on eigenvalue asymptotics.

\subsection{Construction}

\begin{theorem}[Dumitriu--Edelman Representation]
\label{thm:DE-representation}
Let $W$ be an $n \times n$ real Wigner matrix whose entries are:
\begin{enumerate}[(i)]
\item Off-diagonal entries $X_{ij}$, for $i<j$, are iid $\mathcal{N}(0,1)$.
\item Diagonal entries $X_{ii}$ are iid $\mathcal{N}(0,2)$.
\end{enumerate}
Then there is an orthogonal matrix $Q \in O(n)$ such that
\[
  W \;=\; Q^\top \, T\, Q,
\]
where $T$ is real symmetric tridiagonal, with:
\begin{itemize}
\item Diagonal entries $d_i \sim \mathcal{N}(0,1)$, i.i.d.
\item Subdiagonal entries $\displaystyle \alpha_j = \sqrt{\frac12 \,\chi^2_{(n-j)}}$, independent over $j=1,\ldots,n-1$.
\end{itemize}
\end{theorem}

\begin{remark}[General $\beta$-Ensembles]
For $\beta>0$, one can construct a tridiagonal matrix $T_\beta$ with diagonal entries $d_i \sim \mathcal{N}(0,2/\beta)$ and subdiagonal entries $\alpha_j = \sqrt{\frac{1}{\beta}\chi^2_{\beta(n-j)}}$, all independent, whose eigenvalues match the “Gaussian $\beta$-ensemble.” In classical settings $\beta=1,2,4$, these correspond to GOE, GUE, and GSE, respectively.
\end{remark}

\subsection{Key Points in the Proof}

\begin{itemize}
\item The Householder transformation at each step is chosen so that the random “residual vector” is isotropic, enabling the reflection to produce $\chi$-type random lengths.
\item By induction, one shows that each subdiagonal element of $T$ is the length of a Gaussian vector in $\R^{n-j}$, leading to $\chi_{n-j}$ distributions, and that each step is independent from the preceding ones.
\end{itemize}

\begin{remark}
This tridiagonal structure is extremely sparse compared to the original dense Wigner matrix but carries the entire spectral data. The independence among $d_i$ and $\alpha_j$ is the critical simplification.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semicircle Law via the Tridiagonal Form}
\label{sec:semicircle-tridiag}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now present a fairly detailed sketch (or roadmap) for how the tridiagonal form yields the Wigner semicircle law. Recall we aim to show:

\[
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i/\sqrt{n}}
  \;\longrightarrow\;
  \mu_{\mathrm{sc}}(dx)\;=\;\frac{1}{2\pi}\sqrt{4 - x^2}\;\1_{|x|\le2}\,\dd x
  \quad\text{almost surely}.
\]

\subsection{The Scaling and Law of Large Numbers in \(\alpha_j\)}

Write
\[
  T = \begin{pmatrix}
    d_1 & \alpha_1 & & \\
    \alpha_1 & d_2 & \alpha_2 & \\
    & \alpha_2 & d_3 & \ddots \\
    & & \ddots & \ddots
  \end{pmatrix}.
\]
In the Dumitriu--Edelman setting, $\alpha_j^2 = \frac{1}{2}\,\Chi^2_{(n-j)}$, so $\E[\alpha_j^2]=(n-j)/2$ and $\alpha_j^2$ is tightly concentrated around its mean for large $n$. Precisely, for any $\varepsilon>0$,
\[
  \Pr\Bigl(\bigl|\alpha_j^2 - \tfrac{n-j}{2}\bigr| > \varepsilon (n-j)\Bigr)
  \;\le\; e^{-c(n-j)}
\]
for some $c>0$, or a similar bound from large deviation estimates. Hence, with overwhelming probability,
\[
  \alpha_j \;\approx\; \sqrt{\frac{n-j}{2}}.
\]

\subsection{Diagonal vs.\ Subdiagonal Entries}

When we examine $\frac{1}{\sqrt{n}}\,T$, the diagonal entries become $\frac{d_i}{\sqrt{n}}$. Since $d_i \sim \mathcal{N}(0,1)$ i.i.d., with probability going to 1 these entries lie within $O(n^{-1/2})$, so they vanish in the large-$n$ limit. Meanwhile,
\[
  \frac{\alpha_j}{\sqrt{n}}
  \;\approx\;
  \sqrt{\frac{n-j}{2n}}
  \;\approx\;
  \sqrt{\frac{1 - j/n}{2}}.
\]
In the bulk region (i.e.\ $j\approx \theta n$ for $\theta\in (0,1)$), we have $\alpha_j/\sqrt{n} \approx \sqrt{\frac{1-\theta}{2}}$. In short, the subdiagonal elements (scaled by $1/\sqrt{n}$) remain of order $1$, while the diagonal elements vanish.

\subsection{Characteristic Polynomial and Recurrence Analysis}

Denote
\[
  p_n(\lambda) \;=\;
  \det\Bigl(\tfrac1{\sqrt{n}}\,T - \lambda I\Bigr).
\]
Equivalently, $\lambda$ is an eigenvalue of $\frac{1}{\sqrt{n}}\,T$ if and only if $\mu = \sqrt{n}\,\lambda$ is an eigenvalue of $T$. We want to understand the distribution of the roots $\lambda_i(\frac{1}{\sqrt{n}}\,T)$ as $n\to\infty$. The three-term recurrence for $p_n(\lambda)$ can be viewed in the limit as $n\to\infty$, turning into an integral equation for the Stieltjes transform of the limiting measure.

\paragraph{Stieltjes Transform Argument (Sketch).}
Set
\[
  G_n(z)
  = \frac{1}{n}\Tr\Bigl(\bigl(\tfrac{1}{\sqrt{n}}\,T - z\Bigr)^{-1}\Bigr),
\]
the Stieltjes transform. As $n$ grows, the main input is that $\alpha_j\approx \sqrt{\frac{1-j/n}{2}}$; substituting this approximate profile into the recursion for Green’s functions (a linear difference equation akin to orthogonal polynomials) yields a limiting functional equation. Solving that equation for $G(z)$ leads to
\[
  G(z) = \frac{z \pm \sqrt{z^2-4}}{2},
\]
with the appropriate branch cut. This is the well-known Stieltjes transform for the semicircle law on $[-2,2]$. (See advanced texts, e.g.\ \emph{Deift} or \emph{Tao’s} books on RMT for a full rigorous derivation.)

\paragraph{Moment Argument (Sketch).}
One can also proceed by computing or bounding the moments $\E\bigl[\frac{1}{n}\Tr(\frac{1}{\sqrt{n}}\,T)^k\bigr]$ and showing that they match the semicircle moments. Indeed, as $n\to\infty$, the diagonal part becomes negligible, while the subdiagonal structure essentially forces closed loops in the sum expansions, reproducing the Catalan numbers that appear in the Wigner moment method (Lectures 1--2). The advantage of tridiagonalization is a simpler combinatorial interpretation of the entries used in each closed loop.

\subsection{Conclusion: Wigner’s Semicircle Law}

Putting these ingredients together, we conclude that with probability 1,
\[
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i(\frac1{\sqrt{n}}\,W)}
  \;\;\longrightarrow\;\;
  \mu_{\mathrm{sc}}(dx) \;=\; \frac{1}{2\pi}\sqrt{4-x^2}\;\1_{|x|\le2}\,\dd x.
\]
This completes the proof by tridiagonal methods.

\begin{remark}[Universality]
The argument here specifically used the Gaussian Wigner distribution for off-diagonal entries to get an explicit $\chi^2$ structure. However, the final result (semicircle law) remains true under vastly weaker assumptions on the entries. The \emph{universality principle} states that the global spectral behavior is insensitive to fine details of the distribution (e.g.\ it depends primarily on the first two moments).
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Discussions}
\label{sec:extended-discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section offers broader perspectives and more detailed commentary on topics related to the tridiagonal approach.

\subsection{General \texorpdfstring{$\beta$}{beta}-Ensembles and Log-Gases}

The Dumitriu--Edelman framework extends naturally to general $\beta$-ensembles (where $\beta>0$). In that model, one has a random tridiagonal matrix
\[
  T_\beta
  =
  \begin{pmatrix}
   d_1 & \alpha_1 & 0 & \cdots \\
   \alpha_1 & d_2 & \alpha_2 & \ddots \\
   0 & \alpha_2 & d_3 & \ddots \\
   \vdots & \ddots & \ddots & \ddots
  \end{pmatrix}
\]
with $d_i \sim N(0,2/\beta)$ and
\[
  \alpha_j
  = \sqrt{\frac{1}{\beta}\,\chi^2_{\bigl(\beta(n-j)\bigr)}},
  \quad j=1,\ldots,n-1,
\]
all independent. The joint law of eigenvalues is exactly the \emph{Gaussian $\beta$-ensemble}:
\[
  p(\lambda_1,\dots,\lambda_n)
  \;\propto\;
  \exp\Bigl(-\frac{\beta}{4}\sum_{i=1}^n \lambda_i^2\Bigr)
  \;\prod_{i<j}|\lambda_i - \lambda_j|^\beta,
\]
which generalizes GOE ($\beta=1$), GUE ($\beta=2$), and GSE ($\beta=4$).

\subsection{PDE Approach to the Semicircle}

An alternative viewpoint is to treat the density of eigenvalues as evolving under a certain “continuum limit of discrete recursion,” which can be recast as a partial differential equation. In particular, the moment generating function or Stieltjes transform satisfies an algebraic equation,
\[
  G(z)^2 + z\,G(z)+1=0,
\]
leading to the arcsine or semicircle distribution. While the PDE approach can be quite technical, it strongly parallels how Dyson Brownian motion arguments are used to derive the local spectral statistics.

\subsection{Weak vs.\ Almost Sure Convergence}

In many statements of the semicircle law, we see:
\[
  \mu_n = \frac1n \sum_{i=1}^n \delta_{\lambda_i/\sqrt{n}}
  \;\longrightarrow\;
  \mu_{\mathrm{sc}}
\]
either \emph{weakly in probability} or \emph{weakly almost surely}. The difference:
\begin{itemize}
\item \emph{Weak in probability:} For each bounded continuous test function $f$, the random variable $\int f\,\dd \mu_n$ converges in probability to $\int f\,\dd \mu_{\mathrm{sc}}$.
\item \emph{Weak almost sure:} Convergence occurs with probability 1, i.e.\ \(\int f\,\dd \mu_n \to \int f\,\dd \mu_{\mathrm{sc}}\) almost surely for all bounded $f$.
\end{itemize}
For the classical Wigner ensembles, the results typically hold in the stronger almost sure sense.

\subsection{A Note on Fluctuations Beyond the Law of Large Numbers}

While the global limit of the eigenvalue distribution is the semicircle, one can also ask about fluctuations around it, e.g.\ the central limit theorem for linear statistics $\frac{1}{n}\sum f(\lambda_i)$. There's an extensive theory describing these fluctuations: for example, we get Gaussian fluctuations in many cases, related to so-called \emph{trace identities} and the \emph{loop equations} in $\beta$-ensembles.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wishart and MANOVA: Simulation Discussions}
\label{sec:wishart-manova}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Although we have focused on Wigner (symmetric) matrices, very similar tridiagonal or block-tridiagonal models exist for:
\begin{enumerate}[(1)]
\item \emph{Wishart (Laguerre) ensembles:} $X$ is $n\times m$ (real Gaussian). Then $W=XX^\top$ is $n\times n$ and the eigenvalues $\lambda_i$ follow the Marchenko–Pastur distribution in the large-$n,m$ limit.
\item \emph{Jacobi (MANOVA) ensembles:} $X$ is $n\times t$ and $Y$ is $k\times t$, both real Gaussian. The matrix $\bigl(X X^\top + Y Y^\top\bigr)^{-1}(X X^\top)$ has eigenvalues in $[0,1]$ that follow the Jacobi Beta distribution.
\end{enumerate}

In practice, simulating these models is straightforward:

\begin{enumerate}[(a)]
\item Generate data $X$, $Y$ from Gaussian distributions.
\item Form $W$ or the relevant matrix $(X X^\top + Y Y^\top)^{-1} (X X^\top)$.
\item Compute eigenvalues via a standard routine (e.g.\ \texttt{numpy.linalg.eigvalsh} in Python).
\item Plot the empirical distribution of these eigenvalues; compare to known limiting PDFs such as Marchenko–Pastur or Jacobi Beta, which have known closed forms.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercises (Due 2025-02-28)}
\label{sec:exercises}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Below are problems elaborating on the main concepts. They range from verifying computations to exploring deeper aspects of tridiagonalization and simulations.

\subsection*{1. Detailed Householder Steps and Reflection Properties}

\begin{enumerate}[(a)]
\item {\bf Constructing a Reflection that Maps One Vector to Another.}
  Let $x,y\in \R^n$ be nonzero. Show how to pick $v$ so that $Hx=y$, where $H=I-2\frac{v\,v^\top}{\|v\|^2}$ is a Householder reflection. Why does $H$ remain orthogonal?
\item {\bf Eliminating Below-Diagonal Entries.}
  In the first step of the tridiagonalization algorithm, pick a reflection $H_1$ that modifies the subspace spanned by $a_{21},\dots,a_{n1}$. Show explicitly how $H_1$ zeroes out all these subdiagonal entries while keeping $a_{11}$ unchanged (aside from possibly a sign).
\item {\bf Symmetry and Superdiagonal.}
  Why does $H_1 (A) H_1^\top$ also have the corresponding superdiagonal entries zeroed out in row 1? (Hint: Use $A$ is symmetric: $A_{ij}=A_{ji}$.)
\end{enumerate}

\subsection*{2. Three-Term Recurrence Warmup}

\begin{enumerate}[(a)]
\item {\bf Base Cases.} Compute $p_1(\lambda)$ and $p_2(\lambda)$ for a $2\times2$ matrix
\[
  \begin{pmatrix}
    d_1-\lambda & \alpha_1 \\
    \alpha_1 & d_2-\lambda
  \end{pmatrix}.
\]
Verify that $p_2(\lambda) = (d_2-\lambda)\,p_1(\lambda) - \alpha_1^2\,p_0(\lambda)$.
\item {\bf Inductive Step.} Using determinant expansion along the $(k+1)$-st row or column, outline why $p_{k+1}(\lambda) = (d_{k+1}-\lambda)p_k(\lambda) - \alpha_k^2 p_{k-1}(\lambda)$ for $k\ge1$.
\end{enumerate}

\subsection*{3. Tridiagonal Model for GOE and GUE}

\begin{enumerate}[(a)]
\item {\bf Real Case (GOE).} Starting with a real Wigner matrix $W$ ($X_{ij}\sim \mathcal{N}(0,1)$ for $i<j$, $X_{ii}\sim \mathcal{N}(0,2)$), show that the Householder steps produce a tridiagonal $T$ whose diagonal entries are $d_i\sim \mathcal{N}(0,1)$ and subdiagonals $\alpha_j^2 = \frac12 \chi^2_{(n-j)}$.
\item {\bf Complex Case (GUE).} For a complex Hermitian $W$, the off-diagonal entries are $\mathcal{N}(0,\tfrac12)+ i\,\mathcal{N}(0,\tfrac12)$. Sketch how the same approach yields a complex Hermitian tridiagonal form with $d_i$ real normal and $\alpha_j$ drawn from appropriate $\chi$ distributions. (You do not need a fully rigorous proof; just highlight the changes in dimension counting for real vs.\ complex parts.)
\end{enumerate}

\subsection*{4. Semicircle Law via Stieltjes Transform}

\begin{enumerate}[(a)]
\item {\bf Defining the Green’s Function.} For $z\in \C\setminus\R$, let
\[
  G_n(z) = \frac{1}{n} \,\Tr\Bigl(\bigl(\tfrac1{\sqrt{n}} T - z I\bigr)^{-1}\Bigr).
\]
Argue (informally) that $G_n(z)$ converges to a limit $G(z)$ which must satisfy an algebraic equation derived from the tridiagonal structure and the typical size of $\alpha_j$.
\item {\bf Solving for $G(z)$.} Show that $G(z)$ satisfies
\[
  G(z)^2 + z\,G(z) + 1 = 0,
\]
and deduce that
\[
  G(z) = \frac{-z + \sqrt{z^2-4}}{2}.
\]
Hence identify the imaginary part of $G(z)$ on the real interval $(-2,2)$, concluding that the limiting distribution is the semicircle law.
\end{enumerate}

\subsection*{5. Simulation of Tridiagonal vs.\ Dense Wigner}

Write a small program (in Python, MATLAB, or another language):
\begin{enumerate}[(a)]
\item Generate a dense GOE matrix $W$ of size $n=1000$ and scale by $\frac{1}{\sqrt{n}}$. Compute eigenvalues and plot a histogram.
\item Generate the corresponding tridiagonal matrix $T$ from the Dumitriu--Edelman approach (diagonal $d_i\sim \mathcal{N}(0,1)$, subdiag $\alpha_j= \sqrt{\tfrac12\,\chi^2_{n-j}}$). Compute eigenvalues of $\frac{1}{\sqrt{n}}T$ and compare histograms. They should match well with the semicircle shape for sufficiently large $n$.
\item (Optional) Investigate how large $n$ must be before the histogram looks convincingly semicircular.
\end{enumerate}

\subsection*{6. Wishart and MANOVA Exercises}

\begin{enumerate}[(a)]
\item {\bf Wishart from Data Matrix.} Generate an $n\times m$ data matrix $X$ with iid $\mathcal{N}(0,1)$. Form $W=X\,X^\top$. Plot the normalized eigenvalues $\lambda_i$ vs. the Marchenko–Pastur distribution $\mu_{MP}$ (with aspect ratio $m/n$). Discuss approximate agreement for moderate $n,m$.
\item {\bf Jacobi (MANOVA).} Let $X$ be $n\times t$ and $Y$ be $k\times t$ independent $\mathcal{N}(0,1)$. Form the matrix $M=(X\,X^\top +Y\,Y^\top)^{-1} (X\,X^\top)$ and find its eigenvalues in $[0,1]$. Plot their histogram and compare with the Jacobi Beta distribution.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Reading and Next Steps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item {\bf Local Laws and Universality.} After establishing the global semicircle distribution, one may delve into local spectral laws (e.g.\ the sine kernel or GOE Tracy–Widom distribution at the edge). The \emph{local semicircle law} refines the analysis of the Green’s function in small intervals.
\item {\bf Dyson Brownian Motion.} Another approach interprets the eigenvalues as particles with logarithmic repulsion and uses stochastic differential equations to show that equilibrium distributions converge to the $\beta$-ensembles.
\item {\bf Other Ensembles.} Beyond Wigner, Wishart, and Jacobi, one finds many integrable and combinatorial random matrices (e.g.\ discrete random partitions, polynomial ensembles, random tilings). Orthogonal polynomial techniques remain central in these broader contexts.
\item {\bf References.}
  - I. Dumitriu and A. Edelman, \emph{Matrix models for beta ensembles}, J. Math. Phys., 2002.
  - T. Tao, \emph{Topics in Random Matrix Theory}, 2012.
  - P. Deift, \emph{Orthogonal Polynomials and Random Matrices}, 2000.
  - M. Mehta, \emph{Random Matrices}, 3rd ed., Elsevier, 2004.
  - T. Anderson, \emph{An Introduction to Multivariate Statistical Analysis}, for Wishart and MANOVA.
\end{itemize}


\bigskip
\noindent
\textbf{End of Lecture 4.} In this expanded lecture, we detailed how any real symmetric matrix is tridiagonalized, derived the three-term recurrence for the characteristic polynomial, and showed how this structure underlies a clean proof of Wigner’s semicircle law for random Wigner matrices. We also saw how these ideas extend to Wishart and Jacobi ensembles, bridging us toward the broader \(\beta\)-ensemble world. Upcoming lectures will further explore local eigenvalue statistics, universality results, and connections with integrable probability.





\appendix
\setcounter{section}{3}

\section{Problems (due 2025-02-28)}





\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

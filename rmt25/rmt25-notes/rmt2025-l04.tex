\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}
\usepackage{comment}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{example}[proposition]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newenvironment{lnotes}{\section*{Notes for the lecturer}}{}
% \excludecomment{lnotes}


\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 4: Semicircle law via tridiagonalization. \\Orthogonal polynomial ensembles}


\date{Wednesday, January 29, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l04.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents

\section{Recap}

Note: I did some live random matrix simulations
\href{https://lpetrov.cc/simulations/2025-01-28-goe/}{here}
and
\href{https://lpetrov.cc/simulations/2025-01-28-bbp-transition/}{here}
--- check them out. More simulations to come.

\subsection{Gaussian ensembles}

We introduced Gaussian ensembles,
and for GOE ($\beta=1$) we computed the joint eigenvalue density.
The normalization is so that the off-diagonal elements have variance $\frac{1}{2}$
and the diagonal elements have variance $1$.
Then the joint eigenvalue density is
\begin{equation*}
	p(\lambda_1,\ldots,\lambda_n)
	=
	\frac{1}{Z_n}\,
	\prod_{i=1}^n e^{-\frac{1}{2}\lambda_i^2}
	\prod_{1\le i<j\le n}(\lambda_i - \lambda_j),
	\qquad
	\lambda_1\ge \lambda_2\ge \ldots \ge \lambda_n.
\end{equation*}

\subsection{Tridiagonalization}

We showed that any real symmetric matrix \(A\) can be tridiagonalized by an orthogonal transformation \(Q\):
\[
	Q^\top\,A\,Q \;=\; T,
\]
where \(T\) is real symmetric tridiagonal, having nonzero entries only on the main diagonal and the first super-/subdiagonals:
\begin{equation*}
	T \;=\;
	\begin{pmatrix}
		 d_1 & \alpha_1 & 0 & \cdots & 0\\
		 \alpha_1 & d_2 & \alpha_2 & \cdots & 0\\
		 0 & \alpha_2 & d_3 & \ddots & \vdots\\
		 \vdots & \vdots & \ddots & \ddots & \alpha_{n-1}\\
		 0 & 0 & \cdots & \alpha_{n-1} & d_n
	\end{pmatrix}.
\end{equation*}
In the proof, each time we need to act in the orthogonal complement to the
subspace $e_1,\ldots,e_{k-1}$ (starting from $e_1$),
and apply a Householder reflection to zero out everything strictly
below the subdiagonal. (We apply the transformations like
$A\mapsto H A H^{\top}$, so that the first row transforms
in the same way as the first column of $A$).

\section{Tridiagonal random matrices}

\subsection{Distribution of the tridiagonal form of the GOE}

Applying the tridiagonalization to GOE, we obtain the following
random matrix model.

\begin{theorem}
\label{thm:DE-model}
Let \(W\) be an \(n\times n\) GOE matrix (real symmetric) with variances chosen so that each off-diagonal entry has variance \(1/2\) and each diagonal entry has variance \(1\).  Then there exists an orthogonal matrix \(Q\) such that
\[
   W \;=\; Q^\top\,T\,Q,
\]
where \(T\) is a real symmetric tridiagonal matrix
\begin{equation}
	\label{eq:tridiagonal-form}
   T \;=\; \begin{pmatrix}
         d_1 & \alpha_1 & 0 & \cdots \\
         \alpha_1 & d_2 & \alpha_2 & \ddots \\
         0 & \alpha_2 & d_3 & \ddots \\
         \vdots & \ddots & \ddots & \ddots
       \end{pmatrix},
\end{equation}
and the random variables \(\{d_i,\alpha_j\}_{1 \le i \le n,\;1\le j\le n-1}\) are mutually independent, with
\[
  d_i \,\sim\, \mathcal{N}(0,1),
  \qquad
  \alpha_j \;=\; \sqrt{\frac{\chi^2_{\,n-j}}{2}},
\]
where \(\chi^2_{\nu}\) is a chi-square distribution with \(\nu\) degrees of freedom.
\end{theorem}

\begin{remark}[Chi-square distributions]
The \emph{chi-square distribution} with \(\nu\) degrees of
freedom, denoted by \(\chi^2_{\nu}\), is a fundamental
distribution in statistics and probability theory. It arises
naturally as the distribution of the sum of the squares of
\(\nu\) independent standard normal random variables.
Formally, if \(Z_1, Z_2, \ldots, Z_{\nu}\) are independent
random variables with \(Z_i \sim \mathcal{N}(0,1)\), then
the random variable
\[
  Q = \sum_{i=1}^{\nu} Z_i^2
\]
follows a chi-square distribution with \(\nu\) degrees of
freedom, i.e., \(Q \sim \chi^2_{\nu}\). In the context of
\Cref{thm:DE-model}, the $\alpha_j$'s can be called \emph{chi random variables}.

The parameter $\nu$ does not need to be an integer, and the
chi-square distribution is well defined for any positive
real $\nu$, for example, by continuation of the density formula.
The probability density is
\begin{equation*}
	f(x) \;=\; \frac{1}{2^{\nu/2}\,\Gamma(\nu/2)}\,x^{\nu/2-1}\,e^{-x/2},
	\qquad x\ge 0.
\end{equation*}
\end{remark}

\begin{proof}[Proof of \Cref{thm:DE-model}]
	In the process of tridiagonalization,
	we apply Householder reflections.
	Note that the diagonal entries stay fixed,
	and we only change the off-diagonal entries.
	Let us consider these off-diagonal entries.

	In the first step, we apply the reflection in $\mathbb{R}^{n-1}$
	to turn the column vector $(a_{2,1},a_{3,1},\ldots,a_{n,1} )$ into
	a vector parallel to $(1,0,\ldots,0)\in \mathbb{R}^{n-1}$.
	Since the Householder reflection is orthogonal,
	it preserves lengths. So,
	\begin{equation*}
		\alpha_1=\sqrt{a_{21}^2+a_{31}^2+\cdots+a_{n1}^2},\qquad a_{i1}\sim
		\mathcal{N}(0,\dfrac{1}{2}).
	\end{equation*}
	This implies that $\alpha_1$ has the desired chi distribution.
	The distribution of the other entries is obtained similarly by the recursive
	application of the Householder reflections.

	Note that $\alpha_j$'s and $d_i$'s depend on nonintersecting
	subsets of the matrix entries, so they are independent. This completes the proof.
\end{proof}



\subsection{Dumitriu--Edelman G$\beta$E tridiagonal random matrices}

Let us define a general $\beta$ extension of the tridiagonal model for the
GOE.

\begin{definition}
	\label{def:tridiagonal-model-general-beta}
	Let $\beta>0$ be a parameter.
	The tridiagonal G$\beta$E is a random $n\times n$
	tridiagonal real symmetric
	matrix $T$ as in
	\eqref{eq:tridiagonal-form},
	where $d_i\sim \mathcal{N}(0,1)$ are independent standard Gaussians,
	and
	\begin{equation*}
		\alpha_j\sim \frac{1}{\sqrt 2}\chi_{\beta(n-j)},\qquad
		1\le j\le n-1,
	\end{equation*}
	are chi-distributed random variables.
\end{definition}

We showed that for $\beta=1$,
the G$\beta$E is the tridiagonal form of the GOE random matrix model.
The same holds for the two other classical betas:
\begin{proposition}[Without proof]
	\label{prop:tridiagonal-model-beta-classical}
	For $\beta=2$, the G$\beta$E is the tridiagonal form of the GUE random matrix model,
	which is the random complex Hermitian matrix with Gaussian entries and maximal
	independence. Similarly, for $\beta=4$,
	the G$\beta$E is the tridiagonal form of the GSE random matrix model.
\end{proposition}


Moreover, for all $\beta$, the joint eigenvalue density of G$\beta$E is
explicit:
\begin{theorem}[\cite{dumitriu2002matrix}]
	\label{thm:DE-joint-eigenvalue-density}
	Let \(T\) be a G$\beta$E matrix as in \Cref{def:tridiagonal-model-general-beta}.
	Then the joint eigenvalue density is given by
	\begin{equation*}
		p(\lambda_1,\ldots,\lambda_n)
		=
		\frac{1}{Z_{n,\beta}}\,
		e^{-\frac{1}{2}\sum_{i=1}^n \lambda_i^2}
		\prod_{1\le i<j\le n}|\lambda_i - \lambda_j|^\beta,
		\qquad
		\lambda_1\ge \lambda_2\ge \ldots \ge \lambda_n.
	\end{equation*}
\end{theorem}

This theorem is also given without proof. The proof involves
linear algebra and computation of the Jacobians of the
change of variables from the matrix entries to the
eigenvalues in the tridiagonal setting.  It can be found in
the original paper \cite{dumitriu2002matrix}.

\subsection{The case $\beta=2$}

For computation of the local statistics of trandom matricxe, 
th ecase beta=2 is the begst

it hinges on the repres with square vandermonde

proposuition: fact about hte Vandermonde determinant and product

why ios this, this is because th4e density can be represented as a product of 
two determinants, involving
orthogonal polynomials (Hermite ones)

GIVE the main formula

and write out hjte formula for the GUE density in terms of two Hermite polynomials

also give a def of Hermite polynomials, e.g. by differentiating exponent. 

MAKE SURE to preserve the same probability normalization of exponent fo x^2/2


















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Characteristic Polynomial and Three-Term Recurrence}
\label{sec:3term}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Once we have $T = (t_{ij})$ tridiagonal, the characteristic polynomial of $T$ takes a well-known form governed by a three-term recurrence. Denote
\[
  T - \lambda I \;=\;
  \begin{pmatrix}
    d_1 - \lambda & \alpha_1 & 0 & \cdots & 0 \\
    \alpha_1 & d_2 - \lambda & \alpha_2 & \cdots & 0 \\
    0        & \alpha_2 & d_3 - \lambda & \ddots & \vdots \\
    \vdots   & \vdots   & \ddots & \ddots & \alpha_{n-1} \\
    0        & 0        & \cdots & \alpha_{n-1} & d_n - \lambda
  \end{pmatrix}.
\]

\begin{definition}[Characteristic Polynomials $p_k(\lambda)$]
For $1 \le k \le n$, let $T_k$ be the top-left $k \times k$ submatrix of $T$. Define
\[
  p_k(\lambda) \;=\; \det\bigl(T_k - \lambda I_k\bigr).
\]
Moreover, set $p_0(\lambda):=1$ by convention.
\end{definition}

\begin{lemma}[Three-Term Recurrence]
\label{lem:3term-recurrence}
Let $p_k(\lambda)$ be as above, with $p_1(\lambda) = d_1 - \lambda$. Then for $k \ge 1$,
\[
  p_{k+1}(\lambda)
  \;=\;
  (d_{k+1} - \lambda)\,p_k(\lambda)
  \;-\;\alpha_k^2\,p_{k-1}(\lambda).
\]
\end{lemma}

\begin{proof}[Idea of proof]
One checks the base case $k=1,2$ directly by computing determinants of $1\times1$ and $2\times2$ blocks. For the general step, expand $\det(T_{k+1}-\lambda I_{k+1})$ by minors along the last row or column. The block structure ensures exactly the claimed recurrence.
\end{proof}

\begin{remark}
This is analogous to recurrences in orthogonal polynomials, e.g.\ the three-term recursion for polynomials orthonormal with respect to certain measures. In fact, the polynomial $p_n(\lambda)$ can be viewed as an orthogonal polynomial if $\alpha_j>0$. This interplay is vital in random matrix theory.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dumitriu--Edelman Tridiagonal Model for Wigner Ensembles}
\label{sec:dumitriu-edelman}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An influential result by Dumitriu and Edelman (2002) shows that if you have a real Wigner matrix $W$ with the “right” Gaussian and $\chi$-distribution structure, then in the process of Householder reduction to a tridiagonal matrix, the diagonal and subdiagonal entries themselves become \emph{mutually independent} random variables. This leads to a direct handle on eigenvalue asymptotics.

\subsection{Construction}

\begin{theorem}[Dumitriu--Edelman Representation]
\label{thm:DE-representation}
Let $W$ be an $n \times n$ real Wigner matrix whose entries are:
\begin{enumerate}[(i)]
\item Off-diagonal entries $X_{ij}$, for $i<j$, are iid $\mathcal{N}(0,1)$.
\item Diagonal entries $X_{ii}$ are iid $\mathcal{N}(0,2)$.
\end{enumerate}
Then there is an orthogonal matrix $Q \in O(n)$ such that
\[
  W \;=\; Q^\top \, T\, Q,
\]
where $T$ is real symmetric tridiagonal, with:
\begin{itemize}
\item Diagonal entries $d_i \sim \mathcal{N}(0,1)$, i.i.d.
\item Subdiagonal entries $\displaystyle \alpha_j = \sqrt{\frac12 \,\chi^2_{(n-j)}}$, independent over $j=1,\ldots,n-1$.
\end{itemize}
\end{theorem}

\begin{remark}[General $\beta$-Ensembles]
For $\beta>0$, one can construct a tridiagonal matrix $T_\beta$ with diagonal entries $d_i \sim \mathcal{N}(0,2/\beta)$ and subdiagonal entries $\alpha_j = \sqrt{\frac{1}{\beta}\chi^2_{\beta(n-j)}}$, all independent, whose eigenvalues match the “Gaussian $\beta$-ensemble.” In classical settings $\beta=1,2,4$, these correspond to GOE, GUE, and GSE, respectively.
\end{remark}

\subsection{Key Points in the Proof}

\begin{itemize}
\item The Householder transformation at each step is chosen so that the random “residual vector” is isotropic, enabling the reflection to produce $\chi$-type random lengths.
\item By induction, one shows that each subdiagonal element of $T$ is the length of a Gaussian vector in $\mathbb{R}^{n-j}$, leading to $\chi_{n-j}$ distributions, and that each step is independent from the preceding ones.
\end{itemize}

\begin{remark}
This tridiagonal structure is extremely sparse compared to the original dense Wigner matrix but carries the entire spectral data. The independence among $d_i$ and $\alpha_j$ is the critical simplification.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semicircle Law via the Tridiagonal Form}
\label{sec:semicircle-tridiag}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now present a fairly detailed sketch (or roadmap) for how the tridiagonal form yields the Wigner semicircle law. Recall we aim to show:

\[
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i/\sqrt{n}}
  \;\longrightarrow\;
  \mu_{\mathrm{sc}}(dx)\;=\;\frac{1}{2\pi}\sqrt{4 - x^2}\; 1_{|x|\le2}\,
  \quad\text{almost surely}.
\]

\subsection{The Scaling and Law of Large Numbers in \(\alpha_j\)}

Write
\[
  T = \begin{pmatrix}
    d_1 & \alpha_1 & & \\
    \alpha_1 & d_2 & \alpha_2 & \\
    & \alpha_2 & d_3 & \ddots \\
    & & \ddots & \ddots
  \end{pmatrix}.
\]
In the Dumitriu--Edelman setting, $\alpha_j^2 = \frac{1}{2}\,\chi^2_{(n-j)}$, so $ E[\alpha_j^2]=(n-j)/2$ and $\alpha_j^2$ is tightly concentrated around its mean for large $n$. Precisely, for any $\varepsilon>0$,
\[
  \Pr\Bigl(\bigl|\alpha_j^2 - \tfrac{n-j}{2}\bigr| > \varepsilon (n-j)\Bigr)
  \;\le\; e^{-c(n-j)}
\]
for some $c>0$, or a similar bound from large deviation estimates. Hence, with overwhelming probability,
\[
  \alpha_j \;\approx\; \sqrt{\frac{n-j}{2}}.
\]

\subsection{Diagonal vs.\ Subdiagonal Entries}

When we examine $\frac{1}{\sqrt{n}}\,T$, the diagonal entries become $\frac{d_i}{\sqrt{n}}$. Since $d_i \sim \mathcal{N}(0,1)$ i.i.d., with probability going to 1 these entries lie within $O(n^{-1/2})$, so they vanish in the large-$n$ limit. Meanwhile,
\[
  \frac{\alpha_j}{\sqrt{n}}
  \;\approx\;
  \sqrt{\frac{n-j}{2n}}
  \;\approx\;
  \sqrt{\frac{1 - j/n}{2}}.
\]
In the bulk region (i.e.\ $j\approx \theta n$ for $\theta\in (0,1)$), we have $\alpha_j/\sqrt{n} \approx \sqrt{\frac{1-\theta}{2}}$. In short, the subdiagonal elements (scaled by $1/\sqrt{n}$) remain of order $1$, while the diagonal elements vanish.

\subsection{Characteristic Polynomial and Recurrence Analysis}

Denote
\[
  p_n(\lambda) \;=\;
  \det\Bigl(\tfrac1{\sqrt{n}}\,T - \lambda I\Bigr).
\]
Equivalently, $\lambda$ is an eigenvalue of $\frac{1}{\sqrt{n}}\,T$ if and only if $\mu = \sqrt{n}\,\lambda$ is an eigenvalue of $T$. We want to understand the distribution of the roots $\lambda_i(\frac{1}{\sqrt{n}}\,T)$ as $n\to\infty$. The three-term recurrence for $p_n(\lambda)$ can be viewed in the limit as $n\to\infty$, turning into an integral equation for the Stieltjes transform of the limiting measure.

\paragraph{Stieltjes Transform Argument (Sketch).}
Set
\[
  G_n(z)
  = \frac{1}{n}Tr\Bigl(\bigl(\tfrac{1}{\sqrt{n}}\,T - z\Bigr)^{-1}\Bigr),
\]
the Stieltjes transform. As $n$ grows, the main input is that $\alpha_j\approx \sqrt{\frac{1-j/n}{2}}$; substituting this approximate profile into the recursion for Green’s functions (a linear difference equation akin to orthogonal polynomials) yields a limiting functional equation. Solving that equation for $G(z)$ leads to
\[
  G(z) = \frac{z \pm \sqrt{z^2-4}}{2},
\]
with the appropriate branch cut. This is the well-known Stieltjes transform for the semicircle law on $[-2,2]$. (See advanced texts, e.g.\ \emph{Deift} or \emph{Tao’s} books on RMT for a full rigorous derivation.)

\paragraph{Moment Argument (Sketch).}
One can also proceed by computing or bounding the moments $ E\bigl[\frac{1}{n}Tr(\frac{1}{\sqrt{n}}\,T)^k\bigr]$ and showing that they match the semicircle moments. Indeed, as $n\to\infty$, the diagonal part becomes negligible, while the subdiagonal structure essentially forces closed loops in the sum expansions, reproducing the Catalan numbers that appear in the Wigner moment method (Lectures 1--2). The advantage of tridiagonalization is a simpler combinatorial interpretation of the entries used in each closed loop.

\subsection{Conclusion: Wigner’s Semicircle Law}

Putting these ingredients together, we conclude that with probability 1,
\[
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i(\frac1{\sqrt{n}}\,W)}
  \;\;\longrightarrow\;\;
  \mu_{\mathrm{sc}}(dx) \;=\; \frac{1}{2\pi}\sqrt{4-x^2}\; 1_{|x|\le2}
\]
This completes the proof by tridiagonal methods.

\begin{remark}[Universality]
The argument here specifically used the Gaussian Wigner distribution for off-diagonal entries to get an explicit $\chi^2$ structure. However, the final result (semicircle law) remains true under vastly weaker assumptions on the entries. The \emph{universality principle} states that the global spectral behavior is insensitive to fine details of the distribution (e.g.\ it depends primarily on the first two moments).
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Discussions}
\label{sec:extended-discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section offers broader perspectives and more detailed commentary on topics related to the tridiagonal approach.

\subsection{General \texorpdfstring{$\beta$}{beta}-Ensembles and Log-Gases}

The Dumitriu--Edelman framework extends naturally to general $\beta$-ensembles (where $\beta>0$). In that model, one has a random tridiagonal matrix
\[
  T_\beta
  =
  \begin{pmatrix}
   d_1 & \alpha_1 & 0 & \cdots \\
   \alpha_1 & d_2 & \alpha_2 & \ddots \\
   0 & \alpha_2 & d_3 & \ddots \\
   \vdots & \ddots & \ddots & \ddots
  \end{pmatrix}
\]
with $d_i \sim N(0,2/\beta)$ and
\[
  \alpha_j
  = \sqrt{\frac{1}{\beta}\,\chi^2_{\bigl(\beta(n-j)\bigr)}},
  \quad j=1,\ldots,n-1,
\]
all independent. The joint law of eigenvalues is exactly the \emph{Gaussian $\beta$-ensemble}:
\[
  p(\lambda_1,\dots,\lambda_n)
  \;\propto\;
  \exp\Bigl(-\frac{\beta}{4}\sum_{i=1}^n \lambda_i^2\Bigr)
  \;\prod_{i<j}|\lambda_i - \lambda_j|^\beta,
\]
which generalizes GOE ($\beta=1$), GUE ($\beta=2$), and GSE ($\beta=4$).

\subsection{PDE Approach to the Semicircle}

An alternative viewpoint is to treat the density of eigenvalues as evolving under a certain “continuum limit of discrete recursion,” which can be recast as a partial differential equation. In particular, the moment generating function or Stieltjes transform satisfies an algebraic equation,
\[
  G(z)^2 + z\,G(z)+1=0,
\]
leading to the arcsine or semicircle distribution. While the PDE approach can be quite technical, it strongly parallels how Dyson Brownian motion arguments are used to derive the local spectral statistics.

\subsection{Weak vs.\ Almost Sure Convergence}

In many statements of the semicircle law, we see:
\[
  \mu_n = \frac1n \sum_{i=1}^n \delta_{\lambda_i/\sqrt{n}}
  \;\longrightarrow\;
  \mu_{\mathrm{sc}}
\]
either \emph{weakly in probability} or \emph{weakly almost surely}. The difference:
\begin{itemize}
\item \emph{Weak in probability:} For each bounded continuous test function $f$, the random variable $\int f\,d \mu_n$ converges in probability to $\int f\,d \mu_{\mathrm{sc}}$.
\item \emph{Weak almost sure:} Convergence occurs with probability 1, i.e.\ \(\int f\,d \mu_n \to \int f\,d \mu_{\mathrm{sc}}\) almost surely for all bounded $f$.
\end{itemize}
For the classical Wigner ensembles, the results typically hold in the stronger almost sure sense.

\subsection{A Note on Fluctuations Beyond the Law of Large Numbers}

While the global limit of the eigenvalue distribution is the semicircle, one can also ask about fluctuations around it, e.g.\ the central limit theorem for linear statistics $\frac{1}{n}\sum f(\lambda_i)$. There's an extensive theory describing these fluctuations: for example, we get Gaussian fluctuations in many cases, related to so-called \emph{trace identities} and the \emph{loop equations} in $\beta$-ensembles.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wishart and MANOVA: Simulation Discussions}
\label{sec:wishart-manova}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Although we have focused on Wigner (symmetric) matrices, very similar tridiagonal or block-tridiagonal models exist for:
\begin{enumerate}[(1)]
\item \emph{Wishart (Laguerre) ensembles:} $X$ is $n\times m$ (real Gaussian). Then $W=XX^\top$ is $n\times n$ and the eigenvalues $\lambda_i$ follow the Marchenko–Pastur distribution in the large-$n,m$ limit.
\item \emph{Jacobi (MANOVA) ensembles:} $X$ is $n\times t$ and $Y$ is $k\times t$, both real Gaussian. The matrix $\bigl(X X^\top + Y Y^\top\bigr)^{-1}(X X^\top)$ has eigenvalues in $[0,1]$ that follow the Jacobi Beta distribution.
\end{enumerate}

In practice, simulating these models is straightforward:

\begin{enumerate}[(a)]
\item Generate data $X$, $Y$ from Gaussian distributions.
\item Form $W$ or the relevant matrix $(X X^\top + Y Y^\top)^{-1} (X X^\top)$.
\item Compute eigenvalues via a standard routine (e.g.\ \texttt{numpy.linalg.eigvalsh} in Python).
\item Plot the empirical distribution of these eigenvalues; compare to known limiting PDFs such as Marchenko–Pastur or Jacobi Beta, which have known closed forms.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercises (Due 2025-02-28)}
\label{sec:exercises}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Below are problems elaborating on the main concepts. They range from verifying computations to exploring deeper aspects of tridiagonalization and simulations.

\subsection*{1. Detailed Householder Steps and Reflection Properties}

\begin{enumerate}[(a)]
\item {\bf Constructing a Reflection that Maps One Vector to Another.}
  Let $x,y\in \mathbb{R}^n$ be nonzero. Show how to pick $v$ so that $Hx=y$, where $H=I-2\frac{v\,v^\top}{\|v\|^2}$ is a Householder reflection. Why does $H$ remain orthogonal?
\item {\bf Eliminating Below-Diagonal Entries.}
  In the first step of the tridiagonalization algorithm, pick a reflection $H_1$ that modifies the subspace spanned by $a_{21},\dots,a_{n1}$. Show explicitly how $H_1$ zeroes out all these subdiagonal entries while keeping $a_{11}$ unchanged (aside from possibly a sign).
\item {\bf Symmetry and Superdiagonal.}
  Why does $H_1 (A) H_1^\top$ also have the corresponding superdiagonal entries zeroed out in row 1? (Hint: Use $A$ is symmetric: $A_{ij}=A_{ji}$.)
\end{enumerate}

\subsection*{2. Three-Term Recurrence Warmup}

\begin{enumerate}[(a)]
\item {\bf Base Cases.} Compute $p_1(\lambda)$ and $p_2(\lambda)$ for a $2\times2$ matrix
\[
  \begin{pmatrix}
    d_1-\lambda & \alpha_1 \\
    \alpha_1 & d_2-\lambda
  \end{pmatrix}.
\]
Verify that $p_2(\lambda) = (d_2-\lambda)\,p_1(\lambda) - \alpha_1^2\,p_0(\lambda)$.
\item {\bf Inductive Step.} Using determinant expansion along the $(k+1)$-st row or column, outline why $p_{k+1}(\lambda) = (d_{k+1}-\lambda)p_k(\lambda) - \alpha_k^2 p_{k-1}(\lambda)$ for $k\ge1$.
\end{enumerate}

\subsection*{3. Tridiagonal Model for GOE and GUE}

\begin{enumerate}[(a)]
\item {\bf Real Case (GOE).} Starting with a real Wigner matrix $W$ ($X_{ij}\sim \mathcal{N}(0,1)$ for $i<j$, $X_{ii}\sim \mathcal{N}(0,2)$), show that the Householder steps produce a tridiagonal $T$ whose diagonal entries are $d_i\sim \mathcal{N}(0,1)$ and subdiagonals $\alpha_j^2 = \frac12 \chi^2_{(n-j)}$.
\item {\bf Complex Case (GUE).} For a complex Hermitian $W$, the off-diagonal entries are $\mathcal{N}(0,\tfrac12)+ i\,\mathcal{N}(0,\tfrac12)$. Sketch how the same approach yields a complex Hermitian tridiagonal form with $d_i$ real normal and $\alpha_j$ drawn from appropriate $\chi$ distributions. (You do not need a fully rigorous proof; just highlight the changes in dimension counting for real vs.\ complex parts.)
\end{enumerate}

\subsection*{4. Semicircle Law via Stieltjes Transform}

\begin{enumerate}[(a)]
\item {\bf Defining the Green’s Function.} For $z\in C\setminus\mathbb{R}$, let
\[
  G_n(z) = \frac{1}{n} \,Tr\Bigl(\bigl(\tfrac1{\sqrt{n}} T - z I\bigr)^{-1}\Bigr).
\]
Argue (informally) that $G_n(z)$ converges to a limit $G(z)$ which must satisfy an algebraic equation derived from the tridiagonal structure and the typical size of $\alpha_j$.
\item {\bf Solving for $G(z)$.} Show that $G(z)$ satisfies
\[
  G(z)^2 + z\,G(z) + 1 = 0,
\]
and deduce that
\[
  G(z) = \frac{-z + \sqrt{z^2-4}}{2}.
\]
Hence identify the imaginary part of $G(z)$ on the real interval $(-2,2)$, concluding that the limiting distribution is the semicircle law.
\end{enumerate}

\subsection*{5. Simulation of Tridiagonal vs.\ Dense Wigner}

Write a small program (in Python, MATLAB, or another language):
\begin{enumerate}[(a)]
\item Generate a dense GOE matrix $W$ of size $n=1000$ and scale by $\frac{1}{\sqrt{n}}$. Compute eigenvalues and plot a histogram.
\item Generate the corresponding tridiagonal matrix $T$ from the Dumitriu--Edelman approach (diagonal $d_i\sim \mathcal{N}(0,1)$, subdiag $\alpha_j= \sqrt{\tfrac12\,\chi^2_{n-j}}$). Compute eigenvalues of $\frac{1}{\sqrt{n}}T$ and compare histograms. They should match well with the semicircle shape for sufficiently large $n$.
\item (Optional) Investigate how large $n$ must be before the histogram looks convincingly semicircular.
\end{enumerate}

\subsection*{6. Wishart and MANOVA Exercises}

\begin{enumerate}[(a)]
\item {\bf Wishart from Data Matrix.} Generate an $n\times m$ data matrix $X$ with iid $\mathcal{N}(0,1)$. Form $W=X\,X^\top$. Plot the normalized eigenvalues $\lambda_i$ vs. the Marchenko–Pastur distribution $\mu_{MP}$ (with aspect ratio $m/n$). Discuss approximate agreement for moderate $n,m$.
\item {\bf Jacobi (MANOVA).} Let $X$ be $n\times t$ and $Y$ be $k\times t$ independent $\mathcal{N}(0,1)$. Form the matrix $M=(X\,X^\top +Y\,Y^\top)^{-1} (X\,X^\top)$ and find its eigenvalues in $[0,1]$. Plot their histogram and compare with the Jacobi Beta distribution.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Reading and Next Steps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item {\bf Local Laws and Universality.} After establishing the global semicircle distribution, one may delve into local spectral laws (e.g.\ the sine kernel or GOE Tracy–Widom distribution at the edge). The \emph{local semicircle law} refines the analysis of the Green’s function in small intervals.
\item {\bf Dyson Brownian Motion.} Another approach interprets the eigenvalues as particles with logarithmic repulsion and uses stochastic differential equations to show that equilibrium distributions converge to the $\beta$-ensembles.
\item {\bf Other Ensembles.} Beyond Wigner, Wishart, and Jacobi, one finds many integrable and combinatorial random matrices (e.g.\ discrete random partitions, polynomial ensembles, random tilings). Orthogonal polynomial techniques remain central in these broader contexts.
\item {\bf References.}
  - I. Dumitriu and A. Edelman, \emph{Matrix models for beta ensembles}, J. Math. Phys., 2002.
  - T. Tao, \emph{Topics in Random Matrix Theory}, 2012.
  - P. Deift, \emph{Orthogonal Polynomials and Random Matrices}, 2000.
  - M. Mehta, \emph{Random Matrices}, 3rd ed., Elsevier, 2004.
  - T. Anderson, \emph{An Introduction to Multivariate Statistical Analysis}, for Wishart and MANOVA.
\end{itemize}


\bigskip
\noindent
\textbf{End of Lecture 4.} In this expanded lecture, we detailed how any real symmetric matrix is tridiagonalized, derived the three-term recurrence for the characteristic polynomial, and showed how this structure underlies a clean proof of Wigner’s semicircle law for random Wigner matrices. We also saw how these ideas extend to Wishart and Jacobi ensembles, bridging us toward the broader \(\beta\)-ensemble world. Upcoming lectures will further explore local eigenvalue statistics, universality results, and connections with integrable probability.





\appendix
\setcounter{section}{3}

\section{Problems (due 2025-02-28)}

\subsection{Eigenvalue density of G$\beta$E}

Read and understand the main principles of the 
proof of \Cref{thm:DE-joint-eigenvalue-density}
in \cite{dumitriu2002matrix}.






\begin{lnotes}

	Add / finish up the discussion about tridiagonalization. The notes for L3 are updated, but
	mention it here.

	The reflection is mapping any vector into any other vector (unit vectors);
	Also, we apply it in $(n-1)$-dimensional space in the first pass.


\colorbox{yellow}{\parbox{.7\textwidth}{maybe make a simulation for Wishart and MANOVA in L4}}


\subsection{Characteristic Polynomial and Three-Term Recurrence}

Consider \(p_n(\lambda) = \det(T - \lambda I)\).  Because \(T\) is tridiagonal, we have the classical three-term recurrence for these characteristic polynomials:
\[
  p_0(\lambda) := 1,\quad
  p_1(\lambda) := d_1 - \lambda,
\]
\[
  p_{k+1}(\lambda)
  \;=\;
  (d_{k+1} - \lambda)\,p_k(\lambda)
  \;-\;\alpha_k^2\,p_{k-1}(\lambda),
  \quad
  (k=1,\dots,n-1).
\]
The eigenvalues of \(T\) are precisely the roots of \(p_n(\lambda)\).

\subsection{Sketch of the Semicircle Limit Proof}

We want to show that the empirical distribution
\[
  L_n
  \;=\;
  \frac{1}{n}\sum_{i=1}^n \delta_{\lambda_i}
\]
(where \(\lambda_1,\dots,\lambda_n\) are the eigenvalues of \(T\)) converges weakly to the semicircle law
\[
  \mu_{\mathrm{sc}}(dx)
  \;=\;
  \frac{1}{2\pi}\sqrt{4 - x^2}\,\mathbf{1}_{|x|\le 2}\,dx
\]
as \(n\to\infty\).  A typical outline:

\begin{enumerate}[1.]
\item \textbf{Law of Large Numbers for \(\alpha_j\).}
   Since \(\alpha_j^2 = \tfrac12\,\chi^2_{\,n-j}\) has mean \(\tfrac{n-j}{2}\), it is typically of order \(n/2\).  More precisely, for large \(n\), \(\alpha_j \approx \sqrt{\tfrac{n-j}{2}}\) with high probability.

\item \textbf{Scaling by \(\sqrt{n}\).}
   One rescales \(T\) by \(\tfrac{1}{\sqrt{n}}\).  This gives subdiagonal entries
   \[
     \frac{\alpha_j}{\sqrt{n}}
     \;\approx\;
     \sqrt{\frac{n-j}{2n}}
     \;\approx\;
     \sqrt{\frac{1 - j/n}{2}},
   \]
   while the diagonal entries become \(\tfrac{d_i}{\sqrt{n}}\), which vanish in the large-\(n\) limit.  So effectively, the subdiagonal structure drives the main spectral behavior in the bulk, producing the semicircle shape in the limit.

\item \textbf{Orthogonal Polynomial / Recurrence Analysis.}
   The polynomial \(p_n(\lambda)\) satisfies a discrete three-term recurrence whose ``continuum limit'' yields a certain integral equation (specifically the Stieltjes transform for the measure) whose solution is precisely the semicircle distribution.  In more detailed treatments, one shows that the moments or the Cauchy transform of \(L_n\) converge to that of \(\mu_{\mathrm{sc}}\).  The relevant PDE or integral equation is exactly solvable, producing the semicircle.

\end{enumerate}

Hence, with probability 1, as \(n\to\infty\), the empirical spectrum of \(\tfrac{1}{\sqrt{n}}\,W\) converges to the semicircle distribution on \([-2,2]\).  This precisely recovers \emph{Wigner’s semicircle law}.

\begin{remark}[Extensions]
A very similar approach works for the Gaussian Unitary Ensemble (\(\beta=2\)), leading to a random \emph{complex Hermitian} tridiagonal matrix.  For \(\beta=4\), there is a quaternionic block-tridiagonal model.  All of these point toward the same semicircle law for the global spectral distribution.
\end{remark}


\end{lnotes}



\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 5: Determinantal Point Processes and the GUE}


\date{Wednesday, February 5, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l05.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents




\section{Recap}
In
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l04.pdf}{Lecture 4}
we discussed global spectral behavior of
tridiagonal G$\beta$E random matrices,
and obtained the Wigert semicircle law for the eigenvalue density.

In this lecture we shift our focus to another powerful
technique in random matrix theory: the theory of
\emph{determinantal point processes} (DPPs). In the
$\beta=2$ (GUE) case the joint eigenvalue distributions can
be written in determinantal form. We begin by discussing the
discrete version of determinantal processes, and then derive
the correlation kernel for the GUE using orthogonal
polynomial methods. Finally, we show how the
Christoffel--Darboux formula yields a compact representation
of the kernel and indicate how one may represent it as a
double contour integral—an expression well suited for
steepest descent analysis in the large-$n$ limit.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete determinantal point processes}
\label{sec:dpp-discrete}
\subsection{Definition and basic properties}

Let $\mathfrak{X}$ be a (finite or countably infinite)
discrete set. A \emph{point configuration} on $\mathfrak{X}$
is any subset $X\subset\mathfrak{X}$ (with no repeated
points). A random point process is a probability measure on
the space of such configurations.

\begin{definition}[Determinantal Point Process]
A random point process $P$ on $\mathfrak{X}$ is called
\emph{determinantal} if there exists a function (the
\emph{correlation kernel})
$K:\mathfrak{X}\times\mathfrak{X}\to\mathbb{C}$ such that
for any $n$ and every finite collection of distinct points
$x_1,\dots,x_n\in \mathfrak{X}$, the joint probability that
these points belong to the random configuration is
\[
\operatorname{\mathbb{P}}\{x_1,\dots,x_n\in X\}=\det\Bigl[K(x_i,x_j)\Bigr]_{i,j=1}^n.
\]
\end{definition}

Determinantal processes are very useful in probability theory and random matrices.
They are a natural extension of Poisson processes, and have some parallel properties.
Many properties of determinantal processes can be derived from ``linear algebra'' (broadly
understood) applied to the kernel $K$.
There are a few surveys on them:
\cite{Soshnikov2000}, \cite{peres2006determinantal},
\cite{Borodin2009},
\cite{kulesza2012determinantal}.
Let us just mention two useful properties.

\begin{proposition}[Gap Probability]
	If $I\subset\mathfrak{X}$ is a subset, then
	\[
	\operatorname{\mathbb{P}}\{X\cap I=\varnothing\}=\det\Bigl[I-K_I\Bigr],
	\]
	where $K_I$ is the restriction of the kernel to $I$.
	If $I$ is infinite, then the determinant is understood as a
	Fredholm determinant.
\end{proposition}
\begin{remark}
	The Fredholm determinant
	might ``diverge'' (equal to $0$ or $1$).
\end{remark}

\begin{proposition}[Generating functions]
	\label{prop:gen-func}
	Let $f:\mathfrak{X}\to\mathbb{C}$ be a function such that the support of $f-1$ is finite. Then the generating function of the multiplicative statistics of the determinantal point process is given by
	\[
	\mathbb{E}\left[\ssp\prod_{x\in X} f(x)\right]
	=\det\Bigl[I + (\Delta_f - I)K\Bigr],
	\]
	where the expectation is over the random point configuration $X\subseteq\mathfrak{X}$,
	$\Delta_f$ denotes the operator of multiplication by $f$ (i.e., $(\Delta_f g)(x)=f(x)g(x)$)
	and the determinant is interpreted as a Fredholm determinant if $\mathfrak{X}$ is infinite.
\end{proposition}

\begin{remark}[Fredholm Determinant --- Series Definition]
	The Fredholm determinant of an operator $A$ on $\ell^2(\mathfrak{X})$ is given by the series
	\[
	\det(I+A)=\sum_{n=0}^\infty \frac{1}{n!}\sum_{x_1,\dots,x_n\in\mathfrak{X}} \det\bigl[A(x_i,x_j)\bigr]_{i,j=1}^n,
	\]
	where the term corresponding to $n=0$ is defined to be $1$.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Determinantal structure in the GUE}
\label{sec:gue-dpp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correlation functions as densities with respect to Lebesgue measure}
\label{sec:corr-functions-lebesgue}

In the discrete setting discussed above the joint probabilities of finding points in specified subsets of $\mathfrak{X}$ are given by determinants of the kernel evaluated at those points. When the underlying space is continuous (typically a subset of $\mathbb{R}$ or $\mathbb{R}^d$), one works instead with correlation functions which serve as densities with respect to the Lebesgue measure.

Let $X\subset \mathbb{R}$ be a random point configuration. The \emph{$n$-point correlation function} $\rho_n(x_1,\dots,x_n)$ is defined by the relation
\begin{multline*}
\mathbb{P}\{\text{there is a point in each of the infinitesimal intervals } [x_i, x_i+dx_i], \, i=1,\dots,n\}
\\
=\rho_n(x_1,\dots,x_n)\,dx_1\cdots dx_n.
\end{multline*}
For a determinantal point process the correlation functions take a determinantal form:
\[
\rho_k(x_1,\dots,x_k)=\det\Bigl[K(x_i,x_j)\Bigr]_{i,j=1}^k.
\]
\begin{remark}
	The reference measure does not necessarily have to be the Lebesgue measure.
	For example, in the discrete setting, we can also talk about the
	reference measure, it is the counting measure.
	The correlation kernel $K(x,y)$ is better understood not as a function of two variables, but as an operator on the Hilbert space
	$L^2(\mathfrak{X},d\mu)$, where $\mu$ is the reference measure.
	One can also write $K(x,y)\ssp\mu(dy)$ or $K(x,y)\sqrt{\mu(dx)\ssp \mu(dy)}$ to emphasize this structure.
\end{remark}

This formulation is particularly useful in the continuous setting, as it allows one to express statistical properties of the point process in terms of integrals over the kernel. For example, the expected number of points in a measurable set $A\subset \mathbb{R}$ is given by
\[
\mathbb{E}[\#(X\cap A)]=\int_A \rho_1(x)\,dx,
\]
while higher order joint intensities provide information about correlations between points.

\subsection{The GUE eigenvalues as DPP}

\subsubsection{Setup}

We start from the joint eigenvalue density for the Gaussian Unitary Ensemble (GUE)
\begin{equation}
\label{eq:gue-joint-density}
p(x_1,\dots,x_n)
\ssp dx_1\cdots dx_n
=\frac{1}{Z_{n,2}}\prod_{j=1}^n e^{-x_j^2/2}\prod_{1\le i<j\le n} (x_i-x_j)^2
\ssp dx_1\cdots dx_n.
\end{equation}
We will show step by step why this is a determinantal point process,
\[
	\rho_k(x_1,\dots,x_k)=\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^k, \qquad k\ge1,
\]
with the kernel defined as
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y),
\]
where the functions
\[
\psi_j(x)=\frac{1}{\sqrt{h_j}}\,p_j(x)\sqrt{w(x)},\qquad w(x)=e^{-x^2/2},
\]
are constructed from the monic Hermite polynomials $\{p_j(x)\}$ which are orthogonal with respect to the weight $w(x)$:
\[
\int_{-\infty}^\infty p_j(x)p_k(x)e^{-x^2/2}\,dx = h_j\,\delta_{jk}.
\]
Recall that ``monic'' means that the leading coefficient of $p_j(x)$ is $1$,
and we divide by the norm to make the polynomials orthonormal.

\subsubsection{Writing the Vandermonde as a determinant}
The product
\[
\prod_{1\le i<j\le n} (x_i-x_j)^2
\]
is the square of the Vandermonde determinant. Recall that the Vandermonde determinant is given by
\[
\Delta(x_1,\dots,x_n) = \prod_{1\le i<j\le n} (x_j-x_i) = \det\begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^{n-1}\\[1mm]
1 & x_2 & x_2^2 & \cdots & x_2^{n-1}\\[1mm]
\vdots & \vdots & \vdots & \ddots & \vdots\\[1mm]
1 & x_n & x_n^2 & \cdots & x_n^{n-1}
\end{pmatrix}.
\]
Thus, we have
\[
\prod_{1\le i<j\le n} (x_i-x_j)^2 = \left(\det\Bigl[x_i^{j-1}\Bigr]_{i,j=1}^n\right)^2.
\]

\subsubsection{Orthogonalization by linear operations}

Since determinants are invariant under elementary row or column
operations, we can replace the monomials $x^{j-1}$ by
any sequence of monic polynomials of degree $j-1$. In
particular, we choose the monic Hermite polynomials
$p_{j-1}(x)$ and obtain
\[
\det\Bigl[x_i^{j-1}\Bigr]_{i,j=1}^n = \det\Bigl[p_{j-1}(x_i)\Bigr]_{i,j=1}^n.
\]
The first few monic Hermite polynomials are
\begin{equation*}
	p_0(x)=1,\qquad
	p_1(x)=x,\qquad
	p_2(x)=x^2-1,\qquad
	p_3(x)=x^3-3x,\qquad
	p_4(x)=x^4-6x^2+3.
\end{equation*}


The orthogonality condition for these polynomials is
\[
\int_{-\infty}^\infty p_j(x)p_k(x)e^{-x^2/2}\,dx = h_j\,\delta_{jk}.
\]
We define the functions
\begin{equation}
\label{eq:phi_j}
\phi_j(x)=p_j(x)e^{-x^2/4},
\end{equation}
and then introduce the orthonormal functions
\begin{equation}
\label{eq:psi_j}
\psi_j(x)=\frac{1}{\sqrt{h_j}}\phi_j(x)=\frac{1}{\sqrt{h_j}}\,p_j(x)e^{-x^2/4}.
\end{equation}
Note that here the weight splits as $e^{-x^2/2}=e^{-x^2/4}e^{-x^2/4}$, which is useful in the next step.
The functions $\psi_j$ form an orhtonormal basis of the Hilbert space $L^2(\mathbb{R},dx)$:
\begin{equation*}
	\int_{-\infty}^\infty \psi_j(x)\psi_k(x)\,dx = \delta_{jk},\qquad j,k=0,1,\dots.
\end{equation*}

\subsubsection{Rewriting the density in determinantal form}

Substituting the determinant form into the joint density \eqref{eq:gue-joint-density}, we have
\[
p(x_1,\dots,x_n)= \frac{1}{Z_{n,2}}\prod_{j=1}^n e^{-x_j^2/2} \Bigl[\det\Bigl[p_{j-1}(x_i)\Bigr]_{i,j=1}^n\Bigr]^2.
\]
Incorporate the weight factors into the determinant by writing
\[
\prod_{i=1}^n e^{-x_i^2/2} = \prod_{i=1}^n \left(e^{-x_i^2/4}\cdot e^{-x_i^2/4}\right),
\]
so that
\[
\prod_{i=1}^n e^{-x_i^2/4}\det\Bigl[p_{j-1}(x_i)\Bigr]_{i,j=1}^n = \det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n.
\]
Thus, the joint density becomes
\[
p(x_1,\dots,x_n)=\frac{1}{\tilde{Z}_{n,2}} \Bigl[\det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n\Bigr]^2.
\]
This squared-determinant structure is characteristic of determinantal point processes.

We now compute the $k$-point correlation function by integrating out the remaining $n-k$ variables:
\begin{equation}
\label{eq:k-point-corr}
	\rho_k(x_1,\dots,x_k)=\frac{n!}{(n-k)!}\int_{\mathbb{R}^{n-k}} p(x_1,\dots,x_n) \,dx_{k+1}\cdots dx_n.
\end{equation}
\begin{remark}
When defining the \(k\)-point correlation function, one might initially expect a combinatorial factor corresponding to the number of ways of choosing \(k\) variables out of \(n\), namely \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\).
The absence of an extra \(k!\) in the denominator is due to the fact that $x_1,\ldots,x_k $
are fixed, and we are not integrating over all permutations of these variables.
\end{remark}




\begin{theorem}[Determinantal structure for squared-determinant densities]
\label{thm:determinantal}
We have
\[
\rho_k(x_1,\dots,x_k)=\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^k,
\]
with the correlation kernel given by
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y).
\]
\end{theorem}

\begin{proof}
We begin by writing the joint density as
\[
p(x_1,\dots,x_n)=\frac{1}{\tilde{Z}_{n,2}} \left[\det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n\right]^2.
\]
Expanding the square of the determinant, we have
\[
\left[\det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n\right]^2 = \sum_{\sigma,\tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \prod_{i=1}^n \phi_{\sigma(i)-1}(x_i)\phi_{\tau(i)-1}(x_i),
\]
where $S_n$ denotes the symmetric group on $n$ elements.

Next, to obtain the $k$-point correlation function $\rho_k(x_1,\dots,x_k)$, we integrate out the remaining $n-k$ variables
using \eqref{eq:k-point-corr}.
Substituting the expansion of the squared determinant into the expression for $\rho_k$, we have
\begin{multline}
	\label{eq:rho_k_computation_1}
\rho_k(x_1,\dots,x_k)=\frac{n!}{(n-k)!\,\tilde{Z}_{n,2}} \sum_{\sigma,\tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \\
\left\{ \prod_{i=1}^k \phi_{\sigma(i)-1}(x_i)\phi_{\tau(i)-1}(x_i) \prod_{j=k+1}^n \int_{\mathbb{R}} \phi_{\sigma(j)-1}(x)\phi_{\tau(j)-1}(x)\,dx \right\}.
\end{multline}
Now, change the functions $\phi_j(x)$ to the orthonormal functions $\psi_j(x)$ using the relation
\[
\phi_j(x)=\sqrt{h_j}\,\psi_j(x).
\]
This substitution yields
\[
\int_{\mathbb{R}} \phi_{\sigma(j)-1}(x)\phi_{\tau(j)-1}(x)\,dx = \sqrt{h_{\sigma(j)-1}h_{\tau(j)-1}} \int_{\mathbb{R}} \psi_{\sigma(j)-1}(x)\psi_{\tau(j)-1}(x)\,dx.
\]
By the orthonormality of the $\psi_j$'s, we have
\[
\int_{\mathbb{R}} \psi_{\sigma(j)-1}(x)\psi_{\tau(j)-1}(x)\,dx = \delta_{\sigma(j),\tau(j)}.
\]
Therefore, for the indices $j=k+1,\dots,n$, the integrals enforce the condition $\sigma(j)=\tau(j)$. As a result, the double sum over $\sigma$ and $\tau$ reduces to a single sum over permutations on the first $k$ indices, and the factors for the remaining indices simply contribute to the normalization constant.

Let us add more details here.
In \eqref{eq:rho_k_computation_1}, we get, using the symmetry over $x_1,\ldots,x_k $:
\begin{multline}
	\label{eq:rho_k_computation_2}
	\rho_k(x_1,\dots,x_k)=\frac{n!}{(n-k)!\,\tilde{Z}_{n,2}}
	\sum_{\substack{\sigma,\tau\in S_n\\
	\sigma(k+1)=\tau(k+1),\ldots,\sigma(n)=\tau(n) }} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \\
	\left\{ \prod_{i=1}^k \phi_{\sigma(i)-1}(x_i)\phi_{\tau(i)-1}(x_i)
	h_{\sigma(k+1)-1}\cdots h_{\sigma(n)-1}  \right\} .
\end{multline}
By Problem~\ref{prob:norm}, we have $h_j=\sqrt{2\pi}\ssp j!$.

To continue with \eqref{eq:rho_k_computation_2}, we need two general lemmas.
\begin{lemma}[Cauchy--Binet formula]
	\label{lemma:Cauchy_Binet}
	Let $A_{ij}$ and $B_{ij}$ be rectangular matrices of size \(m\times p\) and \(p\times m\), respectively,
	with
	$m\le p$.
	Then
	\begin{equation*}
		\det\left[ \sum_{\ell=1}^p A_{i\ell}B_{\ell j}
		\right]_{i,j=1}^m=
		\sum_{\ell_1<\ell_2<\cdots<\ell_p} \det\Bigl[A_{i,\ell_j}\Bigr]_{i,j=1}^m \det\Bigl[B_{\ell_i, j}\Bigr]_{j=1}^m.
	\end{equation*}
\end{lemma}
\begin{proof}
For any $1 \leq k \leq p$, the coefficient of $z^{p-k}$ in the polynomial $\det(zI_p+X)$ is the sum of the $k \times k$ principal minors of $X$. If $m \leq p$ and $A$ is an $m \times p$ matrix and $B$ is an $p \times m$ matrix, then
\begin{equation}
\label{eq:det_polynomial}
\det(zI_p+BA)=z^{p-m}\det(zI_m+AB).
\end{equation}
If we compare the coefficient of $z^{p-m}$ in
\eqref{eq:det_polynomial}, the left hand side will give the
sum of the principal minors of $BA$ while the right hand
side will give the constant term of $\det(zI_m+AB)$, which
is simply $\det(AB)$. This yields the desired result.
\end{proof}
\begin{lemma}[Andreief identity]
	\label{lemma:Andreief}
	Let $f_i(x),g_i(x)\in L^1(\mathbb{R})$ for \(i=1,\ldots,n\).
	Then
	\[
	\int_{\mathbb{R}^n}
	\det[f_i(x_j)]_{i,j=1}^n
	\det[g_i(x_j)]_{i,j=1}^n
	dx_1\cdots dx_n
	=
	n! \det\Bigl[\int_{\mathbb{R}}
		f_i(x)g_j(x)\,dx
	\Bigr]_{i,j=1}^n.
	\]
\end{lemma}
\begin{proof}
	We have by expanding the determinants in the left-hand side:
	\begin{equation*}
		\int_{\mathbb{R}^n}
		\sum_{\sigma,\tau\in S_n}
		\operatorname{sgn}(\sigma)\operatorname{sgn}(\tau)
		\prod_{i=1}^n f_{\sigma(i)}(x_i)g_{\tau(i)}(x_i)
		dx_1\cdots dx_n.
	\end{equation*}
	Now, we can sum over $\sigma\tau^{-1}$, and use the fact that
	the operation of integration over $\mathbb{R}^n$
	is symmetric in the variables \(x_1,\ldots,x_n\).
	We thus need to integrate the products of $f_{(\sigma\tau^{-1})(i)}(x_i)$,
	yielding the desired determinant in the right-hand side.
	The factor $n!$ comes from the fact that for each fixed $\sigma\tau^{-1}$, there
	are $n!$ different pairs $(\sigma,\tau)$. This completes the proof.
\end{proof}



Collecting these results, one deduces that
\begin{equation}
\rho_k(x_1,\dots,x_k)=
\mathrm{const}\cdot
\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^k, \label{eq:rho_k_determinantal_final_result}
\end{equation}
where the kernel is given by
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y).
\]
The fact that the normalization constant 
in \eqref{eq:rho_k_determinantal_final_result}
is indeed $1$ follows from \Cref{lemma:Andreief}.
Indeed, once the integral of $\rho_n$ over $\mathbb{R}^n$ is equal to \(n!\),
the integral over $x_1>\cdots>x_n$ becomes $1$ by symmetry, as it should be.
This completes the proof of \Cref{thm:determinantal}.
\end{proof}

\subsection{Christoffel--Darboux formula}



\begin{theorem}[Christoffel--Darboux Formula]
Let \(\{p_j(x)\}_{j\ge0}\) be a family of \emph{monic} orthogonal polynomials with respect to a weight function \(w(x)\) on an interval \(I\subset\mathbb{R}\). Their squared norms are given by
\[
\int_I p_j(x)\,p_k(x)\,w(x)\,dx = h_j\,\delta_{jk}.
\]
Define the orthonormal functions
\[
\psi_j(x)=\frac{1}{\sqrt{h_j}}\,p_j(x)\sqrt{w(x)}.
\]
Then the kernel
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y)
=\sqrt{w(x)w(y)}\sum_{j=0}^{n-1}\frac{p_j(x)p_j(y)}{h_j},
\]
admits the closed-form representation
\begin{equation}
\label{eq:CD}
K_n(x,y)=\sqrt{w(x)w(y)}\,\frac{1}{h_{n-1}}\,\frac{p_n(x)p_{n-1}(y)-p_{n-1}(x)p_n(y)}{x-y},
\end{equation}
with the obvious continuous extension when \(x=y\).
\end{theorem}

\begin{proof}
Define
\[
S_n(x,y)=\sum_{j=0}^{n-1}\frac{p_j(x)p_j(y)}{h_j},
\]
so that
\[
K_n(x,y)=\sqrt{w(x)w(y)}\,S_n(x,y).
\]
Our goal is to prove that
\begin{equation}
\label{eq:telescoping}
(x-y)S_n(x,y)=\frac{1}{h_{n-1}}\Bigl[p_n(x)p_{n-1}(y)-p_{n-1}(x)p_n(y)\Bigr].
\end{equation}

Since the polynomials are monic and orthogonal, they satisfy the three-term recurrence relation
\[
x\,p_j(x)=p_{j+1}(x)+\alpha_j\,p_j(x)+\beta_j\,p_{j-1}(x),\quad j\ge0,
\]
with the convention \(p_{-1}(x)=0\) and where \(\beta_j = \frac{h_j}{h_{j-1}}\).
This recurrence comes from the three facts:
\begin{enumerate}
	\item The polynomials are orthogonal with respect to the weight function \(w(x)\) supported on the real line;
	\item The operator of multiplication by \(x\) is self-adjoint with respect to the inner product induced by \(w(x)\).
	\item The multiplication by $x$ of $p_j$ gives $p_{j+1}$ plus a correction of degree $\le j$.
\end{enumerate}


Writing the recurrence for both \(p_j(x)\) and \(p_j(y)\) yields:
\[
\begin{aligned}
x\,p_j(x)&=p_{j+1}(x)+\alpha_j\,p_j(x)+\beta_j\,p_{j-1}(x),\\[1mm]
y\,p_j(y)&=p_{j+1}(y)+\alpha_j\,p_j(y)+\beta_j\,p_{j-1}(y).
\end{aligned}
\]
Multiplying the first equation by \(p_j(y)\) and the second by \(p_j(x)\), and then subtracting, we obtain:
\[
(x-y)p_j(x)p_j(y)=p_{j+1}(x)p_j(y)-p_j(x)p_{j+1}(y)
+\beta_j\Bigl[p_{j-1}(x)p_j(y)-p_j(x)p_{j-1}(y)\Bigr].
\]
Dividing by \(h_j\) and summing over \(j=0,\ldots,n-1\) gives:
\[
(x-y)S_n(x,y)
=\sum_{j=0}^{n-1}\frac{1}{h_j}\Bigl[p_{j+1}(x)p_j(y)-p_j(x)p_{j+1}(y)\Bigr]
+\sum_{j=0}^{n-1}\frac{\beta_j}{h_j}\Bigl[p_{j-1}(x)p_j(y)-p_j(x)p_{j-1}(y)\Bigr].
\]
A reindexing of the sums shows that the series telescopes, leaving only the boundary terms. In particular, one finds
\[
(x-y)S_n(x,y)=\frac{1}{h_{n-1}}\Bigl[p_n(x)p_{n-1}(y)-p_{n-1}(x)p_n(y)\Bigr].
\]
This establishes \eqref{eq:telescoping}, and hence the representation \eqref{eq:CD} for \(K_n(x,y)\).

The continuous extension to \(x=y\) is obtained via l’Hôpital’s rule.
\end{proof}

































\appendix
\setcounter{section}{4}

\section{Problems (due 2025-03-09)}


\subsection{Gap Probability for Discrete DPPs}
Let \(\mathfrak{X}\) be a (finite or countably infinite) discrete set and suppose that a point process on \(\mathfrak{X}\) is determinantal with kernel
\[
K : \mathfrak{X}\times\mathfrak{X}\to\mathbb{C},
\]
so that for any finite collection of distinct points \(x_1,\dots,x_n\in \mathfrak{X}\) the joint probability that these points belong to the configuration is
\[
\mathbb{P}\{x_1,\dots,x_n\in X\}=\det\Bigl[K(x_i,x_j)\Bigr]_{i,j=1}^n.
\]
Show that for any subset \(I\subset\mathfrak{X}\) (finite or such that the Fredholm determinant makes sense) the gap probability
\[
\mathbb{P}\{X\cap I=\varnothing\}=\det\Bigl[I-K_I\Bigr],
\]
where \(K_I\) is the restriction of \(K\) to \(I\times I\).


\subsection{Generating Functions for Multiplicative Statistics}
Let \(f:\mathfrak{X}\to\mathbb{C}\) be a function such that the support of \(f-1\) is finite. Prove that for a determinantal point process on \(\mathfrak{X}\) with kernel \(K\) the generating function
\[
\mathbb{E}\Bigl[\prod_{x\in X} f(x)\Bigr] = \det\Bigl[I+(\Delta_f-I)K\Bigr]
\]
holds, where \(\Delta_f\) is the multiplication operator defined by \((\Delta_f g)(x)=f(x)g(x)\).
\emph{Hint:} Expand the Fredholm determinant series and compare with the definition of the correlation functions.

\subsection{Variance}

Let $I$ be a finite interval,
and let $N(I)$ be the number of points of a determinantal point process in $I$
with the kernel $K(x,y)$.
Find $\operatorname{Var}(I)$
in terms of the kernel $K(x,y)$.


\subsection{Formula for the Hermite polynomials}

Show that the monic Hermite polynomials \(p_j(x)\) are given by
\begin{equation*}
	p_n(x)=(-1)^n e^{x^2/2}\frac{d^n}{dx^n}e^{-x^2/2}.
\end{equation*}

\subsection{Generating function for the Hermite polynomials}

Show that
\begin{equation*}
	\sum_{n=0}^\infty \frac{t^n}{n!} \ssp p_n(x)=e^{tx-t^2/2}.
\end{equation*}


\subsection{Projection Property of the GUE Kernel}
Show that the kernel
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y),
\]
(with the orthonormal functions \(\psi_j\) defined as in the lecture) acts as an orthogonal projection operator on \(L^2(\mathbb{R})\). In other words, prove that for all \(x,y\in\mathbb{R}\)
\[
\int_{-\infty}^\infty K_n(x,z)K_n(z,y)\,dz = K_n(x,y).
\]


\subsection{Recurrence Relation for the Hermite Polynomials}
Show that the monic Hermite polynomials defined by
\[
	p_n(x)=(-1)^n e^{x^2/2}\frac{d^n}{dx^n}e^{-x^2/2}
\]
satisfy the three-term recurrence relation
\[
p_{n+1}(x)=x\,p_n(x)-n\,p_{n-1}(x),
\]
with the convention \(p_{-1}(x)=0\).

\subsection{Differential Equation for the Hermite Polynomials}
Prove that the monic Hermite polynomials \(p_n(x)\) satisfy the second-order differential equation
\[
p_n''(x)-x\,p_n'(x)+n\,p_n(x)=0.
\]

\subsection{Norm of the Hermite Polynomials}
\label{prob:norm}

Show that
\begin{equation*}
	h_n=\int_{-\infty}^{\infty} p_n(x)^2\,e^{-x^2/2}\,dx=n!\sqrt{2\pi}.
\end{equation*}




\subsection{Existence of Determinantal Point Processes with a Given Kernel}
Let \(X\) be a locally compact Polish space equipped with a reference measure \(\mu\), and let \(K(x,y)\) be the kernel of an integral operator \(K\) acting on \(L^2(X,\mu)\). Suppose that:
\begin{enumerate}
	\item \(K\) is Hermitian (i.e. \(K(x,y)=\overline{K(y,x)}\)),
	\item \(K\) is locally trace class, and
	\item \(0\le K\le I\) as an operator, that is, both the operator \(K\) and the operator \(I-K\) are nonnegative definite. For $K$, this condition is
	\[
	\int_X\int_X f(x)\overline{K(x,y)}f(y)\,d\mu(x)\,d\mu(y)\ge0
	\]
	for all \(f\in L^2(X,\mu)\).
\end{enumerate}
Under these conditions there exists a unique determinantal point process on \(X\) with correlation functions given by
\[
\rho_n(x_1,\dots,x_n)=\det\Bigl[K(x_i,x_j)\Bigr]_{i,j=1}^n.
\]
Explain why the condition \(0\le K\le I\) is necessary.
For the proof of the existence and uniqueness of the determinantal point process, see \cite{Soshnikov2000}.


\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

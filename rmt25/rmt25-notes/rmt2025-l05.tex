\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 5: Determinantal Point Processes and the GUE}


\date{Wednesday, February 5, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l05.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents




\section{Recap}
In
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l04.pdf}{Lecture 4}
we discussed global spectral behavior of
tridiagonal G$\beta$E random matrices,
and obtained the Wigert semicircle law for the eigenvalue density.

In this lecture we shift our focus to another powerful
technique in random matrix theory: the theory of
\emph{determinantal point processes} (DPPs). In the
$\beta=2$ (GUE) case the joint eigenvalue distributions can
be written in determinantal form. We begin by discussing the
discrete version of determinantal processes, and then derive
the correlation kernel for the GUE using orthogonal
polynomial methods. Finally, we show how the
Christoffel--Darboux formula yields a compact representation
of the kernel and indicate how one may represent it as a
double contour integralâ€”an expression well suited for
steepest descent analysis in the large-$n$ limit.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete determinantal point processes}
\label{sec:dpp-discrete}
\subsection{Definition and basic properties}

Let $\mathfrak{X}$ be a (finite or countably infinite)
discrete set. A \emph{point configuration} on $\mathfrak{X}$
is any subset $X\subset\mathfrak{X}$ (with no repeated
points). A random point process is a probability measure on
the space of such configurations.

\begin{definition}[Determinantal Point Process]
A random point process $P$ on $\mathfrak{X}$ is called
\emph{determinantal} if there exists a function (the
\emph{correlation kernel})
$K:\mathfrak{X}\times\mathfrak{X}\to\mathbb{C}$ such that
for any $n$ and every finite collection of distinct points
$x_1,\dots,x_n\in \mathfrak{X}$, the joint probability that
these points belong to the random configuration is
\[
\operatorname{\mathbb{P}}\{x_1,\dots,x_n\in X\}=\det\Bigl[K(x_i,x_j)\Bigr]_{i,j=1}^n.
\]
\end{definition}

Determinantal processes are very useful in probability theory and random matrices.
They are a natural extension of Poisson processes, and have some parallel properties.
Many properties of determinantal processes can be derived from ``linear algebra'' (broadly
understood) applied to the kernel $K$.
There are a few surveys on them:
\cite{Soshnikov2000}, \cite{peres2006determinantal},
\cite{Borodin2009},
\cite{kulesza2012determinantal}.
Let us just mention two useful properties.

\begin{proposition}[Gap Probability]
	If $I\subset\mathfrak{X}$ is a subset, then
	\[
	\operatorname{\mathbb{P}}\{X\cap I=\varnothing\}=\det\Bigl[I-K_I\Bigr],
	\]
	where $K_I$ is the restriction of the kernel to $I$.
	If $I$ is infinite, then the determinant is understood as a
	Fredholm determinant.
\end{proposition}
\begin{remark}
	The Fredholm determinant
	might ``diverge'' (equal to $0$ or $1$).
\end{remark}

\begin{proposition}[Generating functions]
	\label{prop:gen-func}
	Let $f:\mathfrak{X}\to\mathbb{C}$ be a function such that the support of $f-1$ is finite. Then the generating function of the multiplicative statistics of the determinantal point process is given by
	\[
	\mathbb{E}\left[\ssp\prod_{x\in X} f(x)\right]
	=\det\Bigl[I + (\Delta_f - I)K\Bigr],
	\]
	where the expectation is over the random point configuration $X\subseteq\mathfrak{X}$,
	$\Delta_f$ denotes the operator of multiplication by $f$ (i.e., $(\Delta_f g)(x)=f(x)g(x)$)
	and the determinant is interpreted as a Fredholm determinant if $\mathfrak{X}$ is infinite.
\end{proposition}

\begin{remark}[Fredholm Determinant --- Series Definition]
	The Fredholm determinant of an operator $A$ on $\ell^2(\mathfrak{X})$ is given by the series
	\[
	\det(I+A)=\sum_{n=0}^\infty \frac{1}{n!}\sum_{x_1,\dots,x_n\in\mathfrak{X}} \det\bigl[A(x_i,x_j)\bigr]_{i,j=1}^n,
	\]
	where the term corresponding to $n=0$ is defined to be $1$.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Determinantal structure in the GUE}
\label{sec:gue-dpp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correlation functions as densities with respect to Lebesgue measure}
\label{sec:corr-functions-lebesgue}

In the discrete setting discussed above the joint probabilities of finding points in specified subsets of $\mathfrak{X}$ are given by determinants of the kernel evaluated at those points. When the underlying space is continuous (typically a subset of $\mathbb{R}$ or $\mathbb{R}^d$), one works instead with correlation functions which serve as densities with respect to the Lebesgue measure.

Let $X\subset \mathbb{R}$ be a random point configuration. The \emph{$n$-point correlation function} $\rho_n(x_1,\dots,x_n)$ is defined by the relation
\begin{multline*}
\mathbb{P}\{\text{there is a point in each of the infinitesimal intervals } [x_i, x_i+dx_i], \, i=1,\dots,n\}
\\
=\rho_n(x_1,\dots,x_n)\,dx_1\cdots dx_n.
\end{multline*}
For a determinantal point process the correlation functions take a determinantal form:
\[
\rho_k(x_1,\dots,x_k)=\det\Bigl[K(x_i,x_j)\Bigr]_{i,j=1}^k.
\]
\begin{remark}
	The reference measure does not necessarily have to be the Lebesgue measure.
	For example, in the discrete setting, we can also talk about the
	reference measure, it is the counting measure.
	The correlation kernel $K(x,y)$ is better understood not as a function of two variables, but as an operator on the Hilbert space
	$L^2(\mathfrak{X},d\mu)$, where $\mu$ is the reference measure.
	One can also write $K(x,y)\ssp\mu(dy)$ or $K(x,y)\sqrt{\mu(dx)\ssp \mu(dy)}$ to emphasize this structure.
\end{remark}

This formulation is particularly useful in the continuous setting, as it allows one to express statistical properties of the point process in terms of integrals over the kernel. For example, the expected number of points in a measurable set $A\subset \mathbb{R}$ is given by
\[
\mathbb{E}[\#(X\cap A)]=\int_A \rho_1(x)\,dx,
\]
while higher order joint intensities provide information about correlations between points.

\subsection{The GUE eigenvalues as DPP}

\subsubsection{Setup}

We start from the joint eigenvalue density for the Gaussian Unitary Ensemble (GUE)
\begin{equation}
\label{eq:gue-joint-density}
p(x_1,\dots,x_n)
\ssp dx_1\cdots dx_n
=\frac{1}{Z_{n,2}}\prod_{j=1}^n e^{-x_j^2/2}\prod_{1\le i<j\le n} (x_i-x_j)^2
\ssp dx_1\cdots dx_n.
\end{equation}
We will show step by step why this is a determinantal point process,
\[
	\rho_k(x_1,\dots,x_k)=\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^k, \qquad k\ge1,
\]
with the kernel defined as
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y),
\]
where the functions
\[
\psi_j(x)=\frac{1}{\sqrt{h_j}}\,p_j(x)\sqrt{w(x)},\qquad w(x)=e^{-x^2/2},
\]
are constructed from the monic Hermite polynomials $\{p_j(x)\}$ which are orthogonal with respect to the weight $w(x)$:
\[
\int_{-\infty}^\infty p_j(x)p_k(x)e^{-x^2/2}\,dx = h_j\,\delta_{jk}.
\]
Recall that ``monic'' means that the leading coefficient of $p_j(x)$ is $1$,
and we divide by the norm to make the polynomials orthonormal.

\subsubsection{Writing the Vandermonde as a determinant}
The product
\[
\prod_{1\le i<j\le n} (x_i-x_j)^2
\]
is the square of the Vandermonde determinant. Recall that the Vandermonde determinant is given by
\[
\Delta(x_1,\dots,x_n) = \prod_{1\le i<j\le n} (x_j-x_i) = \det\begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^{n-1}\\[1mm]
1 & x_2 & x_2^2 & \cdots & x_2^{n-1}\\[1mm]
\vdots & \vdots & \vdots & \ddots & \vdots\\[1mm]
1 & x_n & x_n^2 & \cdots & x_n^{n-1}
\end{pmatrix}.
\]
Thus, we have
\[
\prod_{1\le i<j\le n} (x_i-x_j)^2 = \left(\det\Bigl[x_i^{j-1}\Bigr]_{i,j=1}^n\right)^2.
\]

\subsubsection{Orthogonalization by linear operations}

Since determinants are invariant under elementary row or column
operations, we can replace the monomials $x^{j-1}$ by
any sequence of monic polynomials of degree $j-1$. In
particular, we choose the monic Hermite polynomials
$p_{j-1}(x)$ and obtain
\[
\det\Bigl[x_i^{j-1}\Bigr]_{i,j=1}^n = \det\Bigl[p_{j-1}(x_i)\Bigr]_{i,j=1}^n.
\]
The orthogonality condition for these polynomials is
\[
\int_{-\infty}^\infty p_j(x)p_k(x)e^{-x^2/2}\,dx = h_j\,\delta_{jk}.
\]
We define the functions
\[
\phi_j(x)=p_j(x)e^{-x^2/4},
\]
and then introduce the orthonormal functions
\[
\psi_j(x)=\frac{1}{\sqrt{h_j}}\phi_j(x)=\frac{1}{\sqrt{h_j}}\,p_j(x)e^{-x^2/4}.
\]
Note that here the weight splits as $e^{-x^2/2}=e^{-x^2/4}e^{-x^2/4}$, which is useful in the next step.

\subsubsection{Rewriting the density in determinantal form}

Substituting the determinant form into the joint density \eqref{eq:gue-joint-density}, we have
\[
p(x_1,\dots,x_n)= \frac{1}{Z_{n,2}}\prod_{j=1}^n e^{-x_j^2/2} \Bigl[\det\Bigl[p_{j-1}(x_i)\Bigr]_{i,j=1}^n\Bigr]^2.
\]
Incorporate the weight factors into the determinant by writing
\[
\prod_{i=1}^n e^{-x_i^2/2} = \prod_{i=1}^n \left(e^{-x_i^2/4}\cdot e^{-x_i^2/4}\right),
\]
so that
\[
\prod_{i=1}^n e^{-x_i^2/4}\det\Bigl[p_{j-1}(x_i)\Bigr]_{i,j=1}^n = \det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n.
\]
Thus, the joint density becomes
\[
p(x_1,\dots,x_n)=\frac{1}{\tilde{Z}_{n,2}} \Bigl[\det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n\Bigr]^2.
\]
This squared-determinant structure is characteristic of determinantal point processes.

We now compute the $k$-point correlation function by integrating out the remaining $n-k$ variables:
\begin{equation*}
	\rho_k(x_1,\dots,x_k)=\frac{n!}{(n-k)!}\int_{\mathbb{R}^{n-k}} p(x_1,\dots,x_n) \,dx_{k+1}\cdots dx_n.
\end{equation*}
\begin{remark}
When defining the \(k\)-point correlation function, one might initially expect a combinatorial factor corresponding to the number of ways of choosing \(k\) variables out of \(n\), namely \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\).
The absence of an extra \(k!\) in the denominator is due to the fact that $x_1,\ldots,x_k $
are fixed, and we are not integrating over all permutations of these variables.
\end{remark}




\begin{theorem}[Determinantal structure for squared-determinant densities]
\label{thm:determinantal}
We have
\[
\rho_k(x_1,\dots,x_k)=\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^k,
\]
with the correlation kernel given by
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y).
\]
\end{theorem}

\begin{proof}
We begin by writing the joint density as
\[
p(x_1,\dots,x_n)=\frac{1}{\tilde{Z}_{n,2}} \left[\det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n\right]^2.
\]
Expanding the square of the determinant, we have
\[
\left[\det\Bigl[\phi_{j-1}(x_i)\Bigr]_{i,j=1}^n\right]^2 = \sum_{\sigma,\tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \prod_{i=1}^n \phi_{\sigma(i)-1}(x_i)\phi_{\tau(i)-1}(x_i),
\]
where $S_n$ denotes the symmetric group on $n$ elements.

Next, to obtain the $k$-point correlation function $\rho_k(x_1,\dots,x_k)$, we integrate out the remaining $n-k$ variables:
\[
\rho_k(x_1,\dots,x_k)=\frac{n!}{(n-k)!}\int_{\mathbb{R}^{n-k}} p(x_1,\dots,x_n)\,dx_{k+1}\cdots dx_n.
\]
Since the joint density is symmetric under permutations of the variables, we may assume without loss of generality that the first $k$ variables are the ones being fixed.

Substituting the expansion of the squared determinant into the expression for $\rho_k$, we have
\begin{multline*}
\rho_k(x_1,\dots,x_k)=\frac{n!}{(n-k)!\,\tilde{Z}_{n,2}} \sum_{\sigma,\tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \\
\left\{ \prod_{i=1}^k \phi_{\sigma(i)-1}(x_i)\phi_{\tau(i)-1}(x_i) \prod_{j=k+1}^n \int_{\mathbb{R}} \phi_{\sigma(j)-1}(x)\phi_{\tau(j)-1}(x)\,dx \right\}.
\end{multline*}
Now, change the functions $\phi_j(x)$ to the orthonormal functions $\psi_j(x)$ using the relation
\[
\phi_j(x)=\sqrt{h_j}\,\psi_j(x).
\]
This substitution yields 
\[
\int_{\mathbb{R}} \phi_{\sigma(j)-1}(x)\phi_{\tau(j)-1}(x)\,dx = \sqrt{h_{\sigma(j)-1}h_{\tau(j)-1}} \int_{\mathbb{R}} \psi_{\sigma(j)-1}(x)\psi_{\tau(j)-1}(x)\,dx.
\]
By the orthonormality of the $\psi_j$'s, we have
\[
\int_{\mathbb{R}} \psi_{\sigma(j)-1}(x)\psi_{\tau(j)-1}(x)\,dx = \delta_{\sigma(j),\tau(j)}.
\]
Therefore, for the indices $j=k+1,\dots,n$, the integrals enforce the condition $\sigma(j)=\tau(j)$. As a result, the double sum over $\sigma$ and $\tau$ reduces to a single sum over permutations on the first $k$ indices, and the factors for the remaining indices simply contribute to the normalization constant.

Collecting these results, one deduces that
\[
\rho_k(x_1,\dots,x_k)=
\mathrm{const}\cdot
\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^k,
\]
where the kernel is given by
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y).
\]
To complete the proof, one must verify that the normalization constant is indeed $1$.
We can achieve this by using the fact that $p_n$ is the same as $\rho_n$.
Then, integrating $\rho_n$ over all variables gives the normalization constant,
and we have
\begin{equation}
	\label{eq:gue-normalization}
	\int_{\mathbb{R}^n} \det\Bigl[\sum_{\ell=0}^{n-1}\psi_\ell(x_i)\psi_\ell(x_j)\Bigr]_{i,j=1}^n \,dx_1\cdots dx_n = n!,
\end{equation}
and the integral over $x_1>\cdots>x_n$ is equal to $1$, as it should be.

To prove \eqref{eq:gue-normalization},
define the \(n\times n\) matrix
\[
A=\Bigl[\psi_{j-1}(x_i)\Bigr]_{i,j=1}^n.
\]
Then, by the Cauchy--Binet formula,
\[
\det\Bigl[K_n(x_i,x_j)\Bigr]_{i,j=1}^n = \det\Bigl[AA^\top\Bigr]
=\det\Bigl[A\Bigr]^2.
\]
The Andreief integration formula tells us that
\[
\int_{\mathbb{R}^n} \det\Bigl[A\Bigr]^2\,dx_1\cdots dx_n = n! \det\Bigl[\int_{\mathbb{R}} \psi_{i-1}(x)\psi_{j-1}(x)\,dx\Bigr]_{i,j=1}^n.
\]
Since the \(\psi_j\)'s are orthonormal,
\[
\int_{\mathbb{R}} \psi_{i-1}(x)\psi_{j-1}(x)\,dx = \delta_{ij},
\]
and hence
\[
\det\Bigl[\delta_{ij}\Bigr]_{i,j=1}^n = 1.
\]
This completes the proof of the theorem.
\end{proof}



\subsection{Double Contour Integral Representation and Steepest Descent}
For asymptotic analysis (for example, to derive the sine kernel in the bulk or the Airy kernel at the edge), it is extremely useful to represent the kernel in the form of a double contour integral. One classical route is as follows.

One first expresses the Hermite polynomials via a contour integral representation (see, e.g., the generating function or integral representation for Hermite polynomials):
\[
H_n(z)=\frac{n!}{2\pi i}\oint_\Gamma e^{2zw-w^2}\frac{dw}{w^{n+1}},
\]
with an appropriate choice of contour $\Gamma$. Inserting this representation into the Christoffel--Darboux formula \eqref{eq:CD} and interchanging summation and integration (justified by uniform convergence) one obtains a representation of the kernel as
\begin{equation}
\label{eq:double-contour}
K_n(x,y)=\frac{1}{(2\pi i)^2}\int_{C_1}\int_{C_2} \frac{e^{n\Phi(x,\xi)-n\Phi(y,\eta)}}{\xi-\eta}\,d\xi\,d\eta,
\end{equation}
where $\Phi$ is a certain phase function and the contours $C_1,C_2$ are chosen so that the integrals converge. (The precise form of $\Phi$ depends on the rescaling and normalization.) The representation \eqref{eq:double-contour} is well suited to a steepest descent (saddle point) analysis in the large-$n$ limit, allowing one to derive universal kernels (such as the sine kernel in the bulk)
\[
K_{\mathrm{sine}}(x,y)=\frac{\sin\pi(x-y)}{\pi(x-y)},
\]
or the Airy kernel at the spectral edge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Outlook}
In this lecture we:
\begin{itemize}
  \item Recalled the resolvent (Stieltjes transform) method from Lecture~4 and noted that its analytic completion remains open.
  \item Introduced discrete determinantal point processes and outlined key properties such as the determinantal form of correlation functions and gap probabilities.
  \item Derived (via the orthogonal polynomial method) the determinantal kernel for the GUE, first as a finite sum \eqref{eq:gue-kernel-sum} and then using the Christoffel--Darboux formula \eqref{eq:CD} for a more compact representation.
  \item Indicated how one can represent the kernel as a double contour integral (see \eqref{eq:double-contour}) and how steepest descent techniques are then used to obtain the universal limiting kernels.
\end{itemize}

In subsequent lectures we will use these results to study local eigenvalue statistics (the sine and Airy kernels) and discuss further universality aspects of random matrices.































\appendix
\setcounter{section}{4}

\section{Problems (due DATE)}





\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

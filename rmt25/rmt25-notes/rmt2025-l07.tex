\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}
\usepackage{esint}


%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 7: Double contour integral kernel. Steepest descent and local statistics}


\date{Monday, February 24, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l07.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents


\section{Steepest descent for the GUE kernel}
\label{sec:steepest-descent-GUE}

\subsection{Recap}

We continue the asymptotic analysis of the GUE kernel.

The GUE correlation kernel is defined by
\[
K_n(x,y)=\sum_{j=0}^{n-1}\psi_j(x)\psi_j(y),
\]
where the functions
\[
\psi_j(x)=\frac{1}{\sqrt{h_j}}\,p_j(x)\,e^{-x^2/4}
\]
are built from the monic Hermite polynomials \(p_j(x)\) with normalization constants \(h_j\) ensuring that the \(\psi_j\)'s form an orthonormal system in \(L^2(\mathbb{R})\).

Using the generating function
\[
\exp\Bigl(xt-\frac{t^2}{2}\Bigr)=\sum_{n\ge0}p_n(x)\frac{t^n}{n!},
\]
one obtains by Cauchy’s integral formula
\[
p_n(x)=\frac{n!}{2\pi i}\oint_C\frac{\exp\Bigl(xt-\frac{t^2}{2}\Bigr)}{t^{n+1}}\,dt,
\]
which leads to
\[
\psi_n(x)=\frac{e^{-x^2/4}}{\sqrt{h_n}}\frac{n!}{2\pi i}\oint_C\frac{\exp\Bigl(xt-\frac{t^2}{2}\Bigr)}{t^{n+1}}\,dt.
\]

Starting from the Fourier transform identity
\[
\int_{-\infty}^{\infty} \exp\Bigl(-\frac{t^2}{2}+i\,t\,x\Bigr)\,dt
=\sqrt{2\pi}\,e^{-x^2/2},
\]
and differentiating with respect to \(x\), then changing variables, one obtains
\[
\psi_n(x)=\frac{i\,e^{x^2/4}}{\sqrt{2\pi\,h_n}}
\int_{-i\infty}^{i\infty} s^n\,e^{s^2/2- s\,x}\,ds.
\]

By inserting the above representations for \(\psi_n(x)\) into the kernel sum, one arrives at the double contour integral formula
(after conjugation and the trick with removing $1 / (s-t)$):
\[
K_n(x,y)=\frac{1}{(2\pi)^2}
\oint_C dt\int_{-i\infty}^{i\infty}ds\,
\frac{\exp\Bigl\{\frac{s^2}{2}-sy-\frac{t^2}{2}+tx\Bigr\}}{s-t}\left(\frac{s}{t}\right)^n.
\]
The integration contour $C$ is a small contour around $0$, and
$s$ is passing to the right of $C$.

This representation is especially useful for performing asymptotic analysis (for example, via the steepest descent method) and for deriving results such as the semicircle law.


\subsection{Scaling}
\label{sub:scaling}

Let us now consider the GUE kernel,
\begin{equation*}
	K_n(x,y)=\frac{1}{(2\pi)^2}
	\oint_C dt\int_{-i\infty}^{i\infty}ds\ssp
	\frac{\exp\left\{ \frac{s^2}{2}-sy-\frac{t^2}{2}+tx \right\}}{s-t}\left( \frac{s}{t} \right)^n
	.
\end{equation*}

We know from the Wigner semicircle law
(established for real symmetric matrices with general iid entries in
in \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l02.pdf}{Lecture 2},
and for the GUE in \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l04.pdf}{Lecture 4})
that the eigenvalues live on the scare $\sqrt n$. This means that to capture the local asymptotics,
we need to scale
\begin{equation}
	\label{eq:scaling_x-y}
	x=X\sqrt n+\frac{\Delta x}{\sqrt n},\qquad y=Y\sqrt n+\frac{\Delta y}{\sqrt n},\qquad
	\Delta x,\Delta y\in\mathbb{R}.
\end{equation}
Moreover, if $X\ne Y$ (i.e., different global positions), one can check that the kernel
vanishes. In other words, the local behaviors at different global positions are independent.
In what follows, we take $Y=X$.


\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}[scale=1]
		% Draw coordinate axes
		\draw[->] (-2,0) -- (3,0);
		\draw[->] (0,-3) -- (0,3);

		% t contour: unit circle, counterclockwise
		\draw[ultra thick,->] (1,0) arc (0:360:1);
		% Place the t label near the contour (at 45° outside the circle)
		\coordinate (TLabel) at (.5,.5);
		\node at (TLabel) {\(z\)};

		% s contour: vertical line (imaginary axis) with a detour near the origin
		% It starts at (0,-3), goes vertically to (0,-2), then detours to (0,2)
		% via a Bézier curve (pushed to the right), and finally resumes vertically to (0,3).
		\draw[ultra thick,->,red]
			(0,-3)
			-- (0,-2)
			.. controls (1.8,-2) and (1.8,2) .. node[midway, above right] {\(w\)} (0,2)
			-- (0,3);
	\end{tikzpicture}
	\caption{Integration contours for the GUE kernel.}
	\label{fig:contours}
\end{figure}


Let us also make a change of the integration variables:
\begin{equation*}
	t=z\sqrt n,\qquad s=w\sqrt n.
\end{equation*}
The integration contours for $z$ and $w$ look the same as for $t$ and $s$, up to a rescaling
(\Cref{fig:contours}). However, as $0$
and $t=s$
are the only singularities in the integrand, we can deform the $z,w$
contours as we wish, while keeping $|z|<|w|$
and the general shape as in \Cref{fig:contours}.

We thus have:
\begin{multline}
	\label{eq:K_n_scaling}
	K_n(X\sqrt n+\Delta x/\sqrt n,X\sqrt n+\Delta y/\sqrt n)\\=
	\frac{\sqrt n}{(2\pi)^2}
	\oint_C dz\int_{-i\infty}^{i\infty}dw\ssp
	\frac{\exp
		\left\{
			n\left(
				\log w -\log z
				+\frac{w^2}{2}-\frac{z^2}{2}
				+X(z-w)+\frac{z \Delta x-w \Delta y}{n}
			\right)
		\right\}
	}{w-z}.
\end{multline}
\begin{remark}
	\label{rmk:log-harmless}
	The logarithms in the exponent are harmless, since for the
	estimates we only need the real parts of the logarithms,
	and for the main contributions, we will have $z\approx w$, so
	any phases of the logarithms would cancel.
\end{remark}

The asymptotic analysis of double contour integrals like
\eqref{eq:K_n_scaling} in the context of determinantal point processes
was pioneered in \cite[Section~3]{Okounkov2002}.

\subsection{Critical points}
\label{sub:critical-points}

Let us define
\begin{equation*}
	S(z)\coloneqq
	\frac{z^2}{2}+\log z -X z.
\end{equation*}
Then the exponent contains $n \left( S(w)-S(z) \right)$.
According to the steepest descent ideology, we
should deform the integration contours
to pass through the critical point(s) $z_{cr}$ of $S(z)$.
Moreover, the new $w$ contour should maximize the real part of $S(z)$
at $z_{cr}$, and the new $z$ contour should minimize it.
If $S''(z_{cr})\ne 0$, it is possible to locally choose such contours,
they will be perpendicular to each other at $z_{cr}$.

Thus, we need to find the critical points of $S(z)$.
They are found from the quadratic equation:
\begin{equation}
	\label{eq:critical-points}
	S'(z)=z+\frac{1}{z}-X=0,\qquad
	z_{cr}=\frac{X\pm \sqrt{X^2-4}}{2}.
\end{equation}
Depending on whether $|X|<2$, there are three cases.
Unless $|X|=2$, equation \eqref{eq:critical-points} has a single root, and
thus $S''(z_{cr})\ne 0$.
We will consider the three cases in
\Cref{sub:imaginary-critical-points,sub:real-critical-points,sub:double-critical-points}
below.

\subsection{Imaginary critical points: $|X|<2$, ``bulk''}
\label{sub:imaginary-critical-points}

When $|X|<2$, the critical points are complex conjugate.
Denote them by $z_{cr}$ and $\overline{z_{cr}}$.
Since $S(z)$ has real coefficients, we have
\begin{equation*}
	\operatorname{Re}S(z_{cr})=\operatorname{Re}S(\overline{z_{cr}}).
\end{equation*}
Thus, we need to consider the contribution from both points.
For simplicity of the computations, let us consider only the case $X=0$.
See Problem~\ref{prob:imaginary-critical-points}.
We have
\begin{equation*}
	z_{cr}=i,\qquad
	S''(z_{cr})=2.
\end{equation*}
The behavior of $\operatorname{Re}S(z)$ on the complex plane
can be illustrated by a 3D plot or by a region plot of the regions
where $\operatorname{Re}S(z)-\operatorname{Re}S(z_{cr})$ has constant sign.
See \Cref{fig:ReS_imaginary} for an illustration in the case $X=\frac{1}{2}$.
(We take $X\ne 0$ to break symmetry, for a better intuition.)

\begin{figure}[htpb]
	\centering
	\includegraphics[height=.3\textwidth]{pictures/ReS_imaginary_3D.pdf}
	\qquad
	\includegraphics[height=.3\textwidth]{pictures/ReS_imaginary_region.pdf}
	\caption{A 3D plot and a region plot of the
	regions where $\operatorname{Re}S(z)-\operatorname{Re}S(z_{cr})$ is positive
	(highlighted) or negative, in the case $X=\frac{1}{2}$.
	In this case, $z_{cr}\approx 0.25+0.96 i$.}
	\label{fig:ReS_imaginary}
\end{figure}

From the region plot, we see that the new $z$ contour should
pass through the shaded region $\operatorname{Re}S(z)-\operatorname{Re}S(z_{cr})>0$,
and the new $w$ contour should pass through the unshaded region
$\operatorname{Re}S(z)-\operatorname{Re}S(z_{cr})<0$.

Deforming the contours from \Cref{fig:contours} to the new contours
is impossible without passing through the residue at $w=z$.
Moreover, this residue appears only for certain values of $z$. Namely, for $X=0$,
let us first make the $z$ contour to be the positively (counterclockwise) oriented
unit circle.
It passes through the critical points $z_{cr}=i$ and $\overline{z_{cr}}=-i$.
Since the original $w$ contour is to the right of the $z$ contour, we only
encounter the residue when $z$ is in the right half of the circle.

Thus, we can write
\begin{equation}
	\label{eq:K_n_bulk_deformation}
	\oiint_{\textnormal{old contours}}=
	\oiint_{\textnormal{new contours}}
	+
	\int_{-i}^i 2\pi i\ssp \operatorname{Res}\limits_{w=z}\ssp dz,
\end{equation}
where in the single integral, the $z$ contour passes to the right of the origin,
along the right half of the unit circle.

It remains to consider the two integrals in the right-hand side
of \eqref{eq:K_n_bulk_deformation}.
Recall that the correlation functions are
defined relative to a reference measure, and the right object to scale is
\begin{equation*}
	K_n(x,y)dy=\frac{1}{\sqrt n}\ssp d\left( \Delta y \right).
\end{equation*}
The extra factor $n^{-1/2}$
compensates the prefactor $\sqrt n$ in
\eqref{eq:K_n_scaling}.

The single integral takes the form
\begin{equation}
	\label{eq:K_n_bulk_single}
	\frac{-i}{2\pi}
	\int_{-i}^i
	e^{z (\Delta x -\Delta y)}
	\ssp
	dz
	=\frac{\sin\left( \Delta x-\Delta y \right)}{\pi(\Delta x-\Delta y)},
	\qquad \Delta x,\Delta y\in\mathbb{R}.
\end{equation}
\begin{definition}
	\label{def:sine-kernel}
	The \emph{sine kernel} is defined as
	\begin{equation*}
		K_{\mathrm{sine}}(x,y)\coloneqq
		\begin{cases}
			\dfrac{\sin (x-y)}{\pi (x-y)},&\qquad x\ne 0,\\[10pt]
			\dfrac{1}{\pi},&\qquad x=0.
		\end{cases}
	\end{equation*}
	(The value at $x=y$ is defined by continuity.)

	This kernel is translation invariant, and is often
	defined with a single argument, as
	$K_{\mathrm{sine}}(x-y)$.
\end{definition}

The double integral has both contours
in the ``steepest descent'' regime, which means that
the main contribution is
\begin{equation*}
	\mathrm{const}\cdot
	\frac{e^{n\left( \operatorname{Re}S(z_{cr})-\operatorname{Re}S(z_{cr}) \right)}}{\sqrt n}
	\sim \frac{\mathrm{const}}{\sqrt n}.
\end{equation*}
At this rate, the double integral over the new contours
\emph{does not} contribute to the asymptotics of the correlation functions.
Recall that the correlation functions are expressed as finite-dimensional
determinants of the kernel $K_n(x,y)$, and the error $O(n^{-1/2})$ is
negligible in the limit $n\to+\infty$.
This is because the main term comes from the single integral,
which does not vanish.

We have established the following result:
\begin{proposition}[Bulk asymptotics at $X=0$]
	\label{prop:bulk}
	The correlation kernel $K_n$ of the GUE has the following asymptotics
	close to zero as $n\to+\infty$:
	\begin{equation*}
		\lim_{n\to \infty}
		\frac{1}{\sqrt n}
		K_n\left( \frac{\Delta x}{\sqrt n},\frac{\Delta y}{\sqrt n} \right)
		=
		K_{\mathrm{sine}}\left( \Delta x,\Delta y \right),
		\qquad \Delta x,\Delta y\in\mathbb{R}.
	\end{equation*}
	Consequently, the eigenvalues of the GUE converge to the sine process
	determined by the sine kernel (\Cref{def:sine-kernel}),
	in the sense of finite-dimensional distributions.
\end{proposition}

\begin{remark}
	Beyond $X=0$, the local correlations are essentially the same,
	up to rescaling of the real line by a constant factor (depending
	on the semicircle density).
	See Problem~\ref{prob:imaginary-critical-points}.
\end{remark}

\subsection{Real critical points: $|X|>2$, ``large deviations''}
\label{sub:real-critical-points}

For \(X^2>4\), both solutions
\eqref{eq:critical-points}
are real. Let us assume $X>2$, the case \(X<2\) is similar.
For $X>2$, both solutions are positive.
Label these solutions as
\[
	z_+ \;=\;\frac{X + \sqrt{X^2-4}}{2},
	\qquad
	z_- \;=\;\frac{X - \sqrt{X^2-4}}{2},
	\quad
	\text{so that}\quad z_+z_-=1.
\]
A straightforward check reveals that \(z_+\!>\!1\) and \(z_-\!<\!1\) (for \(X>2\)).
Note that $S''(z)=1-z^{-2}$, which is positive for \(z_+>1\) and negative for \(z_-<1\).  Thus, the critical points \(z_+\) and \(z_-\) are a local minimum and a local maximum.
A crucial observation is that
\begin{equation*}
	S(z_+)<S(z_-).
\end{equation*}
One can deform the $z$ integration contour to pass through
$z_-$ and the $w$ contour to pass through $z_+$.
Then, on these contours, one can show that
\begin{equation*}
	\operatorname{Re}S(w)-\operatorname{Re}S(z)<0.
\end{equation*}
According to the steepest descent ideology,
we see that the main exponential behavior of the double contour integral is
\begin{equation}
	\label{eq:Oexp}
	\exp\left\{ n\left(
		\operatorname{Re}S(z_+)-\operatorname{Re}S(z_-)
\right) \right\}=O( e^{-\delta(X)n} ), \qquad |X|>2.
\end{equation}
Here $\delta(X)>0$ for $|X|>2$, and $\delta(X)\to0$ when $|X|\to2$.

The outcome \eqref{eq:Oexp} reflects the fact that the
Wigner semicircle law places all eigenvalues inside the
interval \(\lvert X\rvert \le 2\).
The probability to see even a single eigenvalue outside $[-2,2]$
is exponentially small.

This exponential decay corresponds to a large deviation regime.
Indeed, if at least one of the diagonal entries of the matrix
is unusually large, this corresponds to
the maximal eigenvalue to get outside the interval \([-2,2]\).
See also Problem~\ref{prob:large-deviation}.


\subsection{Double critical point: $|X|=2$, ``edge''}
\label{sub:double-critical-points}

Throughout the subsection, we assume that $X=2$. The case $X=-2$ is symmetric.

When \(X=2\), the two solutions in \eqref{eq:critical-points} merge into a double critical point
$z_{cr}=1$.
We have
\[
S'(1)=0,\qquad S''(1)=0,\qquad S'''(1)=2.
\]
Thus, the usual quadratic approximation fails and one must expand to third order. Writing
\[
z=1+u,\qquad w=1+v,
\]
with \(u,v\) small, we have
\[
S(1+u)=S(1)+\frac{S'''(1)}{6}\,u^3+O(u^4)
=S(1)+\frac{u^3}{3}+O(u^4),
\]
and similarly for \(S(1+v)\). Hence, the difference in the exponents becomes
\[
S(1+v)-S(1+u)=\frac{v^3-u^3}{3}+O(u^4+v^4).
\]

To capture the correct asymptotics, we rescale the local variables by setting
\[
u=\frac{U}{n^{1/3}},\qquad v=\frac{V}{n^{1/3}},
\]
so that
\[
n\Bigl[S(1+v)-S(1+u)\Bigr]=\frac{V^3-U^3}{3}+O\Bigl(n^{-1/3}\Bigr).
\]
Moreover, the correct edge scaling for the spatial variables is obtained by writing
\[
x=2\sqrt{n}+\frac{\xi}{n^{1/6}},\qquad y=2\sqrt{n}+\frac{\eta}{n^{1/6}},\qquad \xi,\eta\in\mathbb{R}.
\]
We have
\begin{equation*}
	n\left( S(w)-S(z) \right)=n^{1/3}(\xi-\eta)+
	\frac{V^3-U^3}{3}+\xi U-\eta V+O\Bigl(n^{-1/3}\Bigr).
\end{equation*}
The terms $n^{1/3}(\xi-\eta)$ are harmless as they can be removed
by conjugation.

The region plot of $\operatorname{Re}S(z)-\operatorname{Re}S(1)$
(shown in \Cref{fig:ReS_edge})
makes sure that we can deform the $z$ contour so that it passes through $z_{cr}=1$
as the new $U$ contour at the angles $\pm \frac{2\pi}{3}$ (where $\operatorname{Re}U^3>0$),
we can deform the $w$ contour so that it passes through $z_{cr}=1$
as the new $V$ contour at the angles $\pm \frac{\pi}{3}$ (where $\operatorname{Re}V^3<0$).
This will ensure the convergence of the new double integral.

\begin{figure}[htpb]
	\centering
	\includegraphics[height=.3\textwidth]{pictures/ReS_edge.pdf}
	\caption{The plot of the region $\operatorname{Re}S(z)-\operatorname{Re}S(1)>0$ for $X=2$.}
	\label{fig:ReS_edge}
\end{figure}

Thus, we have shown that under the rescaling, the GUE correlation kernel
$K_n(x,y)\ssp dy$ converges to a new kernel.
\begin{definition}
	\label{def:Airy_kernel}
	Define the \emph{Airy kernel} on $\mathbb{R}$ by
	\begin{equation*}
		K_{\mathrm{Ai}}(\xi,\eta)=\frac{1}{(2\pi i)^2}
		\int_{e^{-\frac{\pi i}{3}}\infty}^{e^{\frac{\pi i}{3}}\infty}dV
		\int_{e^{-\frac{2\pi i}{3}}\infty}^{e^{2\frac{\pi i}{3}}\infty}dU
		\ssp
		\frac{\exp\Bigl\{\frac{V^3-U^3}{3}+U\,\xi-V\,\eta\Bigr\}}{V-U}.
	\end{equation*}
	For another formula for the Airy kernel
	which does not involve integrals,
	see
	Problem~\ref{prob:airy}.
\end{definition}

\begin{proposition}
	\label{prop:edge}
	We have
	\begin{equation*}
		\lim_{n\to\infty}
		\frac{1}{n^{1/6}}K_n\Bigl(2\sqrt{n}+\frac{\xi}{n^{1/6}},\,2\sqrt{n}+\frac{\eta}{n^{1/6}}\Bigr)
		\to
		K_{\mathrm{Ai}}(\xi,\eta).
	\end{equation*}
	Consequently, the eigenvalue statistics at the edge of the spectrum converge to the Airy point process, in the sense of fine-dimensional distributions.
\end{proposition}

\subsection{Airy kernel, Tracy--Widom distribution, and convergence of the maximal
eigenvalue}

Let us make a few remarks on the asymptotic results of
\Cref{prop:bulk,prop:edge}.
First,
a rigorous justification of convergence
of contour integrals requires some estimates on the error terms
in the steepest descent analysis, but these
estimates are mild and not hard to obtain.

Second, the GUE has the maximal eigenvalue $\lambda_{max}$. It is reasonable
to assume that the Airy process also (almost surely) admits a maximal point
(usually denoted by $\mathfrak{a}_1$),
and that $\lambda_{max}$
converges to $\mathfrak{a}_1$ under appropriate rescaling:
\begin{equation}
	\label{eq:TW_GUE}
	\lim_{n\to\infty}n^{\frac{1}{6}}\bigl(\lambda_{max}-2\sqrt{n}\bigr)=\mathfrak{a}_1.
\end{equation}
This is indeed the case, but to show \eqref{eq:TW_GUE}, one needs to
show the convergence in distribution:
\begin{equation}
	\label{eq:TW_GUE_convergence}
	\lim_{n\to \infty}
	\mathbb{P}\Bigl(n^{1/6}(\lambda_{max}-2\sqrt{n})\le x\Bigr)
	\to
	\mathbb{P}(\mathfrak{a}_1\le x).
\end{equation}
Both events \eqref{eq:TW_GUE_convergence} are so-called
\emph{gap probabilities}, for example,
\begin{equation*}
	\operatorname{P}(\mathfrak{a}_1\le x)=
	\operatorname{P}(\textnormal{there are no eigenvalues in the interval $(x,\infty)$}),
\end{equation*}
which is expressed as
the Fredholm determinant
\begin{equation}
	\label{eq:gap-probability-Ai-Fredholm}
	\det\left( 1-K_{\mathrm{Ai}} \right)_{(x,\infty)}=
	1+\sum_{m=1}^{\infty}\frac{(-1)^m}{m!}
	\int_x^{\infty}dy_1\int_x^{\infty}dy_2\cdots
	\int_x^{\infty}dy_m
	\ssp
	\det\limits_{i,j=1}^m
	K_{\mathrm{Ai}}(y_i,y_j).
\end{equation}
Thus, to get \eqref{eq:TW_GUE_convergence}), one needs to show the convergence
of sums like this for the GUE kernel
to the corresponding sums for the Airy kernel. This is doable, but tedious.

Moreover, to get convergence in distribution of random variables,
one would also have to argue either \emph{tightness},
or independently show that
\eqref{eq:gap-probability-Ai-Fredholm} defines a
cumulative probability
distribution function in $x$:
\begin{equation}
	\label{eq:TW_GUE_cdf}
	F_2(x)=\det\left( 1-K_{\mathrm{Ai}} \right)_{(x,\infty)}.
\end{equation}
The distribution \eqref{eq:TW_GUE_cdf} is known as the \emph{GUE Tracy--Widom distribution}.
The subscript $2$ indicates that $\beta=2$. There are distributions
$F_\beta$ for all beta, most notably, the GOE and GSE distributions.
The classical distributions $F_1,F_2,F_4$ also appear as fluctuation distributions
in interacting particle systems, while other beta values do
not quite appear in the particle systems
domain.

More details
may be found in the original papers
\cite{tracy1993level},
\cite{Forrester1993},
\cite{tracy_widom1994level_airy}.



\subsection{Remark: what happens for general $\beta$?}

\begin{itemize}
    \item The determinantal structure exploited above is special to the $\beta=2$ case. In contrast, for $\beta=1$ (GOE) and $\beta=4$ (GSE) the eigenvalue correlations are expressed in terms of \emph{Pfaffians} rather than determinants.
			This happens before and after the scaling limit.
		\item
			Earlier attempts to extend the $\beta=2$ techniques
			were determinantal. For example, one can replace the
			squared Vandermonde $\prod_{i<j} (x_i-x_j)^2$ with
			\begin{equation*}
			 \prod_{i<j} (x_i-x_j)(x_i^{\beta/2}-x_j^{\beta/2}).
			\end{equation*}
			This is known as the \emph{Muttalib--Borodin} ensemble
			\cite{forrester2017},
			and the kernel can be computed in a similar way using (bi)orthogonalization.
		\item Local eigenvalue statistics of general $\beta$-ensembles converge to the so-called
			\emph{general $\beta$ sine process}
			and
			\emph{general $\beta$ Airy process}
			in the bulk and at the edge, respectively.
			Detailed analyses of this convergence can be found in
			\cite{RamirezRiderVirag2006RandomAiry},
			\cite{valko2009continuum},
			\cite{gorin2018stochastic},
			and the literature referenced in the recent work
			\cite{gorin2024airy}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cutting corners: setup}

We begin a new topic, which will be the main focus for this and the next week.

In random matrix theory, one often studies the entire spectrum of an $n\times n$ matrix ensemble such as the Gaussian Unitary Ensemble (GUE), the Gaussian Orthogonal Ensemble (GOE), or, more generally, $\beta$-ensembles. However, it is also natural to examine the spectra of \emph{principal minors} of such matrices.

When we say ``cutting corners,'' we typically refer to extracting a top-left $k\times k$ submatrix (or \emph{corner}) out of an $n\times n$ random matrix $H$ and then looking at the interplay among the eigenvalues of all corners $k=1,\dots,n$. This forms a \emph{nested} family of spectra, often described by interlacing (or Gelfand--Tsetlin) patterns.

The \emph{GUE corners process} is a classical example of
this phenomenon. If $H$ is an $n\times n$ GUE matrix, then
the top-left $k\times k$ corners (for $1\le k\le n$) have
jointly distributed eigenvalues that exhibit a
determinantal structure. We will employ the
technique of \emph{polynomial equations} and
then \emph{loop equations} to study global limits
(note that they are not suitable to get local limits like sine and Airy processes).

So far, we have the following access to eigenvalues and corners:
\begin{enumerate}
	\item For $\beta=1,2,4$, we have the actual matrices,
		and can cut the corners in the usual way.
	\item For general $\beta$, we have the joint eigenvalue distribution
		with the interaction term $\prod_{i<j}|x_i-x_j|^\beta$, which is an interpolation.
	\item For general $\beta$, we also have the Dumitriu--Edelman
		tridiagonal model \cite{dumitriu2002matrix}.
\end{enumerate}
Cutting corners from the tridiagonal matrix is not a good idea, for many reasons.
The simplest might be that the $(n-1)\times (n-1)$ corner
eigenvalues do not have the same distribution (up to changing $n$) as the
general $\beta$ ensemble eigenvalues. Maybe we might cut the lower right corners?
Well, this is not a good idea either, because the total number of random variables
(the ``noise'') in the tridiagonal matrix is $O(n)$, while the number of eigenvalues
of all corners is $O(n^2)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Corners of Hermitian matrices}
\label{sec:corners-definition}

\subsection{Principal corners}
Let $H$ be an $n\times n$ Hermitian matrix. For each $1\le k\le n$, define the \emph{top-left $k\times k$ corner} $H^{(k)}$ by
\[
	H^{(k)} \;=\; \bigl[H_{ij}\bigr]_{1\le i,j \le k}.
\]
Since $H$ is Hermitian, each $H^{(k)}$ is also Hermitian. Let
\[
	\lambda_1^{(k)} \;\ge\;\lambda_2^{(k)}\;\ge\;\cdots\;\ge\;\lambda_k^{(k)}
\]
denote the eigenvalues of $H^{(k)}$. Then the collection
\[
	\bigl\{\lambda_j^{(k)} : 1\le j\le k \le n\bigr\}
\]
is called the \emph{corners spectrum} (or \emph{minor
spectrum}) of $H$. When $H$ is random, this triangular array
of eigenvalues becomes a random point configuration in the
two-dimensional set $\{1,\dots,n\}\times \mathbb{R}$.

\subsection{Interlacing}
A fundamental feature of Hermitian matrices is that the eigenvalues of corners interlace with the eigenvalues of the full matrix:
\begin{proposition}
	\label{prop:interlacing}
If $\nu_1\ge\dots\ge \nu_n$ are the eigenvalues of $H$ itself (i.e., the full $n\times n$ matrix), and $\mu_1\ge\cdots\ge\mu_{n-1}$ are the eigenvalues of $H^{(n-1)}$, then we have:
\[
\nu_1\;\ge\;\mu_1\;\ge\;\nu_2\;\ge\;\mu_2\;\ge\;\dots\;\ge\;\mu_{n-1}\;\ge\;\nu_n.
\]
\end{proposition}
\begin{proof}
One can prove the statement using the Courant--Fischer
(min--max) characterization of eigenvalues, often referred
to as the variational principle. Recall that for an
\(n\times n\) Hermitian matrix \(H\) with ordered
eigenvalues \(\nu_1 \ge \nu_2 \ge \cdots \ge \nu_n\), the
\(j\)-th largest eigenvalue \(\nu_j\) admits the variational
characterization
\[
\nu_j
\;=\;
\max_{\substack{V\subset\mathbb{F}^n\\\dim(V)=j}}
\;\min_{\substack{x\in V \\ x\neq 0}}
\;
\frac{x^\ast H\,x}{x^\ast x}
\;=\;
\min_{\substack{W\subset\mathbb{F}^n\\\dim(W)=n-j+1}}
\;\max_{\substack{x\in W \\ x\neq 0}}
\;
\frac{x^\ast H\,x}{x^\ast x},
\]
where \(\mathbb{F}\) is \(\mathbb{R}\), \(\mathbb{C}\), or
the quaternions (depending on \(\beta=1,2,4\),
respectively).
We leave this as Problem~\ref{prob:interlacing}.
\end{proof}

The same interlacing property holds for real symmetric matrices ($\beta=1$),
and in the case $\beta=4$.
Therefore, it is natural to require this property for all $\beta$-ensembles.

\subsection{Orbital measure}

It is natural to consider an extended setup, and
take the matrix $H$ to not just be GUE, but instead fix its eigenvalues.
Let
\begin{equation*}
	H=U\Lambda U^\dagger,\qquad \Lambda=\operatorname{diag}(\lambda_1,\dots,\lambda_n),
\end{equation*}
where $\Lambda$ is fixed and $U\in U(n)$ is Haar (uniformly) distributed.
Denote the set of all such $H$
by $\mathrm{Orbit}(\lambda)$, $\lambda=(\lambda_1,\dots,\lambda_n)\in \mathbb{R}^n$,
$\lambda_1\ge \cdots\ge \lambda_n$.

Then, if we understand the distribution structure
of all corners of a random $H\in \mathrm{Orbit}(\lambda)$,
we can then ``average over'' the GUE eigenvalue ensemble distribution of $\lambda$
to get the GUE corners process.

\begin{remark}
	The setting with orbits presents a bridge into ``asymptotic representation theory''.
	Namely, as $n\to\infty$, how does the corners distribution look like?
	We may ask for a characterization of \emph{all the ways} how
	$\lambda^{(n)}=( \lambda_1^{(n)}\ge \ldots \lambda^{(n)}_n  )$
	goes to infinity, in such a way that the corners
	spectrum converges on all levels $k=1,\ldots,K $ for arbitrary $K$ (independent of
	$n$).
	This problem was solved in \cite{OlVer1996}.
	More direct formulas for projections of orbital measures
	were obtained in \cite{olshanski2013projections}.
\end{remark}


\section{Polynomial equations}

Fix $\lambda=(\lambda_1\ge \ldots \ge \lambda_n )$.
Let $H\in \mathrm{Orbit}(\lambda)$ be a random matrix
(in the case $\beta=2$, but the proof works for $\beta=1,4$ as well).
Let $\mu_1,\ldots,\mu_{n-1}$ be the eigenvalues of the $(n-1)\times(n-1)$ corner $H^{(n-1)}$.
\begin{lemma}
	The distribution of $\mu_1,\ldots,\mu_{n-1}$ is the same as the distribution of
	the roots of the polynomial equation
	\begin{equation*}
		\sum_{i=1}^n \frac{\xi_i}{z-\lambda_i}=0,
	\end{equation*}
	where $\xi_i$ are i.i.d.\ random variables with the distribution $\chi^2_\beta$.
\end{lemma}
\begin{proof}
	$\mu_1,\ldots,\mu_{n-1} $ are the roots of the following
	equation with the determinant of
	order $n+1$:
	\begin{equation*}
		\det\begin{pmatrix}
			U\operatorname{\mathrm{diag}}(\lambda)U^\dagger-z I_N & v^\top\\
			v & 0
		\end{pmatrix}=0,
		\qquad 
		v=\begin{pmatrix}
			0\\0\\\vdots\\0\\1
		\end{pmatrix}.
	\end{equation*}
	Indeed, expanding the determinant along the last row, we get the $(n-1)$th
	determinant, which corresponds to cutting the corner. 
	
	Next, multiply the determinant by $\begin{pmatrix} U^\dagger&0\\0&1 \end{pmatrix}$
	on the left 
	and $\begin{pmatrix} U & 0\\0&1 \end{pmatrix}$ on the right:
	\begin{equation*}
		\det\begin{pmatrix}
			\operatorname{\mathrm{diag}}(\lambda)-z I_N & u^\dagger\\
			u & 0
		\end{pmatrix}=0,
	\end{equation*}
	where $u^\dagger=U^\dagger v^\top$ is the last row of $U^\dagger$.
	The determinant now can be expressed as
	\begin{equation*}
		\det=-\prod_{i=1}^n(\lambda_i-z)\sum_{i=1}^{n}\frac{|u_i|^2}{\lambda_i-z}.
	\end{equation*}
	Since $u$ is a row of a Haar unitary matrix, 
	it is distributed uniformly on the unit sphere in $\mathbb{C}^n$.
	However, we can identify it with a normalized vector from a 
	rotationally invariant measure on $\mathbb{C}^n$,
	the best of which is Gaussian.
	This completes the proof.
\end{proof}



































\appendix
\setcounter{section}{6}

\section{Problems (due 2025-03-25)}

\subsection{General bulk case}
\label{prob:imaginary-critical-points}

Perform the asymptotic analysis of the correlation
kernel as in \Cref{sub:imaginary-critical-points},
but in the general case $-2<X<2$.


\subsection{Large deviations}
\label{prob:large-deviation}

Let \(W_n\) be an \(n\times n\) Wigner real or Hermitian matrix with finite variance entries. Assume that the matrix is normalized so that the variance of each diagonal entry is 1.

\medskip

\textbf{Assumption \cite{BBP2005phase}.} \textit{If a Wigner matrix is normalized to have diagonal variance 1, then a rank 1 perturbation of magnitude $c>0$ is sufficient to shoot the maximum eigenvalue outside the support of the Wigner semicircle law. (For a simulation of this phenomenon, see \href{https://lpetrov.cc/simulations/2025-01-28-bbp-transition/}{here}.)}

\medskip

Consider the following large deviation event. For a fixed \(\eta>0\), let
\[
E_{n,\eta}\coloneqq \Bigl\{ \exists\, i\in\{1,\dots,n\} \text{ such that } W_{ii}\ge \eta \Bigr\}.
\]
Under the above assumption, if for some \(i\) the diagonal
entry \(W_{ii}\) is unusually large, it will push the
maximal eigenvalue of \(W_n\) outside the bulk.

\begin{enumerate}
	\item Assuming that the
		entries are Gaussian,
		\emph{lower bound} the probability of the event \(E_{n,\eta}\) for large \(n\).
	\item
		Assuming another tail behavior of the diagonal entries (exponential or
		power-law tails),
		use the limit theorems for maxima of independent random variables to generalize the
		\emph{lower bound} of $\operatorname{\mathbb{P}}(E_{n,\eta})$.
\end{enumerate}



\subsection{Airy kernel}
\label{prob:airy}

Define the Airy function by
\begin{equation*}
	Ai(\xi)\coloneqq
	\frac{1}{2\pi}\int_{-\infty}^\infty
	e^{i U^3/3+i\xi U} dU=
	\frac{1}{\pi}\int_0^\infty
	\cos\left( \frac{U^3}{3}+\xi U \right)\ssp dU.
\end{equation*}
This integral converges, but only conditionally. To improve convergence,
one should instead integrate
along a complex contour,
from $e^{\frac{5 \pi i}{6}}\infty$ to $0$ to
$e^{\frac{\pi i}{6}}\infty$.

Show that
\begin{equation*}
	K_{\mathrm{Ai}}(\xi,\eta)=
	\frac{Ai(\xi)\ssp Ai'(\eta)-Ai(\eta)\ssp Ai'(\xi)}{\xi-\eta}.
\end{equation*}
Note that this expression is parallel to the sine kernel,
\begin{equation*}
	\frac{\sin(x-y)}{\pi(x-y)}=\frac{\sin x\cos y-\cos x\sin y}{\pi(x-y)},\qquad
	\cos x=(\sin x)'.
\end{equation*}
These correlation kernels are called \emph{integrable}
\cite{its1990differential}.

Hint for the problem: observe that
\begin{equation*}
	\exp\left\{ -i z x+iwy \right\}=\frac{i}{x-y}\left( \frac{\partial}{\partial z}+
	\frac{\partial}{\partial w}\right)\exp\left\{ -i z x+iwy \right\},
\end{equation*}
and use integration by parts in $K_{\mathrm{Ai}}(\xi,\eta)$
from \Cref{def:Airy_kernel}.

\subsection{Interlacing proof}
\label{prob:interlacing}

Finish the proof of \Cref{prop:interlacing}.



\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

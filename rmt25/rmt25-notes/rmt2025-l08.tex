\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 8:  Cutting corners and loop equations}


\date{Wednesday, February 26, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l08.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents


\section{Cutting corners: polynomial equation and distribution}

\subsection{Recap: polynomial equation}

Recall the polynomial equation we proved in the last \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l07.pdf}{Lecture 7}.
Fix $\lambda=(\lambda_1\ge \ldots \ge \lambda_n )$.
Let $H\in \mathrm{Orbit}(\lambda)$ be a random Hermitian matrix
defined as
\begin{equation*}
	H=U\ssp \mathrm{\operatorname{diag}}(\lambda_1,\ldots,\lambda_n)\ssp U^\dagger,
\end{equation*}
where $U$ is Haar-distributed unitary matrix from $U(n)$.
This is the case $\beta=2$,
but the statement holds for the cases $\beta=1,4$ with appropriate modifications.
Let $\mu_1,\ldots,\mu_{n-1}$ be the eigenvalues of the $(n-1)\times(n-1)$ corner $H^{(n-1)}$.
\begin{lemma}
	\label{lemma:corner_step}
	The distribution of $\mu_1,\ldots,\mu_{n-1}$ is the same as the distribution of
	the roots of the polynomial equation
	\begin{equation}
		\label{eq:polynomial_equation}
		\sum_{i=1}^n \frac{\xi_i}{z-\lambda_i}=0,
	\end{equation}
	where $\xi_i$ are i.i.d.\ random variables with the distribution $\chi^2_\beta$.
\end{lemma}
Recall also that this passage from $\lambda$ to $\mu$ works inductively, and
the distribution of the next level eigenvalues $\nu=(\nu_1\ge \ldots \ge \nu_{n-2})$
is given by the same polynomial equation, but with $\lambda$ replaced by $\mu$.
In this way, we can define a \emph{Markov map} from $\lambda$ to $\mu$, which is then iterated
to construct the full array of eigenvalues of the corners of $H$.

For $\beta=\infty$, this map is deterministic, and is equivalent to successive differentiating the
characteristic polynomial of $H$.

\subsection{Extension to general $\beta$}

We extend the polynomial equation to general $\beta$,
by \emph{declaring} (defining) that the general $\beta$ corners distribution
is powered by the passage from $\lambda=(\lambda_1\ge \ldots \ge \lambda_n)$ to $\mu=(\mu_1\ge \ldots \ge \mu_{n-1})$,
where $\mu$ solves \eqref{eq:polynomial_equation} with $\xi_i$ i.i.d.\ $\chi^2_\beta$.
In this way, $\mu$ interlaces with $\lambda$.
For $\beta=1,2,4$, this definition reduces to the one with invariant ensembles
with fixed eigenvalues $\lambda$.


\subsection{Distribution of the eigenvalues of the corners}

Let $\mu$ be obtained from $\lambda$ by the general $\beta$ corners operation.

\begin{theorem}
	\label{thm:corner_step}
	The density of $\mu$ with respect to the Lebesgue measure is given by
	\begin{equation*}
		\frac{\Gamma(N \beta/2)}{\Gamma(\beta/2)^n}
		\prod_{1\le i<j\le n-1}(\mu_i-\mu_j)
		\prod_{i=1}^{n-1}\prod_{j=1}^n |\mu_i-\lambda_j|^{\beta/2-1}
		\prod_{1\le i<j\le n}(\lambda_i-\lambda_j)^{1-\beta}.
	\end{equation*}
\end{theorem}
\begin{proof}
	Let $\varphi_i=\xi_i/\sum_{j=1}^n \xi_j$.
	It is well-known\footnote{See Problem~\ref{prob:dirichlet}.}
	the joint
	density of $(\varphi_1,\ldots,\varphi_n )$ is the
	(symmetric) Dirichlet density
	\begin{equation*}
		\frac{\Gamma(N \beta/2)}{\Gamma(\beta/2)^n}
		\ssp
		w_1^{\beta/2-1}\ldots  w_n^{\beta/2-1}
		\ssp
		dw_1\ldots dw_{n-1}
	\end{equation*}
	(note that the density is $(n-1)$-dimensional).

	We need to compute the Jacobian of the transformation from $\varphi$ to $\mu$,
	if we write
	\begin{equation*}
		\sum_{i=1}^n\frac{\varphi_i}{z-\lambda_i}=
		\frac{\prod_{i=1}^{n-1}(z-\mu_i)}
		{\prod_{i=1}^n(z-\lambda_i)},
	\end{equation*}
	and compute
	(as a decomposition into partial fractions):
	\begin{equation*}
		\varphi_a=
		\frac{\prod_{i=1}^{n-1}(\lambda_a-\mu_i)}{\prod_{i\ne a}(\lambda_a-\lambda_i)}.
	\end{equation*}
	Therefore,
	\begin{equation}
		\label{eq:Jacobian}
		\frac{\partial\varphi_a}{\partial\mu_b}=
		\frac{\prod_{i=1}^{n-1}(\lambda_a-\mu_i)}{\prod_{i\ne a}(\lambda_a-\lambda_i)}
		\ssp\frac{1}{\mu_b-\lambda_a},
		\qquad
		a=1,\ldots,n,\quad b=1,\ldots,n-1.
	\end{equation}
	The Jacobian is essentially the determinant of the matrix $1/(\mu_b-\lambda_a)$,
	which is the Cauchy determinant
	(Problems~\ref{prob:cauchy} and~\ref{prob:n_n_1}).
	The final density is obtained from the symmetric Dirichlet density,
	but we plug in $w=\varphi$, and also multiply by the inverse of the Jacobian determinant~\eqref{eq:Jacobian}.
	After the necessary simplifications, this completes the proof.
\end{proof}



\begin{corollary}[Joint density of the corners]
	\label{cor:corners_density}
	The eigenvalues $\lambda^(k)_j$, $1\le j\le k\le n$,
	of a random matrix from $\mathrm{Orbit}(\lambda)$
	form an interlacing array, with the joint density
	\begin{equation*}
		\propto
		\prod_{k=1}^n
		\prod_{1\le i<j\le k}\left(\lambda_j^{(k)}-\lambda_i^{(k)}\right)^{2-\beta}
		\prod_{a=1}^{k+1}\prod_{b=1}^k
		\left|\lambda_a^{(k+1)}-\lambda_b^{(k)}\right|^{\beta/2-1}.
	\end{equation*}
\end{corollary}
For $\beta=2$, all factors disappear, and we get the
uniform distribution on the interlacing array. This is the \emph{uniform Gibbs property}
which is important for other models, including discrete ensembles.


\section{Loop equations}

Let us write down the \emph{loop equations} for the passage from the
eigenvalues $\lambda$ to the eigenvalues $\mu$.
These loop equations are due to \cite{gorin2022dynamical}
by a limit from a discrete system (related to Jack
symmetric polynomials). Note that despite the name, these are not \textbf{equations},
but rather a statement that some expectations are holomorphic.
We stick to the random matrix setting, and
present a formulation and a proof given by \cite{gorin2025private}.

\subsection{Formulation}

\begin{theorem}
	\label{Theorem_loop_equation}
 We fix $n=1,2,\dots$ and $n+1$ real numbers $\lambda_1\ge\dots\ge\lambda_{n+1}$. For $\beta>0$, consider $n+1$ i.i.d.\ $\chi^2_\beta$ random variables $\xi_i$ and set
 $$
  w_i=\frac{\xi_i}{\sum_{j=1}^{n+1} \xi_j}, \qquad 1\le i \le n+1.
 $$
 We define $n$ random points $\{\mu_1,\dots,\mu_n\}$ as $n$ solutions to the equation
 \begin{equation} \label{eq_mu_equation}
  \sum_{i=1}^{n+1} \frac{w_i}{z-\lambda_i}=0.
 \end{equation}
 Take any \emph{polynomial} $W(z)$ and consider the complex function:
 \begin{equation}
 \label{eq_loop_observable}
 f_W(z)=\operatorname{\mathbb{E}}\left[\prod_{j=1}^n \exp\bigl(W(\mu_j)\bigr) \frac{\prod_{i=1}^{n+1} (z-\lambda_i)}{\prod_{j=1}^n (z-\mu_j)} \left( W'(z)+\sum_{i=1}^{n+1} \frac{\beta/2-1}{z-\lambda_i} + \sum_{j=1}^n \frac{1}{z-\mu_j}\right)\right].
 \end{equation}
 Then $f_W(z)$ is an \emph{entire function} of $z$, in the following sense:
 \begin{itemize}
	 \item For $z\in \mathbb{C}\setminus [\lambda_{n+1},\lambda_1]$, the expectation in \eqref{eq_loop_observable} defines a holomorphic function of $z$.
  \item This function has an analytic continuation to $\mathbb{C}$, which has no singularities.
 \end{itemize}
\end{theorem}
\begin{remark}
 Note that for $z$ in
 $[\lambda_{n+1},\lambda_1]$, the integral determining
 \eqref{eq_loop_observable} might be divergent, and,
 therefore, analytic continuation is the proper way to
 define $f_W(z)$, $z\in [\lambda_{n+1},\lambda_1]$.
\end{remark}

\begin{corollary}
	We have
\begin{equation*}
	f_0(z)=\frac{(n+1)\beta}{2}-1.
\end{equation*}
Here $f_0$ means $f_W$ with $W\equiv 0$.
\end{corollary}
\begin{proof}
This is obtained by sending $z\to \infty$ in
	\eqref{eq_loop_observable}.
\end{proof}

\subsection{Proof of \Cref{Theorem_loop_equation} for $\beta>2$}

\Cref{Theorem_loop_equation} remains
valid for $\beta>0$, but we only prove it for $\beta>2$ here.
We also assume that $\lambda_1>\ldots>\lambda_n $.


We begin by observing that for $z \in \mathbb{C} \setminus [\lambda_{n+1}, \lambda_1]$, the expectation in \eqref{eq_loop_observable} is well-defined and holomorphic in $z$. This follows since for such $z$, the denominators $z-\lambda_i$ and $z-\mu_j$ are bounded away from zero with probability 1.
The key challenge is to show that $f_W(z)$ can be analytically continued to an entire function.
Potential singularities of $f_W(z)$ are inside the intervals $(\lambda_{i+1}, \lambda_{1})$. We will show that these singularities do not actually occur.

Consider a specific interval $(\lambda_2, \lambda_{1})$. We need to show that $f_W(z)$ has no singularities in this interval.
From \Cref{thm:corner_step}, the probability distribution of $\mu = (\mu_1, \ldots, \mu_n)$ has density proportional to:
\begin{equation*}
	\prod_{1\le i<j\le n} (\mu_i - \mu_j) \prod_{i=1}^{n} \prod_{j=1}^{n+1} |\mu_i - \lambda_j|^{\beta/2-1}.
\end{equation*}

Let us analyze the function in \eqref{eq_loop_observable}. For $z \in (\lambda_2, \lambda_{1})$, we need to demonstrate that the expectation
\begin{equation*}
	\operatorname{\mathbb{E}}\left[\prod_{j=1}^n \exp\bigl(W(\mu_j)\bigr) \frac{\prod_{i=1}^{n+1} (z-\lambda_i)}{\prod_{j=1}^n (z-\mu_j)} \left( W'(z)+\sum_{i=1}^{n+1} \frac{\beta/2-1}{z-\lambda_i} + \sum_{j=1}^n \frac{1}{z-\mu_j}\right)\right]
\end{equation*}
is holomorphic.
This expectation is an $(n-1)$-fold integral over $\mu_1, \ldots, \mu_n$.
For $z\in(\lambda_2,\lambda_1)$, we will show that
the one-dimensional integral over $\mu_1$ is already holomorphic,
and the remaining integrals are over domains which do not encounter singularities in $z$. We
need to consider the integral
\begin{equation}
\label{eq:integral_original}
\begin{split}
	&\int\limits_{\lambda_2}^{\lambda_1}
	\prod_{1 \leq i<j \leq n}(\mu_i-\mu_j) \prod_{j=1}^{n}\prod_{i=1}^{n+1}(\mu_j-\lambda_i)^{\beta/2-1} \prod_{j=1}^{n} e^{W(\mu_j)} \frac{\prod_{i=1}^{n+1}(z-\lambda_i)}{\prod_{j=1}^{n}(z-\mu_j)} \\
	&\hspace{150pt}\times
	\left(W'(z) + \sum_{i=1}^{n+1} \frac{\beta/2-1}{z-\lambda_i} + \sum_{j=1}^{n} \frac{1}{z-\mu_j}\right) d\mu_2.
\end{split}
\end{equation}
Note that (here we are using the fact that $\beta>2$)
\begin{multline*}
	0 = \int_{\lambda_2}^{\lambda_1}d\mu_1
	\frac{\partial}{\partial
	\mu_1}\left(\underbrace{\prod_{1 \leq i<j \leq n}(\mu_i-\mu_j)
	\prod_{j=1}^{n}\prod_{i=1}^{n+1}(\mu_j-\lambda_i)^{\beta/2-1} \prod_{j=1}^{n} e^{W(\mu_j)}
	\frac{\prod_{i=1}^{n+1}(z-\lambda_i)}{\prod_{j=1}^{n}(z-\mu_j)}}_{(*)}\right)
	\\
	=
	\int_{\lambda_2}^{\lambda_1}d\mu_1\ssp
	(*)
	\cdot \left[\sum_{j=2}^n \frac{1}{\mu_1-\mu_j} +
	\sum_{i=1}^{n+1} \frac{\beta/2-1}{\mu_1-\lambda_i} +
	W'(\mu_1) + \frac{1}{z-\mu_1}\right].
\end{multline*}

Subtracting this expression from our original integral
\eqref{eq:integral_original}
and noting that
\begin{equation*}
\left(W'(z) + \sum_{i=1}^{n+1} \frac{\beta/2-1}{z-\lambda_i} + \sum_{j=1}^{n} \frac{1}{z-\mu_j}\right) - \left(\sum_{j=2}^n \frac{1}{\mu_1-\mu_j} + \sum_{i=1}^{n+1} \frac{\beta/2-1}{\mu_1-\lambda_i} + W'(\mu_1) + \frac{1}{z-\mu_1}\right)
\end{equation*}
has zero at $z = \mu_1$, we conclude that our integral has no singularity at $\mu_1$, and therefore no singularities in the $[\lambda_2, \lambda_1]$ interval.
This completes the proof of \Cref{Theorem_loop_equation} for $\beta>2$.


\section{Applications of loop equations}

The loop equations provide a powerful tool for analyzing the spectral properties of random matrices through their eigenvalue distributions. Let us derive an equation for the Stieltjes transform of the empirical measures.

\subsection{Stieltjes transform equations}
Starting from Theorem \ref{Theorem_loop_equation} with $W=0$, we have:
\begin{equation} \label{eq:loop_eq_base}
	\operatorname{\mathbb{E}}\left[\frac{\prod_{i=1}^{n+1}(z-\lambda_i)}{\prod_{j=1}^n(z-\mu_j)}\left(\sum_{i=1}^{n+1}\frac{\beta/2-1}{z-\lambda_i} + \sum_{j=1}^n\frac{1}{z-\mu_j}\right)\right] = \frac{(n+1)\beta}{2}-1.
\end{equation}
Let us introduce the empirical Stieltjes transforms:
\begin{align*}
G_\lambda(z) &= \frac{1}{n}\sum_{i=1}^{n+1}\frac{1}{z-\lambda_i}, \\
G_\mu(z) &= \frac{1}{n}\sum_{j=1}^n\frac{1}{z-\mu_j}.
\end{align*}
We also define the ``logarithmic potentials'' (indefinite integrals of the Stieltjes transforms):
\begin{align*}
\int G_\lambda(z)dz &= \frac{1}{n}\sum_{i=1}^{n+1}\ln(z-\lambda_i), \\
\int G_\mu(z)dz &= \frac{1}{n}\sum_{j=1}^n\ln(z-\mu_j).
\end{align*}
We understand the integrals up to the same integration constant (and branch), so the exponent of the difference
yields the original product:
\begin{equation*}
	\frac{\prod_{i=1}^{n+1}(z-\lambda_i)}{\prod_{j=1}^n(z-\mu_j)} = \exp\left(n\left(\int G_\lambda(z) - \int G_\mu(z)\right)\right)
\end{equation*}
We can rewrite equation \eqref{eq:loop_eq_base} as:
\begin{equation} \label{eq:stieltjes_transform_eq}
	\operatorname{\mathbb{E}}\left[\exp\left(n\left(\int G_\lambda(z)\,dz - \int G_\mu(z)\,dz\right)\right)\left(\left(\frac{\beta}{2}-1\right)G_\lambda(z) + G_\mu(z)\right)\right] = \frac{\beta}{2} + \frac{1}{n}\left(\frac{\beta}{2}-1\right).
\end{equation}





\subsection{Asymptotic behavior}

Equation \eqref{eq:stieltjes_transform_eq} can be reinterpreted in terms of a time evolution of eigenvalue distributions. This perspective offers significant insights into the asymptotic behavior of the corners process.

If we think of $\lambda$ as configuration at time $t=1$ and $\mu$ as configuration at time $t=1-\frac{1}{n}$, then denoting the general time parameter as $t$ and setting $G_\lambda = G_1$, $G_\mu = G_{1-\frac{1}{n}}$, we obtain a continuous time evolution of Stieltjes transforms.
(And similarly for all $t$, of course.)

As $n \to \infty$, equation \eqref{eq:stieltjes_transform_eq} transforms into:
\begin{equation*}
\frac{\beta}{2} \exp\left(\frac{\partial}{\partial t}\int G_t(z)\,dz\right) \cdot G_t(z) = \frac{\beta}{2}.
\end{equation*}
This implies
\begin{equation*}
\frac{\partial}{\partial t}\int G_t(z)\,dz + \ln G_t(z) = 0.
\end{equation*}
Taking the derivative with respect to $z$, we get:
\begin{equation}
\label{eq:burgers_equation}
\frac{\partial}{\partial t}G_t(z) + \frac{1}{G_t(z)}\frac{\partial}{\partial z}G_t(z) = 0.
\end{equation}

This is the inviscid Burgers equation, a
fundamental nonlinear PDE in fluid dynamics --- but with complex $z$.
The complex Burgers equation has appeared in descriptions of
limit shapes of models in statistical mechanics, such as lozenge tilings
\cite{OkounkovKenyon2007Limit}.

\begin{remark}
	We see that the Burgers equation \eqref{eq:burgers_equation} does not depend on $\beta$,
	which is expected. Indeed, for example, G$\beta$E eigenvalues
	have the same Wigner semicircle law as $\beta=2$, up to an overall
	rescaling.
\end{remark}

\subsection{Example: G$\beta$E and the semicircle law}

The Stieltjes transform of the semicircular law is given by:
\begin{equation*}
	G(z) = \int\limits_{-2}^{2}\frac{1}{z-x}\frac{\sqrt{4-x^2}}{2\pi}dx =
	\frac{1}{2} \left(z-\sqrt{z^2-4}\right).
\end{equation*}
We take this as the function $G_t(z)$ for $t=1$.
Then, for each $0\le t\le 1$, the 
G$\beta$E solution should be 
\begin{equation*}
	\frac{1}{n}\sum_{i=1}^{\lfloor nt \rfloor }\frac{1}{z-\lambda_i^{(\lfloor nt \rfloor )}}
	\to t\ssp G^{(\sqrt t)}(z),
\end{equation*}
where 
\begin{equation*}
	G^{(c)}(z) \coloneqq \frac{z-\sqrt{z^2-4c^2}}{2c^2},
\end{equation*}
is the Stieltjes transform of the semicircular law on $[-2c, 2c]$.

\begin{lemma}
	\label{lemma:semicircle_and_burgers}
	The function $G_t(z)\coloneqq t\ssp G^{(\sqrt t)}(z)$
	satisfies the Burgers equation \eqref{eq:burgers_equation}.
\end{lemma}
\begin{proof}
	Straightforward verification.
\end{proof}













\appendix
\setcounter{section}{7}

\section{Problems (due 2025-03-25)}


\subsection{Cauchy determinant}
\label{prob:cauchy}

Prove the Cauchy determinant formula:
\begin{equation*}
	\det\left( \frac{1}{x_i-y_j} \right)_{1\le i,j\le n}=
	\frac{\prod_{i<j}(x_i-x_j)(y_i-y_j)}{\prod_{i,j}(x_i-y_j)}.
\end{equation*}

\subsection{Jacobian from $n-1$ to $n$ dependent variables}
\label{prob:n_n_1}

Explain how the factor $\prod_{i=1}^{n-1}\prod_{j=1}^n|\mu_i-\lambda_j|$
appears from the Jacobian of the transformation from $\varphi$ to $\mu$
\eqref{eq:Jacobian},
even though $\partial\varphi_a/\partial\mu_b$ is defined for
$a=1,\ldots,n  $, $b=1,\ldots,n-1$,
but the $\varphi_i$'s are not independent.

\subsection{Dirichlet density}
\label{prob:dirichlet}

Find in the literature or prove on your own
the first statement in the proof of
\Cref{thm:corner_step} about the symmetric Dirichlet density arising from
normalizing the $\xi_i$'s to $\varphi_i$'s.

\subsection{General beta Gaussian density and cutting corners}

Show that if $\lambda_1,\ldots,\lambda_{n+1} $ have the Gaussian beta density of order $n+1$,
\begin{equation*}
	\propto \prod_{1\le i<j\le n+1}(\lambda_i-\lambda_j)^{\beta} \prod_{i=1}^{n+1}e^{-\beta\lambda_i^2/2},
\end{equation*}
and $\mu_1,\ldots,\mu_n $ are obtained from $\lambda_1,\ldots,\lambda_{n+1}$
by cutting the corner (so have the conditional density as in \Cref{thm:corner_step}),
then $\mu_1,\ldots,\mu_n$ have the Gaussian beta density of order $n$.

\subsection{General $\beta$ Corners Process Simulation}
\label{prob:corners_simulation}

This problem explores computational aspects of the general $\beta$ corners process.

\begin{enumerate}[(a)]
\item Write code for generating a sample from the distribution of $\mu = (\mu_1, \ldots, \mu_{n-1})$ given $\lambda = (\lambda_1, \ldots, \lambda_n)$ for arbitrary $\beta > 0$, using the polynomial equation characterization.

\item Let $\lambda = (n, n-1, \ldots, 2, 1)$. For $n = 7$, compute (numerically) the expected values $\mathbb{E}[\mu_i]$ for each $i$, when $\beta = 1, 2, 4,$ and $10$. Describe the behavior as $\beta$ increases.
\end{enumerate}



\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

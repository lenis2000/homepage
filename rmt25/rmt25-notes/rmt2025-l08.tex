\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 8:  Cutting corners and loop equations}


\date{Wednesday, February 26, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l08.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents


\section{Cutting corners: polynomial equations and distribution}

\subsection{Recap: polynomial equations}

Recall the polynomial equation we proved in the last \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l07.pdf}{Lecture 7}.
Fix $\lambda=(\lambda_1\ge \ldots \ge \lambda_n )$.
Let $H\in \mathrm{Orbit}(\lambda)$ be a random Hermitian matrix
defined as 
\begin{equation*}
	H=U\ssp \mathrm{\operatorname{diag}}(\lambda_1,\ldots,\lambda_n)\ssp U^\dagger,
\end{equation*}
where $U$ is Haar-distributed unitary matrix from $U(n)$.
This is the case $\beta=2$, 
but the statement holds for the cases $\beta=1,4$ with appropriate modifications.
Let $\mu_1,\ldots,\mu_{n-1}$ be the eigenvalues of the $(n-1)\times(n-1)$ corner $H^{(n-1)}$.
\begin{lemma}
	\label{lemma:corner_step}
	The distribution of $\mu_1,\ldots,\mu_{n-1}$ is the same as the distribution of
	the roots of the polynomial equation
	\begin{equation}
		\label{eq:polynomial_equation}
		\sum_{i=1}^n \frac{\xi_i}{z-\lambda_i}=0,
	\end{equation}
	where $\xi_i$ are i.i.d.\ random variables with the distribution $\chi^2_\beta$.
\end{lemma}
Recall also that this passage from $\lambda$ to $\mu$ works inductively, and 
the distribution of the next level eigenvalues $\nu=(\nu_1\ge \ldots \ge \nu_{n-2})$
is given by the same polynomial equation, but with $\lambda$ replaced by $\mu$.
In this way, we can define a \emph{Markov map} from $\lambda$ to $\mu$, which is then iterated
to construct the full array of eigenvalues of the corners of $H$.

For $\beta=\infty$, this map is deterministic, and is equivalent to successive differentiating the 
characteristic polynomial of $H$.

\subsection{Distribution of the eigenvalues of the corners}



\begin{theorem}
	\label{thm:corner_step}
	The density of $\mu$ with respect to the Lebesgue measure is given by
	\begin{equation*}
		\frac{\Gamma(N \beta/2)}{\Gamma(\beta/2)^n}
		\prod_{1\le i<j\le n-1}(\mu_i-\mu_j)
		\prod_{i=1}^{n-1}\prod_{j=1}^n |\mu_i-\lambda_j|^{\beta/2-1}
		\prod_{1\le i<j\le n}(\lambda_i-\lambda_j)^{1-\beta}.
	\end{equation*}
\end{theorem}
\begin{proof}
	Let $\varphi_i=\xi_i/\sum_{j=1}^n \xi_j$. The joint
	density of $(\varphi_1,\ldots,\varphi_n )$ is the
	(symmetric) Dirichlet density
	\begin{equation*}
		\frac{\Gamma(N \beta/2)}{\Gamma(\beta/2)^n}
		\ssp
		w_1^{\beta/2-1}\ldots  w_n^{\beta/2-1}
		\ssp
		dw_1\ldots dw_{n-1}
	\end{equation*}
	(note that the density is $(n-1)$-dimensional).

	We need to compute the Jacobian of the transformation from $\varphi$ to $\mu$,
	if we write
	\begin{equation*}
		\sum_{i=1}^n\frac{\varphi_i}{z-\lambda_i}=
		\frac{\prod_{i=1}^{n-1}(z-\mu_i)}
		{\prod_{i=1}^n(z-\lambda_i)},
	\end{equation*}
	and compute
	(as a decomposition into partial fractions):
	\begin{equation*}
		\varphi_a=
		\frac{\prod_{i=1}^{n-1}(\lambda_a-\mu_i)}{\prod_{i\ne a}(\lambda_a-\lambda_i)}.
	\end{equation*}
	Therefore,
	\begin{equation*}
		\frac{\partial\varphi_a}{\partial\mu_b}=
		\frac{\prod_{i=1}^{n-1}(\lambda_a-\mu_i)}{\prod_{i\ne a}(\lambda_a-\lambda_i)}
		\ssp\frac{1}{\mu_b-\lambda_a}.
	\end{equation*}
	The Jacobian is essentially the determinant of the matrix $1/(\mu_b-\lambda_a)$,
	which is the Cauchy determinant
	(Problem~\ref{prob:cauchy}).
	The final density is obtained from the symmetric Dirichlet density,
	but we plug in $w=\varphi$, and also multiply by the Jacobian.
	This completes the proof.
\end{proof}



\begin{corollary}[Joint density of the corners]
	\label{cor:corners_density}
	The eigenvalues $\lambda^(k)_j$, $1\le j\le k\le n$,
	of a random matrix from $\mathrm{Orbit}(\lambda)$
	form an interlacing array, with the joint density
	\begin{equation*}
		\propto
		\prod_{k=1}^n
		\prod_{1\le i<j\le k}\left(\lambda_j^{(k)}-\lambda_i^{(k)}\right)^{2-\beta}
		\prod_{a=1}^{k+1}\prod_{b=1}^k
		\left|\lambda_a^{(k+1)}-\lambda_b^{(k)}\right|^{\beta/2-1}.
	\end{equation*}
\end{corollary}
For $\beta=2$, all factors disappear, and we get the
uniform distribution on the interlacing array. This is the \emph{uniform Gibbs property}
which is important for other models, including discrete ensembles.


































\appendix
\setcounter{section}{7}

\section{Problems (due 2025-03-25)}


\subsection{Cauchy determinant}
\label{prob:cauchy}

Prove the Cauchy determinant formula:
\begin{equation*}
	\det\left( \frac{1}{x_i-y_j} \right)_{1\le i,j\le n}=
	\frac{\prod_{i<j}(x_i-x_j)(y_i-y_j)}{\prod_{i,j}(x_i-y_j)}.
\end{equation*}



\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

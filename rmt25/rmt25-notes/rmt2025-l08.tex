\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 8:  Cutting corners and loop equations}


\date{Wednesday, February 26, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l08.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents


\section{Cutting corners: polynomial equations and distribution}

\subsection{Recap: polynomial equations}

Recall the polynomial equation we proved in the last \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l07.pdf}{Lecture 7}.
Fix $\lambda=(\lambda_1\ge \ldots \ge \lambda_n )$.
Let $H\in \mathrm{Orbit}(\lambda)$ be a random Hermitian matrix
defined as
\begin{equation*}
	H=U\ssp \mathrm{\operatorname{diag}}(\lambda_1,\ldots,\lambda_n)\ssp U^\dagger,
\end{equation*}
where $U$ is Haar-distributed unitary matrix from $U(n)$.
This is the case $\beta=2$,
but the statement holds for the cases $\beta=1,4$ with appropriate modifications.
Let $\mu_1,\ldots,\mu_{n-1}$ be the eigenvalues of the $(n-1)\times(n-1)$ corner $H^{(n-1)}$.
\begin{lemma}
	\label{lemma:corner_step}
	The distribution of $\mu_1,\ldots,\mu_{n-1}$ is the same as the distribution of
	the roots of the polynomial equation
	\begin{equation}
		\label{eq:polynomial_equation}
		\sum_{i=1}^n \frac{\xi_i}{z-\lambda_i}=0,
	\end{equation}
	where $\xi_i$ are i.i.d.\ random variables with the distribution $\chi^2_\beta$.
\end{lemma}
Recall also that this passage from $\lambda$ to $\mu$ works inductively, and
the distribution of the next level eigenvalues $\nu=(\nu_1\ge \ldots \ge \nu_{n-2})$
is given by the same polynomial equation, but with $\lambda$ replaced by $\mu$.
In this way, we can define a \emph{Markov map} from $\lambda$ to $\mu$, which is then iterated
to construct the full array of eigenvalues of the corners of $H$.

For $\beta=\infty$, this map is deterministic, and is equivalent to successive differentiating the
characteristic polynomial of $H$.

\subsection{Extension to general $\beta$}

We extend the polynomial equations to general $\beta$,
by \emph{declaring} (defining) that the general $\beta$ corners distribution
is powered by the passage from $\lambda=(\lambda_1\ge \ldots \ge \lambda_n)$ to $\mu=(\mu_1\ge \ldots \ge \mu_{n-1})$,
where $\mu$ solves \eqref{eq:polynomial_equation} with $\xi_i$ i.i.d.\ $\chi^2_\beta$.
In this way, $\mu$ interlaces with $\lambda$.
For $\beta=1,2,4$, this definition reduces to the one with invariant ensembles
with fixed eigenvalues $\lambda$.


\subsection{Distribution of the eigenvalues of the corners}

Let $\mu$ be obtained from $\lambda$ by the general $\beta$ corners operation.

\begin{theorem}
	\label{thm:corner_step}
	The density of $\mu$ with respect to the Lebesgue measure is given by
	\begin{equation*}
		\frac{\Gamma(N \beta/2)}{\Gamma(\beta/2)^n}
		\prod_{1\le i<j\le n-1}(\mu_i-\mu_j)
		\prod_{i=1}^{n-1}\prod_{j=1}^n |\mu_i-\lambda_j|^{\beta/2-1}
		\prod_{1\le i<j\le n}(\lambda_i-\lambda_j)^{1-\beta}.
	\end{equation*}
\end{theorem}
\begin{proof}
	Let $\varphi_i=\xi_i/\sum_{j=1}^n \xi_j$.
	It is well-known\footnote{See Problem~\ref{prob:dirichlet}.}
	the joint
	density of $(\varphi_1,\ldots,\varphi_n )$ is the
	(symmetric) Dirichlet density
	\begin{equation*}
		\frac{\Gamma(N \beta/2)}{\Gamma(\beta/2)^n}
		\ssp
		w_1^{\beta/2-1}\ldots  w_n^{\beta/2-1}
		\ssp
		dw_1\ldots dw_{n-1}
	\end{equation*}
	(note that the density is $(n-1)$-dimensional).

	We need to compute the Jacobian of the transformation from $\varphi$ to $\mu$,
	if we write
	\begin{equation*}
		\sum_{i=1}^n\frac{\varphi_i}{z-\lambda_i}=
		\frac{\prod_{i=1}^{n-1}(z-\mu_i)}
		{\prod_{i=1}^n(z-\lambda_i)},
	\end{equation*}
	and compute
	(as a decomposition into partial fractions):
	\begin{equation*}
		\varphi_a=
		\frac{\prod_{i=1}^{n-1}(\lambda_a-\mu_i)}{\prod_{i\ne a}(\lambda_a-\lambda_i)}.
	\end{equation*}
	Therefore,
	\begin{equation*}
		\frac{\partial\varphi_a}{\partial\mu_b}=
		\frac{\prod_{i=1}^{n-1}(\lambda_a-\mu_i)}{\prod_{i\ne a}(\lambda_a-\lambda_i)}
		\ssp\frac{1}{\mu_b-\lambda_a}.
	\end{equation*}
	The Jacobian is essentially the determinant of the matrix $1/(\mu_b-\lambda_a)$,
	which is the Cauchy determinant
	(Problem~\ref{prob:cauchy}).
	The final density is obtained from the symmetric Dirichlet density,
	but we plug in $w=\varphi$, and also multiply by the Jacobian.
	This completes the proof.
\end{proof}



\begin{corollary}[Joint density of the corners]
	\label{cor:corners_density}
	The eigenvalues $\lambda^(k)_j$, $1\le j\le k\le n$,
	of a random matrix from $\mathrm{Orbit}(\lambda)$
	form an interlacing array, with the joint density
	\begin{equation*}
		\propto
		\prod_{k=1}^n
		\prod_{1\le i<j\le k}\left(\lambda_j^{(k)}-\lambda_i^{(k)}\right)^{2-\beta}
		\prod_{a=1}^{k+1}\prod_{b=1}^k
		\left|\lambda_a^{(k+1)}-\lambda_b^{(k)}\right|^{\beta/2-1}.
	\end{equation*}
\end{corollary}
For $\beta=2$, all factors disappear, and we get the
uniform distribution on the interlacing array. This is the \emph{uniform Gibbs property}
which is important for other models, including discrete ensembles.


\section{Loop equations}

Let us write down the \emph{loop equations} for the passage from the
eigenvalues $\lambda$ to the eigenvalues $\mu$.
These loop equations are due to \cite{gorin2022dynamical}
by a limit from a discrete system (related to Jack
symmetric polynomials). Note that despite the name, these are not \textbf{equations},
but rather a statement that some expectations are holomorphic.

\subsection{Formulation}

\begin{theorem} \label{Theorem_loop_equation}
 We fix $n=1,2,\dots$ and $n+1$ real numbers $\lambda_1\ge\dots\ge\lambda_{n+1}$. For $\beta>0$, consider $n+1$ i.i.d.\ $\chi^2_\beta$ random variables $\xi_i$ and set
 $$
  w_i=\frac{\xi_i}{\sum_{j=1}^{n+1} \xi_j}, \qquad 1\le i \le n+1.
 $$
 We define $n$ random points $\{\mu_1,\dots,\mu_n\}$ as $n$ solutions to the equation
 \begin{equation} \label{eq_mu_equation}
  \sum_{i=1}^{n+1} \frac{w_i}{z-\lambda_i}=0.
 \end{equation}
 Take any \emph{polynomial} $W(z)$ and consider the complex function:
 \begin{equation}
 \label{eq_loop_observable}
   f_W(z)=\mathbb{E}\left[\prod_{j=1}^n \exp\bigl(W(\mu_j)\bigr) \frac{\prod_{i=1}^{n+1} (z-\lambda_i)}{\prod_{j=1}^n (z-\mu_j)} \left( W'(z)+\sum_{i=1}^{n+1} \frac{\beta/2-1}{z-\lambda_i} + \sum_{j=1}^n \frac{1}{z-\mu_j}\right)\right].
 \end{equation}
 Then $f_W(z)$ is an \emph{entire function} of $z$, in the following sense:
 \begin{itemize}
	 \item For $z\in \mathbb{C}\setminus [\lambda_{n+1},\lambda_1]$, the expectation in \eqref{eq_loop_observable} defines a holomorphic function of $z$.
  \item This function has an analytic continuation to $\mathbb{C}$, which has no singularities.
 \end{itemize}
\end{theorem}
\begin{remark}
 Note that for $z$ in 
 $[\lambda_{n+1},\lambda_1]$, the integral determining
 \eqref{eq_loop_observable} might be divergent, and,
 therefore, analytic continuation is the proper way to
 define $f_W(z)$, $z\in \Omega$. 
\end{remark}


\subsection{Proof}



\section{Applications of loop equations}

























\appendix
\setcounter{section}{7}

\section{Problems (due 2025-03-25)}


\subsection{Cauchy determinant}
\label{prob:cauchy}

Prove the Cauchy determinant formula:
\begin{equation*}
	\det\left( \frac{1}{x_i-y_j} \right)_{1\le i,j\le n}=
	\frac{\prod_{i<j}(x_i-x_j)(y_i-y_j)}{\prod_{i,j}(x_i-y_j)}.
\end{equation*}


\subsection{Dirichlet density}
\label{prob:dirichlet}

Find or prove the first statement in the proof of
\Cref{thm:corner_step} about the symmetric Dirichlet density arising from
normalizing the $\xi_i$'s to $\varphi_i$'s.




\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

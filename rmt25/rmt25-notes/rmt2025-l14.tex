\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 14: Matching Random Matrices to Random Growth II}


\date{Wednesday, April 16, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l14.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents



\section{Recap}

\subsection{Main goal}

In the previous
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13}, we began establishing a remarkable correspondence between two a~priori different objects:

\begin{itemize}
\item The \emph{spiked Wishart ensemble}: an $n\times n$ Hermitian random-matrix process $\{M(t)\}_{t\ge0}$ whose entries come from columns of independent Gaussian random vectors of suitably chosen covariance.
\item An \emph{inhomogeneous last-passage percolation (LPP)} model: an array $\{W_{i,j}\}$ of exponential random weights on a portion of the two-dimensional lattice, whose last-passage times $L(t,n)$ match the largest eigenvalues of $M(t)$, jointly for all $t\in \mathbb{Z}_{\ge0}$.
\end{itemize}

This equivalence, originally due to
\cite{dieker2008largest} (following
\cite{defosseux2010orbit}, \cite{forrester2006jacobians};
see also
\cite{Baryshnikov_GUE2001},
\cite{johansson2000shape} for earlier results of this kind),
can be fully understood by passing to a
\emph{discrete} version of LPP with geometric site-weights
and then applying the \emph{Robinson--Schensted--Knuth}
(RSK) correspondence.

\subsection{Spiked Wishart ensembles and the largest eigenvalue process}

We defined the \emph{generalized} (or spiked) Wishart matrix $M(t)$ of size $n\times n$ by setting
\[
M(t)\;=\;\sum_{m=1}^t A^{(m)}\bigl(A^{(m)}\bigr)^*
\]
where $\{A^{(m)}\}_{m=1}^\infty$ are i.i.d.\ complex Gaussian column vectors of length $n$, with
\[
\mathrm{Var}\bigl(A^{(m)}_i\bigr)
\;=\;
\frac{1}{\pi_i + \hat\pi_m}\,.
\]
Here, $\pi=(\pi_1,\dots,\pi_n)$ and $\hat\pi=(\hat\pi_1,\hat\pi_2,\dots)$ are positive and nonnegative parameters, respectively.  Writing $\lambda_1(t)\ge\cdots\ge\lambda_n(t)\ge0$ for the eigenvalues of $M(t)$, we then saw:

\begin{enumerate}
\item The vectors $\lambda(t)=\bigl(\lambda_1(t),\dots,\lambda_n(t)\bigr)$ form a Markov chain in the \emph{Weyl chamber} $\mathbb{W}^n = \{x_1\ge\cdots\ge x_n\ge0\}$.
\item There is an \emph{interlacing} property: each update $M(t-1)\mapsto M(t)$ via the rank-one matrix $A^{(t)}\bigl(A^{(t)}\bigr)^*$ forces $\lambda(t)$ to interlace with $\lambda(t-1)$:
\[
\lambda_1(t)\;\ge\;\lambda_1(t-1)\;\ge\;\lambda_2(t)\;\ge\;\cdots\;\ge\;\lambda_n(t-1)\;\ge\;\lambda_n(t).
\]
\end{enumerate}

In \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13},
we wrote down the
transition kernel
from $\lambda(t-1)$ to $\lambda(t)$:

\begin{theorem}[\cite{dieker2008largest}]
\label{thm:MarkovChain}
Fix an integer \(n\ge1\).  Let \(\pi=(\pi_1,\dots,\pi_n)\) be a strictly positive \(n\)-vector, and let \(\widehat\pi=(\widehat\pi_1,\widehat\pi_2,\dots)\) be any sequence of nonnegative real parameters.  Under the probability measure \(P^{\pi,\widehat\pi}\), the eigenvalues of the \(n\times n\) generalized Wishart matrices \(\{M(t)\}_{t\ge0}\) form a time-inhomogeneous Markov chain \(\{\mathrm{sp}(M(t))\}_{t\ge0}\) in the Weyl chamber
\[
\mathbb{W}^n
\;=\;
\bigl\{\,x=(x_1,\dots,x_n)\in\mathbb{R}^n_{\ge0}:
x_1\ge x_2\ge\cdots\ge x_n\bigr\}.
\]
More precisely, writing \(x=\mathrm{sp}(M(t-1))\) and \(y=\mathrm{sp}(M(t))\), the one-step transition law from time \((t-1)\) to \(t\) is absolutely continuous on the interior of \(\mathbb{W}^n\) and can be factored as
\begin{equation}
\label{eq:transition-density}
Q_{t-1,t}^{\pi,\widehat\pi}(x,\,dy)
\;=\;
\Bigl[\,
\prod_{i=1}^n \bigl(\pi_i+\widehat\pi_{t}\bigr)
\Bigr]
\cdot
\frac{h_{\pi}(y)}{h_{\pi}(x)}
\;\exp\Bigl(-(\widehat\pi_{t}-1)\sum_{i=1}^n (y_i - x_i)\Bigr)
\;\times\;Q^{(0)}\bigl(x,\,dy\bigr),
\end{equation}
where
\begin{itemize}
\item \(\displaystyle Q^{(0)}\bigl(x,\,dy\bigr)\) is the \emph{standard} (null-spike) Wishart transition kernel, given explicitly by
	\begin{equation}
		\label{eq:Q0}
Q^{(0)}(x,\,dy)
\;=\;
\frac{\Delta(y)}{\Delta(x)}\;\exp\Bigl(\,-\sum_{i=1}^n (y_i - x_i)\Bigr)\,
\mathbf{1}_{\{x\prec y\}}\;dy,
\end{equation}
with \(\Delta(z)=\prod_{1\le i<j\le n}(z_i - z_j)\) the Vandermonde determinant.

\item The function \(h_{\pi}\) is the (continuous) Harish-Chandra orbit integral factor
\[
h_{\pi}(z)
\;=\;
\frac{(-1)^{\binom n2}}{0! 1! \cdots (n-1)! }
\frac{\det\bigl(e^{-\pi_i\,z_j}\bigr)_{i,j=1}^n}{\Delta(\pi)\,\Delta(z)}.
\]
Note that $h_\pi(0)=1$.
\end{itemize}
In particular, the chain starts from \(\mathrm{sp}(M(0))=0\) (the zero matrix).
\end{theorem}




\subsection{Inhomogeneous last-passage percolation}

On the random growth side, we considered an array of
site-weights $\{W_{i,j}\}_{i,j\ge1}$ such that each
$W_{i,j}$ is exponentially distributed with rate $\pi_i +
\hat\pi_j$.  For every integer $t\ge1$, we define $L(t,n)$
to be the maximum total weight of all up-right paths from
$(1,1)$ to $(t,n)$:
\[
L(t,n)\;=\;\max_{\Gamma:\,(1,1)\to(t,n)} \;\sum_{(i,j)\in\Gamma}\; W_{i,j}.
\]
One checks that $L(\,\cdot\,,n)$ satisfies a simple additive recursion:
\[
L(i,j)
\;=\;
W_{i,j}\;+\;\max\bigl\{\,L(i-1,j),\;L(i,j-1)\bigr\},
\]
The main claim which we show in today's lecture is the equality in distribution:
\begin{equation}
	\label{eq:main-claim}
\bigl(L(1,n),\,L(2,n),\,\dots,\,L(t,n)\bigr)
\;\;\stackrel{d}{=}\;\;
\bigl(\lambda_1(1),\,\lambda_1(2),\,\dots,\,\lambda_1(t)\bigr).
\end{equation}

\subsection{RSK via toggles: definitions and weight preservation}

The \emph{Robinson--Schensted--Knuth} correspondence (RSK) was the main new mechanism in
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13}.
In our setup, we adopt a \emph{toggle-based} viewpoint: we
encode arrays by diagonals and successively \emph{toggle}
the diagonals to achieve a fully \emph{ordered} array $R$.
The key to how RSK links LPP and random matrices is its \emph{weight preservation} property.

We work with arrays $W=\left\{ W_{ij} \right\}_{1\le i\le t,\ 1\le j\le n}$
and $R=\left\{ R_{ij} \right\}_{1\le i\le t,\ 1\le j\le n}$,
where $W$ is a nonnegative integer array and $R$ is an ordered array, that is,
$R_{i,j}\le R_{i,j+1}$ and $R_{i,j}\le R_{i+1,j}$ for all $i,j$. Using RSK,
we showed in \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13} that
there is a bijection which maps $W$ to $R$.

We also started to prove the following result, which we now complete:

\begin{theorem}[Weight preservation]
\label{thm:WeightPreservationRecap}
Let $W=\{W_{i,j}\}$ be a nonnegative integer array, and $R=\mathrm{RSK}(W)$.  Denote
\[
\mathrm{row}_i \;=\; \sum_{j=1}^{n} W_{i,j},
\quad
\mathrm{col}_j \;=\; \sum_{i=1}^{t} W_{i,j}
\]
(which are essentially the cdf's of the array $W$\/),
and for $R$ define the diagonal sums starting at each $(i,j)$ and going diagonally down and to the right:
\[
\mathrm{diag}_{i,j}
\;=\;
\sum_{k=0}^{\min(i,j)-1} R_{\,i-k,\,j-k}.
\]
Then for each $1\le j\le n$ and $1\le i\le t$, we have
\begin{equation}
\label{eq:diagIdentityRecap}
\mathrm{diag}_{t,j}
\;=\;
\sum_{m=1}^{j}\mathrm{col}_{m},
\qquad
\mathrm{diag}_{i,n}
\;=\;
\sum_{m=1}^{i}\mathrm{row}_{m}.
\end{equation}
In particular,
the total sum of $W$ over all cells equals the total sum of $R$ over all cells.
\end{theorem}

\begin{proof}[Proof (sketch)]
	One inductively builds $R$ by adding the sites $(i,j)$ one at a time.  Each toggle modifies exactly one diagonal.
	After adding a box $(i,j)$, the diagonal-sum identity
\[
\mathrm{diag}_{i,j}
\;=\;
\mathrm{diag}_{i-1,j} + \mathrm{diag}_{i,j-1}
\;-\;
\mathrm{diag}_{i-1,j-1}
\;+\;
W_{i,j}
\]
holds, expressing that $W$ captures the discrete ``mixed second differences'' of the diagonal sums in $R$.
Thus, the cdf's of $W$ must coincide with the diagonal sums of $R$, as desired.
\end{proof}

\section{Distributions of last-passage times in geometric LPP}
\label{sec:distributions_geometric_LPP}

\subsection{Conditional distribution}

Recall that we are working with the independent geometric random variables
\begin{equation*}
	\operatorname{Prob}\left( W_{ij}=k \right)=(a_ib_j)^k(1-a_ib_j),\qquad k=0,1,\ldots.
\end{equation*}
The parameters $a_1,\ldots,a_t $ and $b_1,\ldots,b_n$ are positive real numbers, and we assume that $a_i b_j<1$ for all $i,j$,
so that the random variables $W_{ij}$ are well-defined.

Recall the notation
\begin{equation*}
	Z(t)=\left( L(t,1),\ldots,L(t,n)  \right),\qquad t\in\mathbb{Z}_{\ge0}.
\end{equation*}
Using the weight-preservation property (\Cref{thm:WeightPreservationRecap}), we can now 
compute the conditional distribution of 
$Z(t)$ given $Z(t-1)$, and, in particular, show that this is a Markov chain.

\begin{theorem}
	\label{thm:conditional_distribution_Z}
\end{theorem}




































\appendix
\setcounter{section}{13}

\section{Problems (due 2025-04-29)}

\subsection{Non-Markovianity}

Show that the sequence of random variables defined in the exponential LPP model,
\[
L(1,n),L(2,n),\dots,L(t,n),
\]
is \textbf{not} a Markov chain.
By virtue of the equivalence with the spiked Wishart ensemble \eqref{eq:main-claim},
you may alternatively show that the sequence
of maximal eigenvalues
\[
\lambda_1(1),\lambda_1(2),\dots,\lambda_1(t)
\]
of successive
Wishart matrices $M(1),M(2),\dots,M(t)$ is \textbf{not} a Markov chain either.




\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

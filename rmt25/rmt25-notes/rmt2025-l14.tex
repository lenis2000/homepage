\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 14: Matching Random Matrices to Random Growth II}


\date{Wednesday, April 16, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l14.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle
\tableofcontents



\section{Recap}

\subsection{Main goal}

In the previous
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13}, we began establishing a remarkable correspondence between two a~priori different objects:

\begin{itemize}
\item The \emph{spiked Wishart ensemble}: an $n\times n$ Hermitian random-matrix process $\{M(t)\}_{t\ge0}$ whose entries come from columns of independent Gaussian random vectors of suitably chosen covariance.
\item An \emph{inhomogeneous last-passage percolation (LPP)} model: an array $\{W_{i,j}\}$ of exponential random weights on a portion of the two-dimensional lattice, whose last-passage times $L(t,n)$ match the largest eigenvalues of $M(t)$, jointly for all $t\in \mathbb{Z}_{\ge0}$.
\end{itemize}

This equivalence, originally due to
\cite{dieker2008largest} (following
\cite{defosseux2010orbit}, \cite{forrester2006jacobians};
see also
\cite{Baryshnikov_GUE2001},
\cite{johansson2000shape} for earlier results of this kind),
can be fully understood by passing to a
\emph{discrete} version of LPP with geometric site-weights
and then applying the \emph{Robinson--Schensted--Knuth}
(RSK) correspondence.

\subsection{Spiked Wishart ensembles and the largest eigenvalue process}

We defined the \emph{generalized} (or spiked) Wishart matrix $M(t)$ of size $n\times n$ by setting
\[
M(t)\;=\;\sum_{m=1}^t A^{(m)}\bigl(A^{(m)}\bigr)^*
\]
where $\{A^{(m)}\}_{m=1}^\infty$ are i.i.d.\ complex Gaussian column vectors of length $n$, with
\[
\mathrm{Var}\bigl(A^{(m)}_i\bigr)
\;=\;
\frac{1}{\pi_i + \hat\pi_m}\,.
\]
Here, $\pi=(\pi_1,\dots,\pi_n)$ and $\hat\pi=(\hat\pi_1,\hat\pi_2,\dots)$ are positive and nonnegative parameters, respectively.  Writing $\lambda_1(t)\ge\cdots\ge\lambda_n(t)\ge0$ for the eigenvalues of $M(t)$, we then saw:

\begin{enumerate}
\item The vectors $\lambda(t)=\bigl(\lambda_1(t),\dots,\lambda_n(t)\bigr)$ form a Markov chain in the \emph{Weyl chamber} $\mathbb{W}^n = \{x_1\ge\cdots\ge x_n\ge0\}$.
\item There is an \emph{interlacing} property: each update $M(t-1)\mapsto M(t)$ via the rank-one matrix $A^{(t)}\bigl(A^{(t)}\bigr)^*$ forces $\lambda(t)$ to interlace with $\lambda(t-1)$:
\[
\lambda_1(t)\;\ge\;\lambda_1(t-1)\;\ge\;\lambda_2(t)\;\ge\;\cdots\;\ge\;\lambda_n(t-1)\;\ge\;\lambda_n(t).
\]
\end{enumerate}

In \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13},
we wrote down the
transition kernel
from $\lambda(t-1)$ to $\lambda(t)$:

\begin{theorem}[\cite{dieker2008largest}]
\label{thm:MarkovChain}
Fix an integer \(n\ge1\).  Let \(\pi=(\pi_1,\dots,\pi_n)\) be a strictly positive \(n\)-vector, and let \(\widehat\pi=(\widehat\pi_1,\widehat\pi_2,\dots)\) be any sequence of nonnegative real parameters.  Under the probability measure \(P^{\pi,\widehat\pi}\), the eigenvalues of the \(n\times n\) generalized Wishart matrices \(\{M(t)\}_{t\ge0}\) form a time-inhomogeneous Markov chain \(\{\mathrm{sp}(M(t))\}_{t\ge0}\) in the Weyl chamber
\[
\mathbb{W}^n
\;=\;
\bigl\{\,x=(x_1,\dots,x_n)\in\mathbb{R}^n_{\ge0}:
x_1\ge x_2\ge\cdots\ge x_n\bigr\}.
\]
More precisely, writing \(x=\mathrm{sp}(M(t-1))\) and \(y=\mathrm{sp}(M(t))\), the one-step transition law from time \((t-1)\) to \(t\) is absolutely continuous on the interior of \(\mathbb{W}^n\) and can be factored as
\begin{equation}
\label{eq:transition-density}
Q_{t-1,t}^{\pi,\widehat\pi}(x,\,dy)
\;=\;
\Bigl[\,
\prod_{i=1}^n \bigl(\pi_i+\widehat\pi_{t}\bigr)
\Bigr]
\cdot
\frac{h_{\pi}(y)}{h_{\pi}(x)}
\;\exp\Bigl(-(\widehat\pi_{t}-1)\sum_{i=1}^n (y_i - x_i)\Bigr)
\;\times\;Q^{(0)}\bigl(x,\,dy\bigr),
\end{equation}
where
\begin{itemize}
\item \(\displaystyle Q^{(0)}\bigl(x,\,dy\bigr)\) is the \emph{standard} (null-spike) Wishart transition kernel, given explicitly by
	\begin{equation}
		\label{eq:Q0}
Q^{(0)}(x,\,dy)
\;=\;
\frac{\Delta(y)}{\Delta(x)}\;\exp\Bigl(\,-\sum_{i=1}^n (y_i - x_i)\Bigr)\,
\mathbf{1}_{\{x\prec y\}}\;dy,
\end{equation}
with \(\Delta(z)=\prod_{1\le i<j\le n}(z_i - z_j)\) the Vandermonde determinant.

\item The function \(h_{\pi}\) is the (continuous) Harish-Chandra orbit integral factor
\[
h_{\pi}(z)
\;=\;
\frac{(-1)^{\binom n2}}{0! 1! \cdots (n-1)! }
\frac{\det\bigl(e^{-\pi_i\,z_j}\bigr)_{i,j=1}^n}{\Delta(\pi)\,\Delta(z)}.
\]
Note that $h_\pi(0)=1$.
\end{itemize}
In particular, the chain starts from \(\mathrm{sp}(M(0))=0\) (the zero matrix).
\end{theorem}




\subsection{Inhomogeneous last-passage percolation}

On the random growth side, we considered an array of
site-weights $\{W_{i,j}\}_{i,j\ge1}$ such that each
$W_{i,j}$ is exponentially distributed with rate $\pi_i +
\hat\pi_j$.  For every integer $t\ge1$, we define $L(t,n)$
to be the maximum total weight of all up-right paths from
$(1,1)$ to $(t,n)$:
\[
L(t,n)\;=\;\max_{\Gamma:\,(1,1)\to(t,n)} \;\sum_{(i,j)\in\Gamma}\; W_{i,j}.
\]
One checks that $L(\,\cdot\,,n)$ satisfies a simple additive recursion:
\[
L(i,j)
\;=\;
W_{i,j}\;+\;\max\bigl\{\,L(i-1,j),\;L(i,j-1)\bigr\},
\]
The main claim which we show in today's lecture is the equality in distribution:
\begin{equation}
	\label{eq:main-claim}
\bigl(L(1,n),\,L(2,n),\,\dots,\,L(t,n)\bigr)
\;\;\stackrel{d}{=}\;\;
\bigl(\lambda_1(1),\,\lambda_1(2),\,\dots,\,\lambda_1(t)\bigr).
\end{equation}

\subsection{RSK via toggles: definitions and weight preservation}

The \emph{Robinson--Schensted--Knuth} correspondence (RSK) was the main new mechanism in
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13}.
In our setup, we adopt a \emph{toggle-based} viewpoint: we
encode arrays by diagonals and successively \emph{toggle}
the diagonals to achieve a fully \emph{ordered} array $R$.
The key to how RSK links LPP and random matrices is its \emph{weight preservation} property.

We work with arrays $W=\left\{ W_{ij} \right\}_{1\le i\le t,\ 1\le j\le n}$
and $R=\left\{ R_{ij} \right\}_{1\le i\le t,\ 1\le j\le n}$,
where $W$ is a nonnegative integer array and $R$ is an ordered array, that is,
$R_{i,j}\le R_{i,j+1}$ and $R_{i,j}\le R_{i+1,j}$ for all $i,j$. Using RSK,
we showed in \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.pdf}{Lecture 13} that
there is a bijection which maps $W$ to $R$.

We also started to prove the following result, which we now complete:

\begin{theorem}[Weight preservation]
\label{thm:WeightPreservationRecap}
Let $W=\{W_{i,j}\}$ be a nonnegative integer array, and $R=\mathrm{RSK}(W)$.  Denote
\[
\mathrm{row}_i \;=\; \sum_{j=1}^{n} W_{i,j},
\quad
\mathrm{col}_j \;=\; \sum_{i=1}^{t} W_{i,j}
\]
(which are essentially the cdf's of the array $W$\/),
and for $R$ define the diagonal sums starting at each $(i,j)$ and going diagonally down and to the right:
\[
\mathrm{diag}_{i,j}
\;=\;
\sum_{k=0}^{\min(i,j)-1} R_{\,i-k,\,j-k}.
\]
Then for each $1\le j\le n$ and $1\le i\le t$, we have
\begin{equation}
\label{eq:diagIdentityRecap}
\mathrm{diag}_{t,j}
\;=\;
\sum_{m=1}^{j}\mathrm{col}_{m},
\qquad
\mathrm{diag}_{i,n}
\;=\;
\sum_{m=1}^{i}\mathrm{row}_{m}.
\end{equation}
In particular,
the total sum of $W$ over all cells equals the total sum of $R$ over all cells.
\end{theorem}

\begin{proof}[Proof (sketch)]
	One inductively builds $R$ by adding the sites $(i,j)$ one at a time.  Each toggle modifies exactly one diagonal.
	After adding a box $(i,j)$, the diagonal-sum identity
\[
\mathrm{diag}_{i,j}
\;=\;
\mathrm{diag}_{i-1,j} + \mathrm{diag}_{i,j-1}
\;-\;
\mathrm{diag}_{i-1,j-1}
\;+\;
W_{i,j}
\]
holds, expressing that $W$ captures the discrete ``mixed second differences'' of the diagonal sums in $R$.
Thus, the cdf's of $W$ must coincide with the diagonal sums of $R$, as desired.
\end{proof}

\section{Distributions of last-passage times in geometric LPP}
\label{sec:distributions_geometric_LPP}

\subsection{Matching RSK to last-passage percolation}

Recall that we are working with the independent geometric random variables
\begin{equation*}
	\operatorname{Prob}\left( W_{ij}=k \right)=(a_ib_j)^k(1-a_ib_j),\qquad k=0,1,\ldots.
\end{equation*}
The parameters $a_1,\ldots,a_t $ and $b_1,\ldots,b_n$ are positive real numbers, and we assume that $a_i b_j<1$ for all $i,j$,
so that the random variables $W_{ij}$ are well-defined.
Let $R=\operatorname{RSK}(W)$.
\begin{lemma}
	\label{lemma:L_as_R}
	The distribution of the top row of the array $R$,
	$R_{t,1},\ldots,R_{t,n}$,
	is the same as the distribution of the last-passage times
	$L(t,1),\ldots,L(t,n)$, defined in the same environment $W=\{W_{ij}\}$.
\end{lemma}
Note that this statement does not rely on the exact distribution of $W$,
and holds for any fixed or random nonnegative integer array $W$.
\begin{proof}[Proof of \Cref{lemma:L_as_R}]
	The values in $R$ update according to the toggle rule.
	Denote by $R^{(i)}$ the array obtained after toggling the $i$-th row (and all previous rows) of $W$.
	Then, the top row of $R^{(i)}$ updates as
	\begin{equation*}
		R^{(i)}_{i,j}=
		W_{i,j}+\max\big\{R^{(i-1)}_{i-1,j},
		R^{(i)}_{i,j-1}\big\}.
	\end{equation*}
	By the induction hypothesis, we have
	\begin{equation*}
		R^{(i-1)}_{i-1,j}=L(i-1,j),\qquad R^{(i)}_{i,j-1}=L(i,j-1).
	\end{equation*}
	This implies that $L(i,j)=R^{(i)}_{i,j}$,
	and we may proceed by induction on $j$ and then on $i$.
\end{proof}

\begin{remark}
	The correspondence between $R_{t,j}$ and $L(t,j)$
	holds only for the top row of the final array
	$R=R^{(t)}$. For rows below the top row (i.e., for
	$R_{k,j}$ with $k < t$), there is no such direct
	correspondence with one-path last-passage times.
	On the other hand, the whole array $R$ can be defined
	through multipath last-passage times. This is
	known as \emph{Greene's theorem} \cite{sagan2001symmetric} for RSK,
	and falls outside the scope of this course.
\end{remark}

\subsection{Distributions in RSK}
\label{sub:distribution_RSK}

Fix $t,n$, and consider the following quantities
in a diagonal of the array $R=\mathrm{RSK}(W)$:
\begin{equation*}
	\lambda_1\coloneqq R_{t,n},
	\lambda_2\coloneqq R_{t-1,n-1},\ldots,
	\lambda_n\coloneqq R_{t-n+1,1}.
\end{equation*}
Clearly, $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_n$ (we pad $\mathrm{diag}$'s by zeroes if necessary),
and these are integers. We regard $\lambda=(\lambda_1,\ldots,\lambda_n)$ as an integer partition,
or a Young diagram.
Denote by $T(\lambda)$ the
space of all \emph{semistandard Young tableaux} (SSYT) of shape $\lambda$, that is,
all collections of numbers $r_{ij}$ which interlace as
\begin{equation*}
	r_{i,j}\le r_{i,j+1},\quad r_{i,j}\le r_{i+1,j},
	\quad
	i=1,\ldots,t,\ j=1,\ldots,n;
	\qquad
	r_{t-k+1,n-k+1}=\lambda_k,\quad k=1,\ldots,n.
\end{equation*}
We are after the distribution of the random Young diagram $\lambda$.

\begin{definition}[Schur polynomial]
	For a partition $\lambda=(\lambda_1,\ldots,\lambda_n)$ with $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$, the Schur polynomial $s_\lambda(x_1,\ldots,x_n)$ in $n$ variables is defined as:
	\begin{equation}
		\label{eq:Schur_polynomial}
		s_\lambda(x_1,\ldots,x_n) =
		\frac{\det(x_i^{\lambda_j+n-j})_{i,j=1}^n}{\det(x_i^{n-j})_{i,j=1}^n}
		= \frac{\det(x_i^{\lambda_j+n-j})_{i,j=1}^n}{\prod_{1
		\leq i < j \leq n}(x_i-x_j)}.
	\end{equation}
	Alternatively, the Schur polynomial has a combinatorial interpretation as a sum over semistandard Young tableaux:
	\begin{equation}
		\label{eq:Schur_polynomial_combinatorial}
		s_\lambda(x_1,\ldots,x_n) = \sum_{T \in T(\lambda)}
		x_n^{\lambda_1+\ldots+\lambda+n }
		\left( \frac{x_{n-1}}{x_n} \right)^{r_{t,n-1}+r_{t-1,n-2}+\ldots+r_{t-n+2,1} }
		\ldots
		\left( \frac{x_2}{x_3} \right)^{r_{t,2}+r_{t-1,1}}
		\left( \frac{x_1}{x_2} \right)^{r_{t,1}},
	\end{equation}
	where $T(\lambda)$ is the set of all semistandard Young tableaux of shape $\lambda$,
	as defined above.
\end{definition}

From \eqref{eq:Schur_polynomial}, it is evident that $s_\lambda(x_1,\ldots,x_n )$ is a
symmetric polynomial in $x_1,\ldots,x_n$.
This is highly non-obvious from the combinatorial definition \eqref{eq:Schur_polynomial_combinatorial}.
See~Problem~\ref{prob:Schur_polynomials_equivalence} for a proof of the equivalence of the two definitions.

The Schur polynomials satisfy the stability property:
\begin{equation}
	\label{eq:stability_property_Schur}
	s_\lambda(x_1,\ldots,x_{n-1},x_n)\big\vert_{x_n=0} =
	\begin{cases}
		s_\lambda(x_1,\ldots,x_{n-1}) & \text{if } \lambda_n=0,\\
		0 & \text{otherwise.}
	\end{cases}
\end{equation}

\begin{theorem}
	\label{thm:Schur_measure}
	Let $\mu=(\mu_1,\ldots,\mu_n)$ be a fixed Young diagram.
	Then, for $R=\operatorname{RSK}(W)$, where $W$ is the array of independent geometric random variables,
	we have
	\begin{equation}
		\label{eq:Schur_measure}
		\operatorname{Prob}\left( R_{t,n}=\mu_1, \ldots, R_{t-n+1,1}=\mu_n \right)
		=
		\prod_{i=1}^t\prod_{j=1}^n
		(1-a_i b_j)\cdot s_\mu(a_1,\ldots,a_t )s_\mu(b_1,\ldots,b_n ).
	\end{equation}
\end{theorem}
Note that if $t<n$, then $\mu_{t+1}=\ldots=\mu_n=0 $, as it should be.
Note also that the statement of the theorem implies that the
expressions in the right-hand side of \eqref{eq:Schur_measure}
sum to one over all $\mu_1\ge \ldots\ge \mu_n\ge0 $,
which is the celebrated \emph{Cauchy identity} for Schur polynomials.
One can alternatively establish the Cauchy identity from the
Cauchy-Binet formula, using the determinantal formulas
\eqref{eq:Schur_polynomial}. See Problem~\ref{prob:Cauchy_identity}.
\begin{proof}[Proof of \Cref{thm:Schur_measure}]
	To get the probability
	\eqref{eq:Schur_measure}, we need to sum the probability weights of all
	ordered arrays $R=(R_{ij})_{1\le i\le t,\ 1\le j\le n}$, such that
	\begin{equation*}
		R_{t,j}=\mu_1,\quad R_{t-1,j-1}=\mu_2,\ldots,
		R_{t-n+1,1}=\mu_n.
	\end{equation*}
	Denote the set of such arrays by $\mathcal{R}(\mu)$.
	Each $R\in \mathcal{R}(\mu)$ has a probability weight which we can express (thanks to the
	RSK bijection) in terms of the original array $W$,
	so in terms of the parameters $a_i$ and $b_j$.

	Our first observation is that the probability weight of $R=\operatorname{RSK}(W)$ depends only on its
	diagonal sums $\mathrm{diag}_{1,n},\ldots,\mathrm{diag}_{t,n},\mathrm{diag}_{t,n-1},\ldots,\mathrm{diag}_{t,1} $
	along the right and the top borders.
	Indeed, knowing these diagonal sums, we know (by the weight-preservation property of RSK, \Cref{thm:WeightPreservationRecap})
	the row and column sums of $W$. However, the joint distribution of all
	elements of $W$ has the following form:
	\begin{equation}
		\label{eq:Prob_W}
		\begin{split}
			\operatorname{Prob}\left( W_{ij}=k_{ij}
			\ \textnormal{for all } i,j \right)
			&=
			\prod_{i=1}^t\prod_{j=1}^n
			(1-a_i b_j)\cdot (a_ib_j)^{k_{ij}} \\
			&=
			\Biggl(\prod_{i=1}^t\prod_{j=1}^n
			(1-a_i b_j)\Biggr)
			\cdot
			\prod_{i=1}^t a_i^{k_{i1}+\ldots+k_{in}}
			\prod_{j=1}^n b_j^{k_{1j}+\ldots+k_{tj}}.
		\end{split}
	\end{equation}
	Thus, we now need to sum expressions
	\eqref{eq:Prob_W} 
	over all $R\in \mathcal{R}(\mu)$,
	and we use the fact that the row/column sums in $W$
	are differences of diagonal sums in $R$, to get the Schur polynomials
	in the combinatorial form 
	\eqref{eq:Schur_polynomial_combinatorial}.
	This completes the proof of \Cref{thm:Schur_measure}.
\end{proof}













































\appendix
\setcounter{section}{13}

\section{Problems (due 2025-04-29)}

\subsection{Non-Markovianity}

Show that the sequence of random variables defined in the exponential LPP model,
\[
L(1,n),L(2,n),\dots,L(t,n),
\]
is \textbf{not} a Markov chain.
By virtue of the equivalence with the spiked Wishart ensemble \eqref{eq:main-claim},
you may alternatively show that the sequence
of maximal eigenvalues
\[
\lambda_1(1),\lambda_1(2),\dots,\lambda_1(t)
\]
of successive
Wishart matrices $M(1),M(2),\dots,M(t)$ is \textbf{not} a Markov chain either.


\subsection{Schur polynomials --- equivalence of definitions}
\label{prob:Schur_polynomials_equivalence}

Show the equivalence of the two definitions of Schur polynomials \eqref{eq:Schur_polynomial} and \eqref{eq:Schur_polynomial_combinatorial}.

\medskip
\noindent
\textbf{Hint:}
Substitute $x_n=1$ and consider how both formulas
expand as linear combinations of Schur
polynomials
$s_\mu(x_1,\ldots,x_{n-1} )$
in $n-1$ variables.
This induction (together with the fact that
Schur polynomials are a linear basis in the ring
of symmetric polynomials in a given fixed number of variables)
will show that the two definitions are equivalent.

\subsection{Schur polynomials --- stability property}

Show the stability property of Schur polynomials \eqref{eq:stability_property_Schur}.


\subsection{Cauchy identity for Schur polynomials}
\label{prob:Cauchy_identity}

Let $a_1, \ldots, a_t$ and $b_1, \ldots, b_n$ be positive parameters satisfying $a_i b_j < 1$ for all pairs $(i,j)$. Prove the Cauchy identity for Schur polynomials:
\begin{equation*}
	\sum_{\mu: \mu_1 \geq \mu_2 \geq \cdots \geq \mu_n \geq 0} s_\mu(a_1,\ldots,a_t)s_\mu(b_1,\ldots,b_n) = \prod_{i=1}^t\prod_{j=1}^n \frac{1}{1-a_i b_j}.
\end{equation*}












\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

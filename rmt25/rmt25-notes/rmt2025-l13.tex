\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 13: Title TBD}


\date{Wednesday, April 9, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l13.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle



\section{Introduction and Motivation}
In this lecture, we explore a remarkable connection between random matrix theory and integrable probability: an \emph{equality in law} between the largest eigenvalue process of a spiked Wishart matrix ensemble and a directed last-passage percolation (LPP) model. This connection was first conjectured by Borodin and P\'ech\'e (2008) in their study of deformed random matrix spectra \cite{BorodinPeche2009}, and was later proved by Dieker and Warren (2009) \cite{diekerWarren2008determinantal} via a clever change-of-measure and a combinatorial bijection (the Robinson--Schensted--Knuth correspondence). In essence, one can construct a stochastic process $\{M(n)\}_{n\ge1}$ of random matrices (the ``generalized Wishart'' process with a finite-rank perturbation to the covariance), and another process $\{Y(N,n)\}_{n\ge1}$ of random times from a directed percolation model. The startling result is that the law of the top eigenvalue of $M(n)$ is identical to the law of the last-passage time $Y(N,n)$, not only for each fixed $n$ but as a joint process in $n$.

This result provides a deep bridge between random matrix eigenvalue fluctuations and probabilistic growth models in the Kardar--Parisi--Zhang (KPZ) universality class. In particular, it allows us to interpret the distribution of the largest eigenvalue in a \emph{spiked Wishart ensemble} (also called a finite-rank deformation of Wishart or Laguerre ensemble) in terms of directed LPP with random weights. Through this connection, integrable probability techniques (such as the RSK bijection and known exact formulas for LPP) can be used to analyze spiked random matrices, and vice versa. One immediate consequence is a precise description of the \textbf{limiting distribution of the top eigenvalue} in the spiked Wishart model: depending on the strength of the spike, the top eigenvalue either follows the Tracy--Widom law of the GOE/GUE, or, if an outlier emerges, converges to a new distribution given by a deformed Airy kernel (the so-called \emph{BBP transition} \cite{BBP2005phase, BorodinPeche2009}). We will give the precise statements of these results and outline their proofs in this lecture. The style and notation will follow that of Lectures 1--11, providing full rigor, motivation, and detailed proofs of the key theorems.

The lecture is organized as follows. In Section 2, we define the generalized (spiked) Wishart random matrix ensemble with a finite-rank perturbation to the covariance. We describe the \emph{largest eigenvalue process} as the number of sample vectors $n$ increases, and state the Markov property and transition kernel for this eigenvalue process (Theorem \ref{thm:MarkovChain}), following \cite{diekerWarren2008determinantal}. In Section 3, we introduce the directed last-passage percolation (LPP) model with geometric/exponential weights, and recall the Robinson--Schensted--Knuth (RSK) correspondence. We state how RSK produces an interlacing array (Gelfand--Tsetlin pattern) from the random LPP environment, and how this implies a Markov property for the vector of last-passage times (Proposition \ref{prop:RSK-Markov}). In Section 4, we prove the main equality-in-law result (Theorem \ref{thm:MainEquality}) by combining the Markov descriptions from Sections 2 and 3. We then discuss the implications for the distribution of the top eigenvalue, including the identification of its scaling limit with known distributions from random matrix theory and KPZ universality (Tracy--Widom and its deformations). A careful statement of the asymptotic law of the spiked Wishart top eigenvalue (following Borodin--P\'ech\'e \cite{BorodinPeche2009}) is given in Theorem \ref{thm:BP-limit}.

\section{The Generalized Wishart Ensemble with Finite-Rank Perturbations}
\subsection{Definition of the spiked Wishart process}
Recall that a (complex) \emph{Wishart matrix} of dimension $N$ with $n$ degrees of freedom (and identity covariance) can be represented as $M = X X^*$, where $X$ is an $N\times n$ random matrix with independent complex Gaussian entries $X_{ij}\sim \mathcal{CN}(0,1)$ (mean zero, variance 1). In that classical case, $M$ is an $N\times N$ positive-semidefinite Hermitian matrix. Its eigenvalues $(\lambda_1,\dots,\lambda_N)$ (with $\lambda_1\ge \cdots \ge \lambda_N \ge 0$) have the joint density of the \emph{Laguerre ensemble} (beta $=2$) and the largest eigenvalue $\lambda_1$ has the well-known Tracy--Widom$_{\rm F}$ (Fisher or Laguerre) distribution in the limit $N,n\to\infty$ with $N/n\to\gamma$. Now we introduce a more general model where the covariance of the underlying Gaussian matrix is not identity but has a finite-rank perturbation (a ``spike'').

\begin{definition}[Generalized Wishart ensemble with parameters $(\pi,\hat\pi)$]\label{def:Wishart}
Fix a positive integer $N$. Let $\pi=(\pi_1,\dots,\pi_N)$ be a fixed $N$-tuple of \emph{positive} real parameters, and let $\hat\pi = (\hat\pi_1,\hat\pi_2,\dots)$ be a sequence of \emph{nonnegative} real parameters (possibly infinite in length). We define an array of complex random variables $\{A_{ij}: 1\le i\le N, j\ge 1\}$ such that under the probability measure $P^{\pi,\hat\pi}$:
\begin{itemize}\item The $A_{ij}$ are independent for all $1\le i\le N$ and $j\ge 1$.
\item Each $A_{ij}$ is a complex Gaussian with mean $0$ and variance $\mathrm{Var}(A_{ij}) = \frac{1}{\pi_i + \hat\pi_j}$ (i.e. $\Re A_{ij}, \Im A_{ij} \sim N(0,\frac{1}{2(\pi_i+\hat\pi_j)})$ independent).
\end{itemize}
For each integer $n\ge 0$, let $A(n)$ denote the $N\times n$ sub-matrix consisting of the first $n$ columns of $A$. We then define an $N\times N$ random Hermitian matrix
\[ M(n) := A(n)\,A(n)^*, \qquad n\ge 0, \]
with the convention $M(0)$ is the zero matrix. We call $\{M(n): n\ge 0\}$ the \textbf{generalized Wishart random-matrix process} with parameters $(\pi,\hat\pi)$.
\end{definition}

In particular, $M(n)$ has the form
\[ M(n) = \sum_{m=1}^n A^{(m)} (A^{(m)})^*, \]
where $A^{(m)}$ denotes the $m$-th column of $A$ (an $N$-dimensional complex random vector with independent entries of variance $1/(\pi_i+\hat\pi_m)$). Thus $M(n)$ can be viewed as the sample covariance matrix of $n$ i.i.d. observations in $\mathbb{C}^N$, where the $j$-th observation has covariance $\mathrm{diag}(1/\hat\pi_j)$ in the basis that diagonalizes $\mathrm{diag}(\pi_1,\dots,\pi_N)$. Equivalently, if we define $\Sigma = \mathrm{diag}(1/\pi_1,\dots,1/\pi_N)$ as a \emph{population covariance matrix} and generate data $X_j \sim \mathcal{CN}(0,\Sigma)$ independently for $j=1,\dots,n$, then $M(n)$ has the same distribution as $\sum_{m=1}^n X_m X_m^*$ conditioned on certain independent exponential weighing factors $\hat\pi_j$ for each sample. We will not need this data interpretation explicitly, but it may help to think of $\pi_i$ as parameters controlling the variance in direction $i$, and $\hat\pi_j$ as parameters controlling the variance of sample $j$. When all $\pi_i=1$ and all $\hat\pi_j=0$, $M(n)$ reduces to the classical complex Wishart($N,n$) with identity covariance.

The introduction of parameters $\pi$ and $\hat\pi$ allows for \textbf{finite-rank deformations of the covariance}: one can think of the $\pi_i$'s as baseline values (say $\pi_i=1$ for all but a few coordinates), and a finite number of them being different from 1 corresponds to a finite-rank perturbation of the identity covariance matrix $\Sigma$ (the directions in which $\pi_i\neq 1$ are "spiked" eigen-directions). Similarly, $\hat\pi_j$ can be viewed as adding a rank-one perturbation associated with each column; if only finitely many of the $\hat\pi_j$ are nonzero, that corresponds to having a finite number of distinguished samples (or boundary inhomogeneities in the equivalent percolation model, as we will see). In summary, the above general definition covers the case of a spiked Wishart ensemble in full generality. We emphasize that $M(n)$ depends on $n$ in a way that $M(n)$ and $M(n-1)$ are not independent but are coupled through shared columns. Indeed $M(n) = M(n-1) + A^{(n)}(A^{(n)})^*$, which is a rank-1 update of $M(n-1)$.

Let us denote by $\lambda_1(n)\ge \lambda_2(n)\ge \cdots \ge \lambda_N(n)\ge 0$ the eigenvalues of $M(n)$ in non-increasing order (padded with zeros if $n < N$, since $\mathrm{rank}(M(n)) \le n$). We will use the notation $\operatorname{sp}(M(n)) = (\lambda_1(n),\dots,\lambda_N(n))$ for the \emph{spectrum} of $M(n)$, viewed as a vector in the \emph{Weyl chamber} $W^N = \{x=(x_1,\dots,x_N)\in\mathbb{R}^N: x_1 \ge x_2 \ge \cdots \ge x_N\}$. We are particularly interested in the \textbf{largest eigenvalue process} $\{\lambda_1(n):n\ge0\}$, i.e. the sequence of the top eigenvalue as the number of samples $n$ grows. Our goal is to describe the law of this process and to identify it with a combinatorial growth model.

Before stating the main result, we need a fundamental property of the eigenvalue sequence $\operatorname{sp}(M(n))$ as $n$ increases, namely that it forms a \emph{Markov chain} in $W^N$. Intuitively, this is because $M(n)$ evolves by adding independent ``increments'' $A^{(n)}(A^{(n)})^*$ at each step. Moreover, the interlacing constraints from linear algebra (Cauchy interlacing theorem) imply that the eigenvalues of $M(n)$ interlace with those of $M(n-1)$. Indeed, since $M(n) = M(n-1) +$ (rank-$1$ positive semidef. matrix), it is known that
\begin{equation}\label{eq:interlace}
\lambda_1(n) \ge \lambda_1(n-1) \ge \lambda_2(n) \ge \lambda_2(n-1) \ge \cdots \ge \lambda_N(n-1) \ge \lambda_N(n),
\end{equation}
with the convention $\lambda_{N+1}(n) := 0$ and $\lambda_{N+1}(n-1):=0$. In other words, $\operatorname{sp}(M(n-1))$ and $\operatorname{sp}(M(n))$ form an \emph{interlacing sequence} in $W^N$. Such sequences are often pictorially arranged as a \textbf{Gelfand--Tsetlin pattern}: a triangular array of real numbers
\[
\begin{matrix}
\lambda_N(n) \\
\lambda_{N-1}(n) & \lambda_{N-1}(n-1) \\
\vdots & & \vdots \\
\lambda_{2}(n) & \lambda_{2}(n-1) & \cdots & \lambda_{2}(1)\\
\lambda_{1}(n) & \lambda_{1}(n-1) & \cdots & \lambda_{1}(1) & \lambda_{1}(0)
\end{matrix}
\]
with each row interlacing the next. This triangular representation will become important later when we connect to the RSK correspondence. For now, we proceed to formalize the Markov property of $\{\operatorname{sp}(M(n))\}_{n\ge0}$.

\subsection{Markov chain and transition kernel for eigenvalues}
We say a random process $\{X(n):n\ge0\}$ taking values in $W^N$ is an \emph{inhomogeneous Markov chain} if for each $m<n$, the conditional law of $X(n)$ given $(X(n-1)=x_{n-1},\;X(n-2)=x_{n-2},\dots,X(m)=x_m)$ depends only on $x_{n-1}$ (and possibly on $n$). In other words, the process has the Markov property but the transition kernel may depend on the time step $n$. In our case, since at each step $n$ a new column $A^{(n)}$ with variance parameters $\{\pi_i+\hat\pi_n:1\le i\le N\}$ is added, the transition law from $M(n-1)$ to $M(n)$ will indeed depend on the index $n$ through $\hat\pi_n$. We denote by $Q^{\pi,\hat\pi}_{n-1,n}(x,dy)$ the transition kernel: for $x\in W^N$ given as the eigenvalue vector of $M(n-1)$, $Q^{\pi,\hat\pi}_{n-1,n}(x,\cdot)$ is the distribution of $\operatorname{sp}(M(n))$.

\begin{theorem}[Markov chain for spiked Wishart eigenvalues]\label{thm:MarkovChain}
For any fixed $N$, any strictly positive $N$-vector $\pi=(\pi_1,\dots,\pi_N)$, and any nonnegative sequence $\hat\pi=(\hat\pi_1,\hat\pi_2,\dots)$, the process $\{\operatorname{sp}(M(n)):n\ge0\}$ is a Markov chain in $W^N$ under $P^{\pi,\hat\pi}$. Moreover, its one-step transition law from time $n-1$ to $n$ is given by an explicit density $Q^{\pi,\hat\pi}_{n-1,n}(x, y)$ on $W^N$ which factorizes in terms of $\pi$ and $\hat\pi_n$. In particular, if $x=(x_1\ge\cdots\ge x_N)$ are the eigenvalues at time $n-1$, and $y=(y_1\ge\cdots\ge y_N)$ at time $n$, then $Q^{\pi,\hat\pi}_{n-1,n}(x, y)$ is supported on interlacing configurations ($x$ and $y$ satisfying \eqref{eq:interlace}) and can be written in closed form as
\begin{equation}\label{eq:transition-density}
Q^{\pi,\hat\pi}_{n-1,n}(x,y) \;=\; \frac{1}{\displaystyle h_\pi(x) } \; \frac{1}{\displaystyle h_\pi(y) } \; \prod_{i=1}^N (y_i - x_i) \prod_{i<j} \frac{y_i - x_j}{x_i - x_j}\; \exp\!\Big(-\hat\pi_n \sum_{i=1}^N (y_i - x_i)\Big)\,,
\end{equation}
for $y$ interlacing $x$, where $h_\pi(z) := \prod_{i=1}^N \prod_{j=1}^N (\pi_i + \hat\pi_j + z_i)$ is a symmetric Harish-Chandra (orbit integral) function.  Consequently, $\operatorname{sp}(M(n))$ has a smooth density on $W^N$ for each $n$, and the joint distribution of $(\operatorname{sp}(M(1)),\dots,\operatorname{sp}(M(n)))$ can be written as the product of these kernels:
\begin{equation}\label{eq:chain-factorization}
P^{\pi,\hat\pi}\{\operatorname{sp}(M(1))\in dx^{(1)}, \dots, \operatorname{sp}(M(n))\in dx^{(n)}\} = Q^{\pi,\hat\pi}_{0,1}(0, dx^{(1)}) \prod_{m=2}^n Q^{\pi,\hat\pi}_{m-1,m}(x^{(m-1)}, dx^{(m)})\,,
\end{equation}
with $x^{(0)}=0$ (all eigenvalues zero initially).
\end{theorem}

\begin{proof}[Proof Sketch]
A full proof of this result can be found in \cite{diekerWarren2008determinantal} (Theorem~3.1) and references therein. The idea is to derive the transition density via a \emph{change-of-measure} from the well-known case of the standard Wishart ensemble (all $\pi_i=1$, $\hat\pi_j=0$). In the standard case, the Markov property and transition kernel have been established earlier by Defosseux (2008) and by Forrester and Rains (2007) \cite{ForresterRains2007} (for real matrices) using intertwining of dynamics on Gelfand--Tsetlin patterns. In fact, for $\pi_i\equiv1$ and $\hat\pi_j\equiv0$, $M(n)$ can be realized as the principal $N\times N$ minor of an $(N+n)\times(N+n)$ GUE random matrix (by adding $n$ trivial eigenvalues at zero), which immediately implies the eigenvalue process is Markov with transition law expressible in terms of Jacobi polynomials (the well-known $\beta=2$ random birth process). The explicit factorized form \eqref{eq:transition-density} is essentially a multivariate extension of the Beta distribution for interlacing eigenvalues (sometimes called the \emph{Kaneko--Macdonald--Meixner ensemble}), and can be derived by conditioning on the eigenvectors (using the Harish-Chandra/Itzykson--Zuber integral formula for unitary integrals) or via the oriented swap interactions in the Gelfand--Tsetlin pattern (see O'Connell \cite{OConnell2003} for the discrete version).

To extend to general $(\pi,\hat\pi)$, one performs a tilt of measure. Observe that under $P^{\pi,\hat\pi}$, the joint density of the entire trajectory $(M(0)=0,\;M(1),\dots,M(n))$ is absolutely continuous w.r.t. the law $P^{\mathbf{1},0}$ of the standard case (with $\mathbf{1}=(1,\dots,1)$). In fact, each entry $A_{ij}$ under $P^{\pi,\hat\pi}$ has variance $\frac{1}{\pi_i+\hat\pi_j}$, whereas under the standard case it has variance $1$. By the Gaussian change-of-variance formula, we have
\[
\frac{dP^{\pi,\hat\pi}}{dP^{\mathbf{1},0}}\Big|_{\sigma(M(0),\ldots,M(n))} = \prod_{i=1}^N \prod_{m=1}^n \frac{1}{\sqrt{2\pi(\pi_i+\hat\pi_m)}} \exp\!\Big\{-\frac{|A_{im}|^2}{2}\Big(\frac{1}{1} - \frac{1}{\pi_i+\hat\pi_m}\Big)\Big\}\,,
\]
which simplifies to
\[
\frac{dP^{\pi,\hat\pi}}{dP^{\mathbf{1},0}} = \prod_{m=1}^n \prod_{i=1}^N \frac{1}{\sqrt{\pi_i+\hat\pi_m}} \cdot \exp\!\Big\{-\frac{\pi_i+\hat\pi_m - 1}{\pi_i+\hat\pi_m}\frac{|A_{im}|^2}{2}\Big\}\,.
\]
Now, conditioning on the eigenvalue processes, one can integrate out the angular parts using the fact that $(M(m-1),M(m))$ in the standard case has a density invariant under conjugation by a unitary matrix (the eigenvectors are Haar-distributed due to unitary invariance of the Wishart ensemble). Applying the Harish-Chandra--Itzykson--Zuber integral for the exponential term and simplifying yields exactly the ratio of symmetric polynomials in \eqref{eq:transition-density} (see \cite[Sec.~3]{diekerWarren2008determinantal} for details). The upshot is that the tilted process $\operatorname{sp}(M(n))$ remains Markov with the modified transition kernel given by \eqref{eq:transition-density}. The lengthy algebraic verification that \eqref{eq:transition-density} indeed defines a valid transition density (satisfying appropriate normalization and consistency conditions) is omitted here.
\end{proof}

\begin{remark}
The factorized kernel \eqref{eq:transition-density} may seem complicated, but it has a nice interpretation in terms of symmetric functions. In fact, $h_\pi(x)$ is proportional to the \emph{multivariate Laguerre weight} with external source $\pi$; specifically $h_\pi(x) = \prod_{i=1}^N \prod_{j=1}^N (\pi_i + \hat\pi_j + x_i) = \prod_{i=1}^N \prod_{j=1}^N (\pi_i + \hat\pi_j) \prod_{i=1}^N \prod_{j=1}^N (1 + x_i/(\pi_i + \hat\pi_j))$. Expanding the second product via Weyl's integration formula yields a Schur polynomial $s_\lambda(x)$ in $x$ times a factor that depends only on $\pi,\hat\pi$. Thus one can rewrite the kernel in terms of ratios of Schur polynomials (or alternately, Jack or Meixner symmetric polynomials), which links it to the Meixner orthogonal ensemble studied by Johansson. We will see a discrete analogue of this factorization in Proposition \ref{prop:RSK-Markov} when we derive the transition probabilities from the RSK correspondence.
\end{remark}

The Markov chain $\operatorname{sp}(M(n))$ starts from $\operatorname{sp}(M(0)) = (0,\dots,0)\in W^N$. For each fixed $n$, the law of $\operatorname{sp}(M(n))$ is just the eigenvalue distribution of an $N\times N$ Wishart matrix with $n$ degrees of freedom and covariance parameters $\pi_1,\dots,\pi_N$ (sometimes called the \emph{generalized Laguerre ensemble}). The joint density of eigenvalues $(\lambda_1,\dots,\lambda_N)$ of $M(n)$ (with $\lambda_1\ge\cdots\ge\lambda_N$) can be obtained by marginalizing the chain up to time $n$. Specifically, integrating \eqref{eq:chain-factorization} over the intermediate steps $x^{(1)},\dots,x^{(n-1)}$ yields
\[
P^{\pi,\hat\pi}\{\operatorname{sp}(M(n)) \in dy\} = \frac{h_\pi(y)}{\prod_{i=1}^N \prod_{m=1}^n (\pi_i+\hat\pi_m)} \cdot \prod_{1\le i<j\le N} (y_i - y_j)^2 \, \prod_{i=1}^N e^{-y_i}\, dy_1\cdots dy_N\,,
\]
which is exactly the known Laguerre ensemble density (with parameters $\pi_i$ entering through a shift in the weight $e^{-y}$) --- see e.g. \cite{mehta2004random}. In particular, the marginal law of the \textbf{largest eigenvalue} $\lambda_1(n)$ can be written as
\begin{equation}\label{eq:Fredholm}
\mathbb{P}\{\lambda_1(n) \le t\} = \det(I - K_{n,t})_{L^2(0,\infty)}\,,
\end{equation}
for a certain integrable kernel $K_{n,t}$ known from random matrix theory (it is an $N$-point Laguerre kernel with parameters $(\pi,\hat\pi)$, often expressible in terms of confluent hypergeometric functions). We will not need the explicit form of this Fredholm determinant, but we mention it because it is one way to derive the asymptotic law of $\lambda_1(n)$ as $N,n\to\infty$. Instead, we will obtain the asymptotic law later by using the coupling with a last-passage percolation model, which is more tractable from the point of view of KPZ universality.

\section{Directed Last-Passage Percolation (LPP) and the RSK Correspondence}
\subsection{Definition of the LPP model with exponential/geometric weights}
We now turn to a seemingly different probabilistic model: a model of random paths in a grid with random weights. Fix the same dimension $N$ as above. Consider an infinite array of independent, nonnegative random weights $\{W_{ij}: 1\le i\le N,\; j\ge 1\}$ defined under the probability measure $P^{\pi,\hat\pi}$ as follows:
\begin{itemize}\item Each $W_{ij}$ is an independent random variable with an \emph{exponential} distribution of rate $(\pi_i + \hat\pi_j)$.
\end{itemize}
In other words, $\mathbb{P}\{W_{ij} > w\} = \exp\{-(\pi_i+\hat\pi_j) w\}$ for $w\ge0$. Equivalently $\mathbb{E}[W_{ij}] = \frac{1}{\pi_i+\hat\pi_j}$. These rates $(\pi_i+\hat\pi_j)$ are chosen deliberately to mirror the variance parameters of $A_{ij}$ in the generalized Wishart model (Definition \ref{def:Wishart}). Indeed, note that $\mathrm{Var}(A_{ij}) = \mathbb{E}[W_{ij}]$ under our definitions -- this is not a coincidence but a clue that $W_{ij}$ will serve as a sort of ``geometric analogue'' of $|A_{ij}|^2$.

We interpret $\{W_{ij}\}$ as random weights on the vertices of a directed lattice in the first quadrant. Specifically, consider the set of lattice points
\[ \{(i,j): i=1,\dots,N,\ j=1,2,\dots\}.\]
We say a path $\Gamma$ is an \textbf{up-right path} from $(1,1)$ to $(N,n)$ if it is a sequence of lattice points starting at $(1,1)$ and ending at $(N,n)$, with steps either one step to the right or one step down. Since each step either increases the column index by 1 or the row index by 1, any such path from $(1,1)$ to $(N,n)$ must consist of $(N-1)$ down-steps and $(n-1)$ right-steps, for a total of $(N+n-2)$ steps. We define the \textbf{weight} of a path $\Gamma$ to be the sum of the $W_{ij}$ along its vertices:
\[ \mathcal{W}(\Gamma) := \sum_{(i,j)\in \Gamma} W_{ij}, \]
where by $(i,j)\in \Gamma$ we mean that the vertex $(i,j)$ is visited by the path $\Gamma$. The random variable of interest is the \emph{maximum total weight achievable among all such paths}, i.e.
\begin{equation}\label{eq:LPPdef}
Y(N,n) := \max_{\Gamma:\, (1,1)\to(N,n)} \; \mathcal{W}(\Gamma)\,.
\end{equation}
We call $Y(N,n)$ the \textbf{last-passage time} to $(N,n)$, in analogy with the usual terminology of growth models (if we interpret $W_{ij}$ as random passage times on a lattice, then the longest time to reach a certain site is given by the maximal weight path).

The random field $\{Y(i,j): 1\le i\le N,\ j\ge 1\}$ can be computed recursively by dynamic programming, using the so-called \emph{Robinson--Schensted--Knuth (RSK) growth rule} or equivalently the Bellman equation for LPP. Indeed, it is immediate from the definition that
\begin{equation}\label{eq:LPP-recursion}
Y(i,j) = W_{ij} + \max\{\, Y(i-1,j),\; Y(i,j-1)\,\}\,,
\end{equation}
for $i>1, j>1$, with boundary conditions $Y(1,j) = \sum_{k=1}^j W_{1k}$ (since from $(1,1)$ to $(1,j)$ one must move right $j-1$ times along the top row) and $Y(i,1) = \sum_{\ell=1}^i W_{\ell,1}$ (moving down along the first column). The recursion \eqref{eq:LPP-recursion} expresses that the optimal path to $(i,j)$ either comes from above (then last step is down, contributing $W_{ij}$ plus the optimal weight to $(i-1,j)$) or from the left (last step is right from $(i,j-1)$). It is the fundamental equation of integrable growth models, equivalent to the so-called \emph{Robinson--Schensted--Knuth insertion algorithm} in combinatorics. We will describe the RSK algorithm more explicitly in the next subsection.

The quantity $Y(N,n)$ appears in many contexts: it is the length of the longest increasing path in a random $N\times n$ array (if $W_{ij}$ were thought of as lengths or as indicator of a path's presence), it is also the total service time in a series of $N$ exponential queueing servers with $n$ customers (the \emph{Jackson network} interpretation \cite{Baryshnikov_GUE2001}), and it is a prototype of models in the KPZ universality class (often called the \emph{exponential corner growth model} or directed percolation). Standard references on the connections between random matrices and such growth models include Baryshnikov (2001) \cite{Baryshnikov_GUE2001} and Johansson (2000) \cite{Johansson2000}. In particular, when all $\pi_i=1$ and $\hat\pi_j=0$ (the homogeneous case where all $W_{ij}\sim \mathrm{Exp}(1)$ i.i.d.), it is known that $Y(N,n)$ (for large $N,n$ with $N/n$ fixed) has fluctuations of order $N^{1/3}$ and converges to the Tracy--Widom GUE distribution after centering and scaling \cite{Johansson2000}. This is the same limiting law as the largest eigenvalue of an $N\times N$ GUE matrix. In fact, Baryshnikov \cite{Baryshnikov_GUE2001} showed that for the homogeneous case, the entire sequence $\{Y(k,n):1\le k\le N\}$ (for fixed $n$) has the same distribution as the ordered eigenvalues $(\lambda_1,\dots,\lambda_N)$ of an $N\times N$ GUE (Gaussian unitary ensemble) matrix. This was one of the first precise links between random matrix spectra and last-passage percolation. In our present setting, the weights are not identically distributed but have rates $\pi_i+\hat\pi_j$, which introduces a spatial inhomogeneity (often called \emph{boundary deformation} of the LPP model). As we will see, even in this deformed case, one can relate the LPP times to random matrix eigenvalues --- in fact, exactly to the spiked Wishart eigenvalues introduced in Section 2.

Let us define the whole \textbf{vector of last-passage times to the bottom row} at column $j$ as
\[ Z(j) := \big( Y(1,j),\, Y(2,j),\, \dots,\, Y(N,j)\big)\,\in W^N, \]
where we list the values in increasing order $Y(1,j)\le Y(2,j)\le \cdots \le Y(N,j)$.\footnote{We have $Y(1,j)\le \cdots\le Y(N,j)$ almost surely because giving the path more freedom to move down can only increase the maximum weight. This is easily checked from \eqref{eq:LPP-recursion}. Thus $Z(j)\in W^N$ indeed.} In particular, $Y(N,n)$ is the largest component of $Z(n)$. The sequence $\{Z(n):n\ge0\}$, with $Z(0)=(0,\dots,0)$, is a random process in $W^N$. We will now describe how the RSK correspondence shows that $Z(n)$ is an inhomogeneous Markov chain and, crucially, has the \emph{same transition kernel} as the eigenvalue chain $\operatorname{sp}(M(n))$ from Section 2. This will pave the way to prove the equality in distribution.

\subsection{The Robinson--Schensted--Knuth (RSK) correspondence}
The RSK algorithm is a combinatorial bijection that associates to any matrix of nonnegative integers a pair of Young Tableaux of the same shape. We refer to standard texts (e.g. Fulton \cite{fulton1997young}) for background on Young Tableaux and the classical (unweighted) RSK correspondence. Here we will use a more probabilistic perspective suitable for our random weights. Specifically, RSK can be described as a procedure that takes an $N\times n$ array of weights (for example $W_{ij}$ for $1\le i\le N,\,1\le j\le n$) and outputs a \textbf{Gelfand--Tsetlin (GT) pattern} of depth $N$ and width $n$. A GT pattern of depth $N$ is a triangular array $x=(x_i^k)_{1\le i\le k \le N}$ of real numbers with $k$ entries in row $k$, satisfying the interlacing constraints
\[ x^k_i \;\ge\; x^{k-1}_i \;\ge\; x^k_{i+1} \qquad \text{for each $1\le i < k \le N$.} \]
We denote by $\mathcal{GT}_{N}$ the set of all GT patterns with $N$ levels. It is well-known that there is a bijection between $\mathcal{GT}_N$ and the set of pairs of \emph{semi-standard Young Tableaux} of a certain shape with entries in $\{1,\dots,N\}$ (the shape is given by the bottom row of the GT pattern). In fact, the classical RSK correspondence says that if $(P,Q)$ is the pair of SSYTs associated with the matrix, then the shape $\lambda$ (a partition of some integer) satisfies $\lambda_1 = Y(N,n)$, $\lambda_2 = Y(N-1,n)$, ..., $\lambda_N = Y(1,n)$, where $Y(k,n)$ are exactly the LPP times defined earlier (this is a consequence of Greene's theorem, which generalizes the Schensted theorem on longest increasing subsequences to the case of $k$ disjoint increasing subsequences) \cite{Greene1974, fulton1997young}. In other words, the shape of the tableau (or equivalently the bottom row $(x^N_1,\dots,x^N_N)$ of the GT pattern) is precisely the sorted vector of last-passage times to $(k,n)$ for $k=1,\dots,N$. Thus we have:
\begin{equation}\label{eq:RSK-property}
x^N_i(n) \;=\; Y(i,n)\qquad \text{for $i=1,\dots,N$},
\end{equation}
where we use the notation $x^N_i(n)$ to denote the $i$-th entry of the bottom row of the GT pattern produced by applying RSK to the submatrix of weights $\{W_{ij}: 1\le i\le N, 1\le j\le n\}$.

In our context, the $W_{ij}$ are not integers a.s., but the RSK algorithm can be extended to real-valued matrices as well, by an appropriate limiting procedure. Essentially, one can approximate the $W_{ij}$ by rationals or by increments of small integers and apply the discrete RSK, then take a limit (this is sometimes called the \emph{greedy algorithm} or \emph{greene's algorithm} for continuous input). For our purposes, we take \eqref{eq:RSK-property} as the defining property linking the LPP values to the GT pattern generated by RSK. Equation \eqref{eq:RSK-property} is precisely the statement of (the weighted version of) Greene's theorem: it asserts that the maximum weight of an up-right path that uses $i$ down-steps (i.e. reaches row $i$) is equal to the $i$-th smallest entry in the RSK output shape. For completeness, we note that in the unweighted case ($W_{ij}=1$ for all entries in some submatrix), $Y(i,n)$ would simply be the length of the longest path using $i$ down-steps, which is exactly the length of the $i$-th longest increasing subsequence of a corresponding sequence --- Greene's theorem then says this equals the sum of the first $i$ parts of the Young diagram (the shape's first $i$ row lengths), which recovers the Schensted correspondence in the case $i=1$.

Now, a crucial property of RSK is that it is a \emph{bijection}. This means that given the output GT pattern, one can reconstruct the input matrix (or rather, the multiset of input entries). In particular, if we feed in a random matrix $\{W_{ij}:1\le i\le N, 1\le j\le n\}$, the distribution of the output pattern $X(n)\in \mathcal{GT}_N$ (where $X(n)$ denotes the GT pattern after processing $n$ columns of the matrix) can be characterized by this bijectivity. For example, suppose $W_{ij}$ are geometric random variables taking values in $\mathbb{Z}_{\ge0}$ (the discrete analogue of our exponential weights). Specifically, assume each $W_{ij}$ is independent with $\mathbb{P}\{W_{ij}=k\} = (1-a_i b_j) (a_i b_j)^k$ for $k\in\{0,1,2,\dots\}$, where $0<a_i, b_j <1$ are parameters. This is a geometric distribution with mean $\frac{a_i b_j}{1-a_i b_j}$. In the special case $a_i = a$ for all $i$ and $b_j=b$ for all $j$, such a model is equivalent (after a certain mapping) to the totally asymmetric simple exclusion process (TASEP) with certain initial conditions, and RSK provides a coupling to the Poissonized Plancherel measure (see \cite{OConnell2003}). In our more general case with distinct $a_i, b_j$, the distribution of the GT pattern $X(n)$ can be described by known results: it turns out that \emph{the bottom row of the GT pattern $X(n)$ evolves as an inhomogeneous Markov chain in $W^N$ with an explicit transition probability depending on $a_i, b_j$}. Indeed, the following proposition is essentially proved by O'Connell \cite{OConnell2003}:

\begin{proposition}[Markov property of LPP via RSK]\label{prop:RSK-Markov}
Apply the RSK algorithm column-by-column to the infinite array of geometric random variables $\{\xi_{ij}: 1\le i\le N,\ j\ge1\}$ with $\mathbb{P}\{\xi_{ij}=k\} = (1-a_i b_j) (a_i b_j)^k$. After inserting $n$ columns, let $X(n) = (x_i^k(n))_{1\le i\le k\le N}$ be the resulting GT pattern. Then the sequence of bottom rows $\{x^N(n): n\ge0\}$ is an inhomogeneous Markov chain in $W^N$. Moreover, its one-step transition probability from $x^{N}(n-1)=\lambda$ to $x^N(n)=\lambda'$ is given by
\begin{equation}\label{eq:RSK-transition}
\mathbb{P}\{x^N(n) = \lambda' \mid x^N(n-1)=\lambda\} \;=\; \prod_{i=1}^N (1 - a_i b_{\lambda_i - \lambda'_{i-1}})\; (a_i b_{\lambda_i - \lambda'_i})\,,
\end{equation}
for any interlacing nondecreasing sequences $\lambda=(\lambda_1\le \cdots\le \lambda_N)$ and $\lambda'=(\lambda'_1\le \cdots\le \lambda'_N)$ in $\mathbb{Z}_{\ge0}^N$. Here we interpret $\lambda_0= -\infty$ and $\lambda'_{N+1}=+\infty$ in the above product. (Equivalently, one can express \eqref{eq:RSK-transition} in the symmetric form $\displaystyle \mathbb{P}_{\lambda\to\lambda'} = \frac{s_\alpha(\lambda')\, s_\alpha(\lambda/\lambda')}{s_\alpha(\lambda)}$, where $s_\alpha$ is the Schur polynomial in variables $\alpha_i = a_i$ and $\alpha_{N+j}=b_j^{-1}$ and $s_\alpha(\lambda/\lambda')$ is a skew Schur function corresponding to the interlacing $\lambda'/\lambda$ \cite{OConnell2003}.)
\end{proposition}

We will not derive formula \eqref{eq:RSK-transition} here. Intuitively, it comes from the fact that RSK is bijective: each possible new bottom row $\lambda'$ that interlaces with the previous $\lambda$ corresponds to exactly one way of inserting the new column of geometric weights into the existing GT pattern, and the probability of that insertion is the product of probabilities of each $\xi_{ij}$ that entered the shape. The result is a product formula of simple factors $(1- a_i b_j)$ or $(a_i b_j)$ for each unit increase or non-increase in the shape at a given row. We refer to \cite{OConnell2003} for a rigorous proof.

Now, Proposition \ref{prop:RSK-Markov} demonstrates an important fact: the bottom-row process of the random GT pattern is Markov with a \emph{factorized transition probability}. In fact, comparing \eqref{eq:RSK-transition} with the transition density \eqref{eq:transition-density} for the eigenvalue process, one sees a clear correspondence upon identifying $a_i = \frac{\pi_i}{1+\pi_i}$ and $b_j = \frac{\hat\pi_j}{1+\hat\pi_j}$ (so that $a_i b_j = \frac{\pi_i \hat\pi_j}{(1+\pi_i)(1+\hat\pi_j)}$). Indeed, if we formally take $\xi_{ij}\sim$ Geometric$(a_i b_j)$ and let $a_i,b_j \to0$ such that $\frac{a_i}{b_j}\to \frac{\pi_i}{\hat\pi_j}$ and $a_i b_j \to \frac{\pi_i \hat\pi_j}{M}$ for some scaling $M\to\infty$, then $\xi_{ij}/M$ converges in distribution to an exponential random variable of mean $\frac{1}{\pi_i + \hat\pi_j}$. Under this limit, the product form \eqref{eq:RSK-transition} exactly matches (the discrete analogue of) the continuous transition kernel $Q^{\pi,\hat\pi}_{n-1,n}$ in Theorem \ref{thm:MarkovChain}. By this reasoning (which can be made rigorous by appropriate limit transitions on the generating functions), we conclude that the bottom-row process $Z(n) = (Y(1,n),\dots,Y(N,n))$ of our exponential LPP model $\{Y(i,j)\}$ is a Markov chain on $W^N$, and its one-step transition law from $Z(n-1)=x$ to $Z(n)=y$ coincides with $Q^{\pi,\hat\pi}_{n-1,n}(x,y)$ given in \eqref{eq:transition-density}. In particular, the largest component $Y(N,n)$ evolves according to the same mechanism as the largest eigenvalue $\lambda_1(n)$ of $M(n)$.

We summarize the above argument in a lemma for clarity:

\begin{lemma}\label{lem:LPP-Markov}
For the directed LPP model with exponential rates $\{\pi_i + \hat\pi_j\}$ defined in \eqref{eq:LPPdef}, the process $\{Z(n) = (Y(1,n),\dots,Y(N,n)) : n\ge0\}$ is an inhomogeneous Markov chain on $W^N$ under $P^{\pi,\hat\pi}$. Its transition kernel $P\{Z(n)=y \mid Z(n-1)=x\}$ is identical to the kernel $Q^{\pi,\hat\pi}_{n-1,n}(x,y)$ of Theorem \ref{thm:MarkovChain}. Equivalently, for any fixed $n$, the joint distribution of $(Y(1,n),\dots,Y(N,n))$ is the same as the joint distribution of the ordered eigenvalues $(\lambda_1(n),\dots,\lambda_N(n))$ of the generalized Wishart matrix $M(n)$.
\end{lemma}

\begin{proof}[Proof (Sketch)]
The Markov property and transition probabilities have been explained above via RSK and Proposition \ref{prop:RSK-Markov}. We rigorously justify that the exponential case can be obtained as a limit of the geometric case. One approach is to note that the generating function of the transition probability $\mathbb{P}\{x^N(n)=\lambda' \mid x^N(n-1)=\lambda\}$ has the Schur polynomial form indicated in the parenthetical remark after \eqref{eq:RSK-transition}. By substituting $\alpha_i = \frac{\pi_i}{1+\pi_i}$ and $\alpha_{N+j} = \frac{1}{1+\hat\pi_j}$ into that expression (which corresponds to taking $a_i = \frac{\pi_i}{1+\pi_i}$ and letting $b_j\to 1/(1+\hat\pi_j)$ as geometric parameter tends to continuous exponential), it is known \cite[Eq.~(4.2)]{diekerWarren2008determinantal} that one recovers exactly the Laplace transform of the continuous transition density \eqref{eq:transition-density}. This proves that the discrete Markov chain converges to the continuous one, hence establishing the claim. Another approach is to discretize time, replacing each exponential $W_{ij}$ by a geometric random variable $\lfloor W_{ij}/\epsilon \rfloor$ for small $\epsilon$, apply Proposition \ref{prop:RSK-Markov}, and then let $\epsilon\to0$. We omit the technical details. Finally, the statement about the joint distribution for fixed $n$ follows from a simple induction on $n$. Both processes $Z(n)$ and $\operatorname{sp}(M(n))$ start at $(0,\dots,0)$ for $n=0$ and share the same one-step transition kernels for each $1\le m\le n$. Hence by the chain factorization \eqref{eq:chain-factorization}, their distributions coincide for each $n$.
\end{proof}

\section{Equality in Law of the Largest Eigenvalue Process and LPP}
We are now in position to state and prove the main theorem of this lecture, which establishes the coupling between spiked Wishart eigenvalues and directed last-passage percolation.

\begin{theorem}[Dieker--Warren \cite{diekerWarren2008determinantal} equality in law]\label{thm:MainEquality}
For any fixed $N\in \mathbb{N}$, any strictly positive parameters $\pi_1,\dots,\pi_N$, and any nonnegative sequence $\hat\pi_1,\hat\pi_2,\dots$, the law of the entire process $\{\lambda_1(n): n\ge0\}$ of the top eigenvalue of the generalized Wishart matrix $M(n)$ is the same as the law of the process $\{Y(N,n): n\ge0\}$ of the last-passage time to $(N,n)$ in the directed percolation model. Equivalently, the finite-dimensional distributions coincide: for any $0<n_1<n_2<\cdots<n_k$,
\[ \mathbb{P}\{\lambda_1(n_1)\le x_1,\, \lambda_1(n_2)\le x_2,\,\dots, \lambda_1(n_k)\le x_k \} = \mathbb{P}\{Y(N,n_1)\le x_1,\, Y(N,n_2)\le x_2,\,\dots, Y(N,n_k)\le x_k \}. \]
In particular, for each fixed $n$, $\lambda_1(n)$ and $Y(N,n)$ are identically distributed.
\end{theorem}

\begin{proof}
In fact, we will prove a stronger statement: not only the largest eigenvalue processes, but the \emph{entire eigenvalue vector processes} coincide in law with the LPP time vector processes. That is, $\{\operatorname{sp}(M(n)):n\ge0\}$ has the same distribution as $\{Z(n)=(Y(1,n),\dots,Y(N,n)):n\ge0\}$ under $P^{\pi,\hat\pi}$. The largest eigenvalue corresponds to the last component of these vectors, so this implies the theorem. To prove the vector equality, we use the Markov chain characterizations. By Theorem \ref{thm:MarkovChain} and Lemma \ref{lem:LPP-Markov}, we know both processes are Markov on $W^N$ with identical one-step transition kernels $Q^{\pi,\hat\pi}_{n-1,n}(x,y)$. Furthermore, at time $n=0$, both $\operatorname{sp}(M(0))$ and $Z(0)$ are deterministically $(0,\dots,0)$ (because $M(0)$ is the zero matrix and by definition $Y(i,0)=0$ for all $i$). Therefore, the two processes are in fact the same Markov chain in law, since a Markov chain is completely determined by its initial distribution and transition kernels. More formally, one can show by induction on $n$ that for any $n\ge0$ and any Borel set $B\subseteq ( \mathbb{R}_{\ge0})^N$,
\[
P^{\pi,\hat\pi}\{\operatorname{sp}(M(n))\in B\} = \int_{(\mathbb{R}_{\ge0})^N} P^{\pi,\hat\pi}\{\operatorname{sp}(M(n-1))\in dx\}\, Q^{\pi,\hat\pi}_{n-1,n}(x, B) ,
\]
and
\[
P^{\pi,\hat\pi}\{Z(n)\in B\} = \int_{(\mathbb{R}_{\ge0})^N} P^{\pi,\hat\pi}\{Z(n-1)\in dx\}\, Q^{\pi,\hat\pi}_{n-1,n}(x, B) .
\]
For $n=0$, we have equality since $P^{\pi,\hat\pi}\{\operatorname{sp}(M(0))\in B\} = \mathbf{1}_{\{0\in B\}} = P^{\pi,\hat\pi}\{Z(0)\in B\}$. Assuming the equality holds for $n-1$, the above integral equations and $Q$-kernel equality imply it holds for $n$ as well. This completes the induction and yields $P^{\pi,\hat\pi}\{\operatorname{sp}(M(n))\in B\} = P^{\pi,\hat\pi}\{Z(n)\in B\}$ for all $n$. Restricting to the largest components (since $\lambda_1(n)=Y(N,n)$ as elements of the same pattern) proves the stated equality in law for the largest eigenvalue processes.

An alternative argument (which is essentially equivalent but couched in a different language) is to explicitly construct a joint probability space on which the two processes can be coupled to be equal almost surely. This is hinted at by our initial construction: we defined both the matrix array $\{A_{ij}\}$ and the weight array $\{W_{ij}\}$ on the \emph{same probability space} $P^{\pi,\hat\pi}$ in Section 2. Indeed, under $P^{\pi,\hat\pi}$, $A_{ij}$ and $W_{ij}$ are independent with the given distributions. Now consider running the RSK algorithm on the infinite array $\{\xi_{ij}\}$ defined by $\xi_{ij} = \lfloor |A_{ij}|^2\rfloor$ (the integer part of $|A_{ij}|^2$). Since $|A_{ij}|^2$ has mean $1/(\pi_i+\hat\pi_j)$ and decays exponentially in the tail, for small $\epsilon>0$ we have $\mathbb{P}\{|A_{ij}|^2\in [k\epsilon,(k+1)\epsilon)\} \approx (\pi_i+\hat\pi_j)\epsilon$ for infinitesimal $\epsilon$. Thus $\xi_{ij}$ is approximately geometric with parameter $a_i b_j$ as in Proposition~\ref{prop:RSK-Markov}. By RSK, for each $n$ we get a GT pattern $X(n)$ whose bottom row $x^N(n)$ is exactly the sorted list of $\{\sum_{(i,j)\in\Gamma} \xi_{ij}: \Gamma: (1,1)\to(N,n)\}$, i.e. the last-passage times in the \emph{discretized} weight array $\xi$. But note that $\xi_{ij} \le |A_{ij}|^2 \le \xi_{ij}+1$. Summing along any path yields
\[ \mathcal{W}(\Gamma) = \sum_{(i,j)\in\Gamma} W_{ij} = \sum_{(i,j)\in\Gamma} |A_{ij}|^2 \le \sum_{(i,j)\in\Gamma} \xi_{ij} + (N+n) \le \mathcal{W}(\Gamma)+ (N+n). \]
Since any path from $(1,1)$ to $(N,n)$ uses exactly $N+n-1$ edges, the difference between $\sum \xi_{ij}$ and $\sum |A_{ij}|^2$ is at most $N+n-1$ for that path. Therefore the maximizing path for $\sum \xi_{ij}$ is also the maximizing path for $\sum |A_{ij}|^2$, and so $Y(N,n) = \max_{\Gamma} \sum |A_{ij}|^2 = \max_{\Gamma} \sum \xi_{ij}$ (at least for $n\ge1$; the case $n=0$ is trivial). This shows that $Y(N,n) = x^N_N(n)$ (the largest entry of the bottom row of $X(n)$) exactly. On the other hand, by properties of RSK (Greene's theorem weighted version), the entire bottom row $x^N(n)$ of $X(n)$ is nothing but the ordered eigenvalues of $M(n) = A(n)A(n)^*$. This is because the joint distribution of eigenvalues of $A(n)A(n)^*$ and the joint distribution of path weights $\{\sum |A_{ij}|^2\}$ are both given by the same Schur polynomial structure (in fact, one can check these measures satisfy the same Gibbs property as in Proposition \ref{prop:RSK-Markov}). Hence we can identify $x^N_i(n) = \lambda_i(n)$ for each $i=1,\dots,N$. In particular, taking $i=N$ we get $\lambda_1(n) = x^N_N(n) = Y(N,n)$ as an equality on this coupling. This coupling argument is essentially the one given in \cite{diekerWarren2008determinantal} via a change-of-measure and RSK. We do not formalize it further here, as the Markov chain argument is sufficient to conclude equivalence in law.
\end{proof}

\begin{remark}
It is noteworthy that the above theorem provides an \emph{exact distributional identity} between two very different probabilistic systems. Many special cases of this identity were known before. For instance, when $\pi_i\equiv1$ and $\hat\pi_j\equiv0$ (no spike, homogeneous case), the result reduces to $Y(N,n) \stackrel{d}{=} \lambda_1(n)$ for a standard Wishart($N,n$) matrix. In particular, taking $n=N$ yields $Y(N,N) \stackrel{d}{=} \lambda_1(N)$ for an $N\times N$ Wishart matrix, which (for complex entries) has the same distribution as $\lambda_1$ of an $N\times N$ GUE matrix (Laguerre and GUE largest eigenvalues coincide in distribution up to parameter). Therefore $Y(N,N)$ has Tracy--Widom GUE fluctuations asymptotically by Forrester and Johansson \cite{johansson2000shape}. But $Y(N,N)$ is also the last-passage time in an $N\times N$ i.i.d. exponential grid, which was independently proved to have Tracy--Widom fluctuations by Johansson \cite{johansson2000shape}. Our theorem explains that these are two manifestations of the same law. Another special case: if one takes $N=1$, then $\lambda_1(n)$ is simply a sum of $n$ exponential random variables of rates $(\pi_1+\hat\pi_j)$, and $Y(1,n)$ is the same sum, so trivially the equality in law holds. The first nontrivial case is $N=2$. Then $\lambda_1(n)$ for Wishart($2,n$) and the longest path weight in a $2\times n$ grid are equal in distribution, a fact that can be checked by direct calculation as well (both have a Beta--Binomial type distribution). The full generality of Theorem \ref{thm:MainEquality} is due to Dieker \& Warren \cite{diekerWarren2008determinantal}. We emphasize that the theorem holds for finite $N$ and $n$, not just in the limit.
\end{remark}

\subsection*{The law of the top eigenvalue and asymptotic distribution}
Having established the equivalence in law of $\lambda_1(n)$ and $Y(N,n)$, we can leverage known results from either side to describe the distribution of the top eigenvalue in the spiked model. One convenient consequence is an explicit formula for the distribution function of $\lambda_1(n)$ in terms of a combinatorial sum (or equivalently a Fredholm determinant). Indeed, from the LPP perspective, we have by definition
\[ \{\lambda_1(n) \le t\} = \{Y(N,n) \le t\} = \{\text{there exists no path from $(1,1)$ to $(N,n)$ of total weight $>t$}\}.\]
This event is easier to complement: $\{\lambda_1(n) > t\}$ means there is \emph{at least one path of weight $>t$}. By inclusion-exclusion, one can sum over all up-right paths $\Gamma$ (there are finitely many) the probabilities $\mathbb{P}\{\mathcal{W}(\Gamma) > t\}$, subtract the overlaps where two specific paths have weight $>t$, and so on. Because the $\{W_{ij}\}$ are independent exponentials, the probability that a \emph{given path} $\Gamma$ has weight $>\!t$ is
\[ \mathbb{P}\Big\{\sum_{(i,j)\in\Gamma} W_{ij} > t\Big\} = e^{-(\pi_1+\hat\pi_1+\cdots+\pi_N+\hat\pi_n) t} \sum_{k=0}^{N+n-2} \frac{t^k}{k!} \prod_{(i,j)\in\Gamma} (\pi_i+\hat\pi_j)^k,\]
essentially because the sum of $(N+n-1)$ independent exponentials can be written as an Erlang (Gamma) distribution. Summing over all $\binom{N+n-2}{N-1}$ paths from $(1,1)$ to $(N,n)$ yields an expression involving symmetric sums of the $\pi_i+\hat\pi_j$. After cancellation, one ends up with a formula:
\begin{equation}\label{eq:TWfiniteFredholm}
\mathbb{P}\{\lambda_1(n) \le t\} \;=\; \sum_{k=0}^{N(n-1)} c_{k}(\pi,\hat\pi)\, e^{-t (\pi_1+\cdots+\pi_N+\hat\pi_1+\cdots+\hat\pi_n)}\, \frac{t^k}{k!}\,,
\end{equation}
for some coefficients $c_k(\pi,\hat\pi)$ depending on the parameters but not on $t$. This is recognized as the beginning of the Laplace transform expansion of a \emph{Fredholm determinant} of an integrable kernel (or equivalently the generating function of some determinantal point process). Indeed, one can show
\[ \mathbb{P}\{\lambda_1(n) \le t\} = \det\big(I - K_{N,n}\big)_{L^2(0,t)}\,,\]
where $K_{N,n}(x,y)$ is a kernel of the form $\sum_{i=1}^{N} \sum_{j=1}^{N} e^{-\pi_i x} P_{ij}(x,y) e^{-\pi_j y}$ for some polynomial/exponential term $P_{ij}$, see \cite{BorodinPeche2009}. The precise form of $K_{N,n}$ is not too important for us; what matters is that the top eigenvalue distribution can be characterized as such a determinant and thus amenable to asymptotic analysis using steepest-descent methods (Riemann--Hilbert for the kernel or the known results about Laguerre polynomials).

We mention one important asymptotic regime: let $N$ and $n$ tend to infinity such that $N/n \to \gamma \in(0,1]$ (the aspect ratio tends to $\gamma$). If all $\pi_i=1$ (no spike) and all $\hat\pi_j=0$ (homogeneous), then $\lambda_1(n)$ after centering at $n+(N-1)$ and scaling by $n^{1/3}$ converges to the Tracy--Widom distribution for $\beta=2$ (GUE). If one $\pi_i$ is different, say $\pi_N=1+\theta$ with $\theta>0$ fixed and $\pi_1=\cdots=\pi_{N-1}=1$, then this corresponds to a rank-1 spike in the covariance matrix. It is known from the Baik--Ben~Arous--P\'ech\'e (BBP) phase transition \cite{BBP2005phase} that if $\theta$ is small (specifically, $\theta < \sqrt{\gamma}$), then $\lambda_1(n)$ still lies at the edge of the Marchenko--Pastur bulk and has Tracy--Widom fluctuations. But if $\theta > \sqrt{\gamma}$, then an \emph{outlier} eigenvalue separates from the bulk and its fluctuations are no longer Tracy--Widom; instead, $\lambda_1(n)$ approaches the limit $(1+\theta)(1+\frac{\gamma}{\theta}) n$ in expectation and has $\mathcal{O}(1)$ fluctuations around that limit (i.e. it converges to a certain distribution independent of $n$). Borodin and P\'ech\'e \cite{BorodinPeche2009} identified this limiting distribution for the outlier: it is given by a Fredholm determinant with a \emph{deformed Airy kernel}, often called the ``Airy kernel with two sets of parameters'' \cite{BBP2005phase}. We state their result in our notation:

\begin{theorem}[Borodin--P\'ech\'e asymptotic law for spiked eigenvalue]\label{thm:BP-limit}
Consider the spiked Wishart ensemble with one spike: $\pi_1=\cdots=\pi_{N-1}=1$ and $\pi_N = 1+\theta$ for some $\theta>0$, and $\hat\pi_j=0$ for all $j$. Let $c = \sqrt{\gamma}$, where $\gamma = \lim_{N,n\to\infty} N/n \in(0,1]$. Define the critical value $\theta_c = c$ (so if $\theta>\theta_c$, the spike is supercritical and produces an outlier eigenvalue). Then:
\begin{itemize}\item If $0<\theta < \theta_c$, then the top eigenvalue $\lambda_1(N)$ (with $N,n\to\infty$, $\gamma$ fixed) has fluctuations governed by the Tracy--Widom distribution of type~2 (GUE). Specifically,
\[ \mathbb{P}\left\{\frac{\lambda_1(N) - \mu_{N,n}}{\sigma_{N,n}} \le x\right\} \to F_2(x) \qquad \text{as $N,n\to\infty$,}\]
where $\mu_{N,n} = (1+\sqrt{\gamma})^2 n$ and $\sigma_{N,n} = (\sqrt{\gamma}+1)(\gamma^{-1/6}+\gamma^{1/6})\, n^{1/3}$ are the usual centering and scaling for the Laguerre ensemble edge.
\item If $\theta = \theta_c = c$, the spike is critical and the top eigenvalue still lies at the edge of the bulk but the fluctuations are enlarged (of order $n^{2/3}$). In fact, one finds
\[ \mathbb{P}\left\{\frac{\lambda_1(N) - \mu_{N,n}}{\tilde\sigma_{N,n}} \le x\right\} \to F_2^{(1)}(x) \]
where $F_2^{(1)}(x)$ is the distribution of $\max(A(t)-t^2,0)$ for the Airy$_2$ process $A(t)$ (this is sometimes called the BBP crossover distribution of order 1).
\item If $\theta > \theta_c$, the spike is supercritical and $\lambda_1(N)$ separates from the bulk. In this regime,
\[ \mathbb{P}\{\lambda_1(N) \le (1+\theta)(1+\frac{\gamma}{\theta})\, n + s\} \to \det(I - K_{\rm Ai}^{(\theta)})_{L^2((-\infty,\,s])},\]
where the limit law on the right is given by a deformed Airy kernel $K_{\rm Ai}^{(\theta)}$ whose exact form is specified in \cite{BorodinPeche2009}. This is the distribution of the outlier, which can be understood as the law of the maximum of the Airy$_2$ process with a parabolic shift $-t^2 + \omega |t|$ where $\omega$ is related to $\theta$. In particular, as $\theta\to\infty$, the outlier distribution converges to a GOE Tracy--Widom law (since a very large spike essentially decouples the top eigenvalue which then behaves like the edge of a one-dimensional Gaussian).
\end{itemize}
\end{theorem}

We will not derive this result here, as it involves significant asymptotic analysis. However, we can offer some intuition via the LPP coupling: a spike $\pi_N = 1+\theta$ means the $N$-th row of the weight lattice has systematically \emph{lower} weights (since $\mathbb{E}W_{Nj} = 1/(1+\theta + \hat\pi_j) < 1$), so paths that spend a lot of time in the bottom row accumulate less weight and might not be optimal unless the spike is small. If $\theta$ is below threshold, the effect is minor and a typical maximal path will still snake through all $N$ rows (the bottom row included), picking up typical Tracy--Widom fluctuations. If $\theta$ is above threshold, however, the optimal path might avoid the bottom row altogether for most of its length, effectively traveling through an $(N-1)\times n$ subgrid of heavier weights. In that case, the last-passage time is essentially the sum of weights in a smaller grid plus possibly a final segment dropping down at the end; this combinatorial picture leads to the deformed Airy kernel. Borodin \& P\'ech\'e used a rigorous steepest descent approach on the Fredholm determinant to compute this limit.

In closing, we have demonstrated in this lecture how a finite-$N$ identity between two processes can yield powerful insights about the distributions involved. The interplay of random matrix theory, symmetric function theory (RSK, Schur polynomials), and integrable probability (determinantal processes, KPZ universality) exemplified by this result is a hallmark of modern probability in the context of random growth models. We have focused on one example (spiked Wishart vs. directed percolation), but many other such correspondences exist (e.g., GUE minors vs. polynuclear growth, or stochastic six-vertex model vs. $q$-TASEP). The methodology is often similar: identify a Markov or determinantal structure and then apply combinatorial bijections or analytic continuation in parameters. The result is a web of equivalences that allow us to transport results from one domain to another, as we did here by equating $\lambda_1(n)$ with $Y(N,n)$.

































\appendix
\setcounter{section}{12}

\section{Problems (due 2025-04-29)}





\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}

\end{document}

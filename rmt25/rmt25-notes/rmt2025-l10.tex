\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 10: Dyson Brownian Motion}


\date{Monday, March 24, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l10.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle

\begin{abstract}
	This lecture begins the second half of the course after the Spring Break, and
	here we start with the Dyson Brownian Motion. 
	In the later lectures we explore random growth models and their universal scaling limits
	which are closely connected to random matrix theory.
\end{abstract}

\section{Motivations}
\subsection{Why introduce time?}
Our previous lectures dealt with static matrix ensembles (e.g., GUE, GOE, and so on). However, there are both \emph{physical} and \emph{mathematical} reasons to study a dynamical model for random matrices. For instance:
\begin{enumerate}
\item In physics, one often interprets random matrices as Hamiltonians of quantum systems. It is natural to let these Hamiltonians vary in time and to describe how spectra evolve.
\item Such time-dependent models are vital for studying \emph{universality results} in random matrix theory. Rigorous proofs of local eigenvalue correlations often involve coupling or evolving an ensemble toward (or away from) a known reference ensemble.
\item Dynamical extensions yield intriguing connections to 2D statistical mechanics, representation theory, and Markov chain interpretations such as \emph{nonintersecting path ensembles}.
\end{enumerate}

\subsection{Simple example: $1\times1$ case}
When $N=1$, an $N\times N$ Hermitian matrix is just a single real entry. Thus GUE/GOE/GSE distributions each reduce to a real Gaussian variable with mean $0$ and variance $1$. If we allow \emph{time}, the natural time evolution is standard \emph{Brownian motion} $B(t)$ on $\mathbb{R}$.

Recall that a standard one--dimensional Brownian motion \(B(t)\) is a continuous stochastic process with the following key properties:
\begin{enumerate}
    \item \textbf{Continuity:} \(t\mapsto B(t)\) is almost surely continuous.
    \item \textbf{Independent increments:} For any \(0\leq s < t\), the increment \(B(t)-B(s)\) is independent of the past \(\{B(u): 0\le u \le s\}\).
    \item \textbf{Gaussian increments:} \(B(t)-B(s)\) is normally distributed with mean \(0\) and variance \(t-s\); that is,
    \[
    B(t)-B(s) \sim \mathcal{N}(0,\,t-s).
    \]
\end{enumerate}
Thus, if the process starts at \(B(0)=a\), then for any fixed \(t>0\),
\[
B(t)\sim \mathcal{N}(a,\,t).
\]

Our goal is to generalize this to the case of \emph{matrix-valued} Brownian motion and, ultimately, to see how the \emph{eigenvalues} of such a matrix evolve.

\section{Matrix Brownian motion and its eigenvalues}
\label{sec:matrix_BM}

\subsection{Definition}
Let $X(t)$ be an $N\times N$ matrix whose entries are i.i.d.\ real/complex Brownian motions (depending on $\beta=1,2$). For instance:
\begin{itemize}
\item If $\beta=1$: $X(t)$ has entries that are i.i.d.\ real Brownian motions.
\item If $\beta=2$: $X(t)$ has entries that are i.i.d.\ complex Brownian motions (independent real and imaginary parts).
\end{itemize}
Since $X(t)$ may not be Hermitian, define
\[
	\mathcal{M}(t) \;=\; \frac{1}{\sqrt{2}}\bigl(X(t) + X^\dagger(t)\bigr).
\]
Here $X^\dagger(t)$ is the conjugate transpose. Then $\mathcal{M}(t)$ is an \emph{Hermitian} matrix (or real symmetric for $\beta=1$).

\begin{lemma}
\label{lemma:time_fixed_law}
If $\mathcal{M}(0) = A$ is a fixed deterministic matrix, then $\mathcal{M}(t)$ at time $t$ is distributed as
\[
A \;+\;\sqrt{t}\, G_{\beta},
\]
where $G_{\beta}$ is a random Hermitian matrix from the Gaussian ensemble with $\beta=1$ or $2$.
\end{lemma}
\begin{proof}[Sketch of proof]
	Straightforward observation.
\end{proof}

For the one-dimensional case, notice that $a+\sqrt t\ssp Z$, where $Z\sim \mathcal{N}(0,1)$, is a Gaussian random variable with mean $a$ and variance $t$, and every such Gaussian variable can be represented in this form.

\subsection{Eigenvalues as Markov process}
We now focus on $\lambda_i(t)$, the (ordered) eigenvalues of $\mathcal{M}(t)$. Denote
\[
\lambda(t) = \bigl(\lambda_1(t)\ge \dots \ge \lambda_N(t)\bigr).
\]
\begin{theorem}
\label{thm:lambda_is_markov}
As $t$ varies, the process $\lambda(t)$ is a continuous-time Markov process in $\mathbb{R}^N$.
\end{theorem}
\begin{proof}[Sketch of proof]
	Assume $\beta=2$, the case $\beta=1$ is similar.
We need to show that $\lambda(t)$ depends on its future and past only through its instantaneous value. Using the independent increment property of $X(t)$, consider times $0< u< t$. We have
\[
\mathcal{M}(t) \;=\; \mathcal{M}(u)\;+\;\bigl(\mathcal{M}(t)-\mathcal{M}(u)\bigr).
\]
Since $\mathcal{M}(u)$ diagonalizes to $\mathrm{diag}\bigl(\lambda_1(u),\ldots,\lambda_N(u)\bigr)$ by some unitary $U_u$, we can write
\[
U_u^\dagger\,\mathcal{M}(t)\,U_u \;=\;\mathrm{diag}\bigl(\lambda_1(u),\ldots,\lambda_N(u)\bigr)\;+\; U_u^\dagger\bigl(\mathcal{M}(t)-\mathcal{M}(u)\bigr)\,U_u.
\]
The second term again has i.i.d.\ random entries (due to unitary invariance of
GUE), independent of $\mathcal{M}(s)$ for $s\le u$.
Therefore, conditioned on $\mathcal{M}(s)$, $s\le u$, the dependence only
comes through $\lambda(u)$, and the eigenvalues $\lambda_i(s)$ for $s\ge u$ follow
the same dynamics. This proves the Markov property.
\end{proof}

\section{Dyson Brownian Motion}
We now describe the stochastic differential equation (SDE)
for $\lambda(t)$ explicitly, following the classical result due to Dyson
\cite{dyson1962brownian}. Let us first briefly discuss what is an SDE.

\subsection{Stochastic differential equations}








\subsection{Heuristic Derivation of the SDE}

Let \(\mathcal{M}(t)\) be an \(N\times N\) Hermitian matrix-valued process defined by
\[
\mathcal{M}(t)=\mathcal{M}(0)+X(t),
\]
where \(X(t)\) is a matrix Brownian motion (for example, in the GUE case, \(X(t)=B(t)+i\tilde{B}(t)\) with independent real Brownian motions in the appropriate entries).

Denote by \(\lambda_1(t) \le \lambda_2(t) \le \cdots \le \lambda_N(t)\) the ordered eigenvalues of \(\mathcal{M}(t)\). We wish to understand how a small time increment affects these eigenvalues. For a very small time increment \(\Delta t\), one can write
\[
\mathcal{M}(t+\Delta t)=\mathcal{M}(t)+\Delta \mathcal{M},
\]
where the increment \(\Delta \mathcal{M}\) has (approximately) independent Gaussian entries of order \(\sqrt{\Delta t}\).

Assume that at time \(t\) the matrix is diagonalized as
\[
\mathcal{M}(t)=U \Lambda(t) U^*,
\]
with \(\Lambda(t)=\operatorname{diag}(\lambda_1(t),\dots,\lambda_N(t))\) and \(U\) unitary. By standard perturbation theory for Hermitian matrices (see, e.g., Rayleigh–Schrödinger perturbation theory), the first–order change in an eigenvalue \(\lambda_i\) due to a perturbation \(\Delta \mathcal{M}\) is given by
\[
\Delta\lambda_i = \langle u_i,\,\Delta \mathcal{M}\,u_i \rangle,
\]
where \(u_i\) is the normalized eigenvector corresponding to \(\lambda_i(t)\).

However, in addition to this linear term, one must take into account the second-order correction (the Itô correction) coming from the randomness of the off–diagonal elements. A careful analysis (which we outline below) shows that, after averaging, the drift term involves a sum over \(j\neq i\) with reciprocal differences. In particular, one obtains
\[
\Delta\lambda_i = \Delta B_i + \frac{1}{2}\Delta t\,\sum_{j\neq i}\frac{1}{\lambda_i(t)-\lambda_j(t)} + o(\Delta t),
\]
where \(\Delta B_i\) represents the contribution from the independent Brownian motions (with variance \(\Delta t\)) and the second term arises from the Itô correction due to the off–diagonal fluctuations.

Passing to the limit \(\Delta t\to 0\) and noting that the variance in the diagonal increments matches that of standard Brownian motion, we conclude that the eigenvalues satisfy the following SDE:
\begin{equation}
\label{eq:DBM_final}
d\lambda_i(t)=\frac{\beta}{2}\sum_{\substack{j=1\\ j\neq i}}^N \frac{dt}{\lambda_j(t)-\lambda_i(t)}+dW_i(t), \quad i=1,\dots,N.
\end{equation}
Here, \(W_i(t)\) are independent standard Brownian motions, and the parameter \(\beta>0\) arises from the symmetry class (with \(\beta=1\) for GOE, \(\beta=2\) for GUE, and \(\beta=4\) for GSE).

\begin{remark}
The drift term in \eqref{eq:DBM_final} is singular when eigenvalues approach each other. This strong repulsion prevents collisions of eigenvalues and ensures that the ordering is preserved for all \(t>0\).
\end{remark}

\begin{exercise}
Using first–order perturbation theory and the Itô formula, derive heuristically the SDE \eqref{eq:DBM_final} for the case \(\beta=1\). (Hint: Expand the determinant condition for eigenvalues and collect terms up to order \(\Delta t\).)
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Invariant Distribution and Transition Probabilities}

One of the striking features of Dyson Brownian Motion is that it has a well–understood invariant (or stationary) distribution. In particular, if the eigenvalue process is started from a “degenerate” initial condition (e.g., \(\lambda(0)=(0,\dots,0)\)), then for each fixed \(t>0\) the density of \(\lambda(t)=(\lambda_1(t),\dots,\lambda_N(t))\) is given by
\[
p_t(\lambda_1,\dots,\lambda_N) \propto \prod_{1\le i<j\le N}|\lambda_i-\lambda_j|^\beta \exp\left(-\frac{1}{2t}\sum_{i=1}^N\lambda_i^2\right).
\]
In other words, for \(\beta=2\) this is exactly the joint eigenvalue density of the GUE (up to a scaling factor).

The transition probability from an initial configuration \(\lambda(0)=(a_1,\dots,a_N)\) to a configuration \(\lambda(t)=(x_1,\dots,x_N)\) (with \(x_1<x_2<\cdots<x_N\) and similarly for \(a_i\)) can also be written explicitly. In fact, one obtains the following result for the \(\beta=2\) case:

\begin{theorem}[Transition Density for \(\beta=2\)]
Let \(\lambda(0)=(a_1,\dots,a_N)\) and let \(\lambda(t)=(x_1,\dots,x_N)\) with \(a_1<\cdots<a_N\) and \(x_1<\cdots<x_N\). Then the transition probability density is given by
\[
P\bigl(\lambda(t)=\vec{x}\mid \lambda(0)=\vec{a}\bigr)
=N! \left(\frac{1}{\sqrt{2\pi t}}\right)^N \prod_{1\le i<j\le N}\frac{x_j-x_i}{a_j-a_i} \det\Biggl[\exp\Bigl(-\frac{(x_i-a_j)^2}{2t}\Bigr)\Biggr]_{i,j=1}^N.
\]
\end{theorem}

This formula is obtained by interpreting the matrix process as \(A+\sqrt{t}\,\text{GUE}\) and applying the Harish-Chandra--Itzykson--Zuber (HCIZ) formula to integrate over the unitary group.

\begin{exercise}
Show that as \(t\to 0\) the above transition density converges (in the sense of distributions) to a delta–function on \(\vec{x}=\vec{a}\).
\end{exercise}

\begin{remark}
For general \(\beta\) the invariant density is given by the same form (with exponent \(\beta\)) and these densities are central in studying the universality of eigenvalue statistics.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Determinantal Structure for \(\beta=2\): The Correlation Kernel}

For the special case \(\beta=2\), the process of eigenvalues is \emph{determinantal}; that is, for any fixed time \(t>0\) the \(k\)-point correlation functions can be written as
\[
\rho_k(x_1,\dots,x_k)=\det\Bigl[K_t(x_i,x_j)\Bigr]_{i,j=1}^k,
\]
for a suitable correlation kernel \(K_t(x,y)\).

One standard derivation of this determinantal structure is via the explicit form of the joint density (which is a Vandermonde determinant squared times a Gaussian weight) and the use of orthogonal polynomial techniques. Alternatively, one may derive the transition kernel directly from the HCIZ formula.

In our context, one obtains the following explicit double contour integral representation for the correlation kernel:

\begin{theorem}[Determinantal Kernel for \(\beta=2\)]
\label{thm:det_kernel}
Assume the eigenvalue process starts at \(\lambda(0)=(a_1,\dots,a_N)\). Then for time \(t>0\), the correlation kernel is given by
\begin{equation}
\label{eq:det_kernel_explicit}
K_t(x,y)=\frac{1}{(2\pi i)^2\,t}\iint_{\Gamma_z,\Gamma_w} \frac{\exp\Bigl(\frac{w^2-2yw}{2t}\Bigr)}{\exp\Bigl(\frac{z^2-2xz}{2t}\Bigr)}
\prod_{i=1}^N \frac{w-a_i}{z-a_i}\,\frac{dw\,dz}{w-z},
\end{equation}
where the contours \(\Gamma_z\) and \(\Gamma_w\) are chosen as follows:
\begin{itemize}
    \item The \(z\)-contour, \(\Gamma_z\), is a closed contour encircling the points \(a_1,\dots,a_N\).
    \item The \(w\)-contour, \(\Gamma_w\), is chosen such that it encloses \(\Gamma_z\) (or is deformed appropriately) and the integral converges.
\end{itemize}
\end{theorem}

\begin{remark}
The determinantal structure implies that for any \(k\ge1\),
\[
\rho_k(x_1,\dots,x_k)=\det\Bigl[K_t(x_i,x_j)\Bigr]_{i,j=1}^k.
\]
This representation is fundamental in the asymptotic analysis (e.g., for proving the sine kernel or Airy kernel limits) and in establishing universality.
\end{remark}

\begin{exercise}
Show that in the limit \(t\to 0\) the kernel \(K_t(x,y)\) from \eqref{eq:det_kernel_explicit} converges (after appropriate rescaling) to a kernel that recovers the initial condition \(\lambda(0)=(a_1,\dots,a_N)\). (Hint: Examine the asymptotics of the Gaussian factors.)
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Non-Colliding Brownian Motions and Their Equivalence to DBM for \(\beta=2\)}

An important and beautiful result is that for \(\beta=2\) the process defined by the SDE \eqref{eq:DBM_final} is identical in law to \(N\) independent Brownian motions conditioned never to collide. In other words, the repulsion between eigenvalues in DBM exactly mimics the effect of conditioning independent particles to remain ordered.

\begin{theorem}
For \(\beta=2\), let \(B_1(t), \dots, B_N(t)\) be \(N\) independent Brownian motions starting from \(a_1< a_2< \cdots < a_N\). Then the law of these processes conditioned on the event that
\[
B_1(t) < B_2(t) < \cdots < B_N(t) \quad \text{for all } t>0
\]
coincides with the law of the eigenvalue process \(\lambda(t)\) given by Dyson Brownian Motion.
\end{theorem}

A standard way to justify this result is via the Karlin–McGregor formula (or Lindström–Gessel–Viennot lemma), which expresses the probability that independent Brownian paths do not intersect as a determinant of single–particle transition densities.

\begin{exercise}
For \(N=2\), derive the Karlin–McGregor formula for the probability that two independent Brownian motions started at \(a_1\) and \(a_2\) and ending at \(x_1\) and \(x_2\) (with \(a_1<x_1\) and \(a_2<x_2\)) do not collide. Verify that the result can be written as
\[
P(\text{non-intersection}) = P_t(a_1\to x_1)P_t(a_2\to x_2)-P_t(a_1\to x_2)P_t(a_2\to x_1),
\]
where \(P_t(x\to y)=\frac{1}{\sqrt{2\pi t}}\exp\left(-\frac{(y-x)^2}{2t}\right)\).
\end{exercise}

\begin{exercise}
Explain why two independent Brownian motions started at distinct points intersect almost surely, and thus conditioning on non-intersection is a nontrivial event.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concluding Remarks and Further Directions}

We have now seen in detail how the eigenvalue evolution of Hermitian matrix Brownian motion (Dyson Brownian Motion) is described by a system of interacting diffusions given by \eqref{eq:DBM_final}. For the \(\beta=2\) (GUE) case, the eigenvalue process enjoys a determinantal structure; its transition probabilities can be written in closed form via the HCIZ formula, and its correlation functions are given by a determinantal kernel (see \eqref{eq:det_kernel_explicit}). Moreover, the equivalence of DBM with non-colliding Brownian motions in the \(\beta=2\) case deepens our understanding of the repulsion between eigenvalues.

These results have far-reaching implications. In particular:
\begin{itemize}
    \item The determinantal structure allows one to rigorously prove universality of local eigenvalue statistics (leading to the sine kernel in the bulk and the Airy kernel at the edge).
    \item The DBM formulation provides a dynamical approach to many problems in random matrix theory, and has applications in combinatorics, integrable systems, and mathematical physics.
    \item The connection to non-colliding processes links DBM with models in interacting particle systems and stochastic growth.
\end{itemize}

















\appendix
\setcounter{section}{9}
\section{Problems (due 2025-04-29)}

\subsection{Collisions}

Show that two independent standard 1D Brownian motions, started at $a_1\neq a_2$, almost surely intersect.

\subsection{Estimate on the modulus of continuity}

Let $B(t)$ be a standard 1D Brownian motion with $B(0)=0$,
defined as a process with independent increments and $B(t)-B(s)\sim \mathcal{N}(0,t-s)$,
without any continuity assumptions.

Show that
\begin{equation*}
	\operatorname{\mathbb{E}}|B(t)-B(s)|^2 \;\le\; |t-s|
\end{equation*}
implies that
that one can take an almost
surely continuous modification of the function $t\mapsto B(t)$.


\subsection{}

\begin{enumerate}[1.]



\item \textbf{Generator for the Dyson SDE.} For $\beta>0$, write the generator $\mathcal{L}$ of the Markov process \eqref{eq:Dyson_SDE} in differential form. That is, show that for any $C^2$ test function $f(\lambda_1,\dots,\lambda_N)$,
\[
\frac{d}{dt}\,\mathbb{E}\bigl[f(\lambda(t))\bigr]
\;=\;\mathbb{E}\bigl[\mathcal{L}f(\lambda(t))\bigr],
\]
where
\[
\mathcal{L} \;=\; \sum_{i=1}^N \bigl(2\,\tfrac{\partial^2}{\partial \lambda_i^2}\bigr)
\;+\;2\beta\sum_{i<j}\,\frac{1}{\lambda_i-\lambda_j}\bigl(\tfrac{\partial}{\partial \lambda_i}-\tfrac{\partial}{\partial \lambda_j}\bigr).
\]
Verify formally that the density from \Cref{prop:beta_invariant} is stationary for $\mathcal{L}$ when $\lambda(0)=0$.



\item \textbf{Limit shape for Non-Intersecting Bridges.} Let $b_1(t),\dots,b_N(t)$ be $N$ real Brownian bridges from $a_1<\dots<a_N$ at $t=0$ to $z_1<\dots<z_N$ at $t=1$, conditioned on no intersections. Simulate numerically for $N=10$ and $a_i=i,\,z_i=2i$. Describe visually how the paths arrange themselves and how they may approximate a continuous shape as $N$ grows.

\item \textbf{Determinantal representation of transition densities (general $\beta=2$).} Fill in details for the matrix integral approach that leads to the transition probability formula in \Cref{thm:DBM_transition_beta2}, including the key use of the unitary integral (the HCIZ formula). You may take the HCIZ formula as known or provide a reference. Make explicit the factor by which $P(\lambda(t)=x\,|\,\lambda(0)=a)$ is normalized.

\item \textbf{DBM short time expansions.} Let $\lambda_i(0)=0$ for all $i$, and consider \eqref{eq:Dyson_SDE} for small $t$. Expand the drift and diffusion terms to second order in $t$, and compare with the approach in \Cref{sub:sde_proof_idea}. Why do the $\tfrac{1}{\lambda_i-\lambda_j}$ terms not blow up when $\lambda_i\approx \lambda_j$ for small $t$?
\end{enumerate}







































\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

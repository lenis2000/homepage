\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 10: Dyson Brownian Motion}


\date{Monday, March 24, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l10.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle

\begin{abstract}
	This lecture begins the second half of the course after the Spring Break, and
	here we start with the Dyson Brownian Motion.
	In the later lectures we explore random growth models and their universal scaling limits
	which are closely connected to random matrix theory.
\end{abstract}

\section{Motivations}
\subsection{Why introduce time?}
Our previous lectures dealt with static matrix ensembles (e.g., GUE, GOE, and so on). However, there are both \emph{physical} and \emph{mathematical} reasons to study a dynamical model for random matrices. For instance:
\begin{enumerate}
\item In physics, one often interprets random matrices as Hamiltonians of quantum systems. It is natural to let these Hamiltonians vary in time and to describe how spectra evolve.
\item Such time-dependent models are vital for studying \emph{universality results} in random matrix theory. Rigorous proofs of local eigenvalue correlations often involve coupling or evolving an ensemble toward (or away from) a known reference ensemble.
\item Dynamical extensions yield intriguing connections to 2D statistical mechanics, representation theory, and Markov chain interpretations such as \emph{nonintersecting path ensembles}.
\end{enumerate}

\subsection{Simple example: $1\times1$ case}
When $N=1$, an $N\times N$ Hermitian matrix is just a single real entry. Thus GUE/GOE/GSE distributions each reduce to a real Gaussian variable with mean $0$ and variance $1$. If we allow \emph{time}, the natural time evolution is standard \emph{Brownian motion} $B(t)$ on $\mathbb{R}$.

Recall that a standard one--dimensional Brownian motion \(B(t)\) is a continuous stochastic process with the following key properties:
\begin{enumerate}
    \item \textbf{Continuity:} \(t\mapsto B(t)\) is almost surely continuous.
    \item \textbf{Independent increments:} For any \(0\leq s < t\), the increment \(B(t)-B(s)\) is independent of the past \(\{B(u): 0\le u \le s\}\).
    \item \textbf{Gaussian increments:} \(B(t)-B(s)\) is normally distributed with mean \(0\) and variance \(t-s\); that is,
    \[
    B(t)-B(s) \sim \mathcal{N}(0,\,t-s).
    \]
\end{enumerate}
Thus, if the process starts at \(B(0)=a\), then for any fixed \(t>0\),
\[
B(t)\sim \mathcal{N}(a,\,t).
\]

Our goal is to generalize this to the case of \emph{matrix-valued} Brownian motion and, ultimately, to see how the \emph{eigenvalues} of such a matrix evolve.

\section{Matrix Brownian motion and its eigenvalues}
\label{sec:matrix_BM}

\subsection{Definition}
Let $X(t)$ be an $N\times N$ matrix whose entries are i.i.d.\ real/complex Brownian motions (depending on $\beta=1,2$). For instance:
\begin{itemize}
\item If $\beta=1$: $X(t)$ has entries that are i.i.d.\ real Brownian motions.
\item If $\beta=2$: $X(t)$ has entries that are i.i.d.\ complex Brownian motions (independent real and imaginary parts).
\end{itemize}
Since $X(t)$ may not be Hermitian, define
\[
	\mathcal{M}(t) \;=\; \frac{1}{\sqrt{2}}\bigl(X(t) + X^\dagger(t)\bigr).
\]
Here $X^\dagger(t)$ is the conjugate transpose. Then $\mathcal{M}(t)$ is an \emph{Hermitian} matrix (or real symmetric for $\beta=1$).

\begin{lemma}
\label{lemma:time_fixed_law}
If $\mathcal{M}(0) = A$ is a fixed deterministic matrix, then $\mathcal{M}(t)$ at time $t$ is distributed as
\[
A \;+\;\sqrt{t}\, G_{\beta},
\]
where $G_{\beta}$ is a random Hermitian matrix from the Gaussian ensemble with $\beta=1$ or $2$.
\end{lemma}
\begin{proof}[Sketch of proof]
	Straightforward observation.
\end{proof}

For the one-dimensional case, notice that $a+\sqrt t\ssp Z$, where $Z\sim \mathcal{N}(0,1)$, is a Gaussian random variable with mean $a$ and variance $t$, and every such Gaussian variable can be represented in this form.

\subsection{Eigenvalues as Markov process}
We now focus on $\lambda_i(t)$, the (ordered) eigenvalues of $\mathcal{M}(t)$. Denote
\[
\lambda(t) = \bigl(\lambda_1(t)\ge \dots \ge \lambda_N(t)\bigr).
\]
\begin{theorem}
\label{thm:lambda_is_markov}
As $t$ varies, the process $\lambda(t)$ is a continuous-time Markov process in $\mathbb{R}^N$.
\end{theorem}
\begin{proof}[Sketch of proof]
	Assume $\beta=2$, the case $\beta=1$ is similar.
We need to show that $\lambda(t)$ depends on its future and past only through its instantaneous value. Using the independent increment property of $X(t)$, consider times $0< u< t$. We have
\[
\mathcal{M}(t) \;=\; \mathcal{M}(u)\;+\;\bigl(\mathcal{M}(t)-\mathcal{M}(u)\bigr).
\]
Since $\mathcal{M}(u)$ diagonalizes to $\mathrm{diag}\bigl(\lambda_1(u),\ldots,\lambda_N(u)\bigr)$ by some unitary $U_u$, we can write
\[
U_u^\dagger\,\mathcal{M}(t)\,U_u \;=\;\mathrm{diag}\bigl(\lambda_1(u),\ldots,\lambda_N(u)\bigr)\;+\; U_u^\dagger\bigl(\mathcal{M}(t)-\mathcal{M}(u)\bigr)\,U_u.
\]
The second term again has i.i.d.\ random entries (due to unitary invariance of
GUE), independent of $\mathcal{M}(s)$ for $s\le u$.
Therefore, conditioned on $\mathcal{M}(s)$, $s\le u$, the dependence only
comes through $\lambda(u)$, and the eigenvalues $\lambda_i(s)$ for $s\ge u$ follow
the same dynamics. This proves the Markov property.
\end{proof}

\section{Dyson Brownian Motion}
We now describe the stochastic differential equation (SDE)
for $\lambda(t)$ explicitly, following the classical result due to Dyson
\cite{dyson1962brownian}. Let us first briefly discuss what is an SDE.

\subsection{Stochastic differential equations - an informal introduction}

In order to describe the eigenvalues of a time-dependent Hermitian matrix, we rely on \emph{stochastic differential equations} (SDEs). These are differential equations where one or more of the terms involve \emph{random noise}. For simplicity, we start with the one-dimensional setup and later extend it to systems of equations such as those arising in Dyson Brownian Motion.

In an ordinary differential equation (ODE), a function \(x(t)\) evolves according to a deterministic rule of the form
\[
\frac{dx(t)}{dt} \;=\; b\bigl(x(t)\bigr),
\]
where \(b(\,\cdot\,)\) is a deterministic function called the \emph{drift}. If one imposes an initial condition \(x(0)=x_0\), then classical theorems guarantee that, under mild regularity assumptions, a unique solution exists for all \(t\ge0\).

An SDE generalizes this setup by adding a \emph{stochastic (or noise) term} to the right-hand side. Concretely, suppose \(W(t)\) is a standard one-dimensional Brownian motion. Then the simplest SDE has the form
\[
dx(t) \;=\; \sigma\, dW(t),
\]
where \(\sigma\) is a nonnegative constant. This equation may be formally interpreted as
\[
\frac{dx(t)}{dt} \;=\; \sigma\,\frac{dW(t)}{dt},
\]
but it should be emphasized that \(\tfrac{dW}{dt}\) does not exist in the usual sense of classical calculus (Brownian motion is nowhere differentiable almost surely). Instead, one interprets the equation via the \emph{Itô integral}
\[
x(t) \;=\; x(0)\;+\;\int_0^t \sigma\, dW(s).
\]
This integral is defined carefully through a limit of sums involving the increments \(W(t_{k+1})-W(t_k)\), yielding an \emph{almost sure} continuous stochastic process \(t\mapsto x(t)\).

\medskip
More generally, one allows both \emph{drift} and \emph{diffusion} terms:
\begin{equation}
\label{eq:1D_SDE_general}
dx(t) \;=\; b\bigl(x(t)\bigr)\,dt \;+\; \sigma\bigl(x(t)\bigr)\,dW(t).
\end{equation}
Here,
\begin{itemize}
\item \(b(\cdot)\) is the \emph{drift coefficient}, capturing deterministic motion;
\item \(\sigma(\cdot)\) is the \emph{diffusion coefficient}, encoding how strongly the process is randomized by Brownian motion.
\end{itemize}
Under suitable Lipschitz and growth conditions on \(b\) and \(\sigma\), one can show \emph{existence and pathwise uniqueness} of strong solutions to \eqref{eq:1D_SDE_general}. Concretely, this means there is almost surely a unique process \(x(t)\) satisfying \eqref{eq:1D_SDE_general} for each realization of the Brownian motion \(W(t)\). One constructs such a solution, for example, by an iterative limit of approximations.
The simplest discrete-time approximation, analogous to Euler’s method for ordinary differential equations. Over a small time step \(\Delta t\), one approximates
\[
x_{n+1} \;=\; x_n \;+\; b(x_n)\,\Delta t \;+\; \sigma(x_n)\,\bigl(W(t_{n+1}) - W(t_n)\bigr).
\]
This scheme converges to the true solution pathwise under standard Lipschitz conditions on \(b\) and \(\sigma\).


\medskip
A major utility of SDEs is in performing \emph{Itô calculus}. Suppose \(x(t)\) solves the SDE \eqref{eq:1D_SDE_general} and let \(f\colon\mathbb{R}\to\mathbb{R}\) be a sufficiently smooth function. One might try to apply the usual chain rule to \(f(x(t))\), but must account for the extra “noise” term. The correct extension is the \emph{Itô formula}:
\[
df\bigl(x(t)\bigr)
\;=\;
\frac{\partial f}{\partial x}\bigl(x(t)\bigr)\,dx(t)
\;+\;\frac12\,
\frac{\partial^2 f}{\partial x^2}\bigl(x(t)\bigr)\,\bigl(dW(t)\bigr)^2,
\]
where \((dW(t))^2\) is interpreted as \(dt\) in a formal sense. Substituting \eqref{eq:1D_SDE_general} yields:
\[
df\bigl(x(t)\bigr)
\;=\;
b\bigl(x(t)\bigr)\,\frac{\partial f}{\partial x}\bigl(x(t)\bigr)\,dt
\;+\;
\sigma\bigl(x(t)\bigr)\,\frac{\partial f}{\partial x}\bigl(x(t)\bigr)\,dW(t)
\;+\;
\frac12\,\sigma^2\bigl(x(t)\bigr)\,\frac{\partial^2 f}{\partial x^2}\bigl(x(t)\bigr)\,dt.
\]
This identity is an indispensable tool for analyzing stochastic processes, both in theoretical and applied contexts.

\medskip
To handle matrix-valued processes, one must consider multi-dimensional (or matrix-dimensional) analogs of \eqref{eq:1D_SDE_general}. For instance, if \(X(t)\in\mathbb{R}^n\) is an \(n\)-dimensional stochastic process, the SDE becomes
\[
dX(t)
\;=\;
b\bigl(X(t)\bigr)\,dt
\;+\;
\sigma\bigl(X(t)\bigr)\,dW(t),
\]
where \(b(\cdot)\colon\mathbb{R}^n\to\mathbb{R}^n\) and \(\sigma(\cdot)\colon\mathbb{R}^n\to \mathbb{R}^{n\times n}\). Here \(W(t)\) is an \(n\)-dimensional Brownian motion, and the product \(\sigma\bigl(X(t)\bigr)\,dW(t)\) is understood as a matrix-vector multiplication in each small time increment. Existence, uniqueness, and Itô’s formula all generalize naturally under suitable regularity assumptions.

\paragraph{Summary}
Although SDEs can be introduced rigorously via measure-theoretic tools, the above \emph{informal} derivation and discussion provide a workable framework for many typical computations. The key points are:
\begin{itemize}
\item Brownian motion’s roughness prevents classical differential calculus, so new techniques (Itô integrals) are needed.
\item The Itô formula extends the classical chain rule by adding a second-order correction term.
\item Existence and uniqueness theorems ensure that SDEs define well-posed dynamical systems in a stochastic setting.
\item Extending to matrix-valued (or multi-dimensional) settings is conceptually straightforward but requires careful linear algebraic bookkeeping and additional regularity arguments.
\end{itemize}
Equipped with these ideas, we can rigorously address how the eigenvalues of a random matrix evolve over continuous time, culminating in the Dyson Brownian Motion description of Hermitian ensembles.

\subsection{Heuristic derivation of the SDE for the Dyson Brownian Motion}


Let $\mathcal{M}(t)$ be an $N\times N$ Hermitian matrix evolving as $\mathcal{M}(0)=A$ plus i.i.d.\ Gaussian increments in time. Denote its ordered eigenvalues at time $t$ by
\[
\lambda_1(t)\;\ge\;\dots\;\ge\;\lambda_N(t).
\]
We aim to find an SDE for $\lambda_i(t)$.

For a small increment $\Delta t$, we have
\[
\mathcal{M}(t+\Delta t)
\;=\;
\mathcal{M}(t)\;+\;\Delta \mathcal{M},
\]
where the entries of $\Delta \mathcal{M}$ are (approximately) independent $\mathcal{N}(0,\Delta t)$ random variables (real or complex). Suppose we diagonalize $\mathcal{M}(t)=U\,\mathrm{diag}(\lambda_1(t),\dots,\lambda_N(t))\,U^\dagger$.

\begin{proof}[Sketch of the computation]
Search for the $i$-th eigenvalue of the form
\[
\lambda \;=\; \lambda_i(T)\;+\;\Delta\lambda
\quad\bigl[\text{expect } \Delta\lambda \approx O(\sqrt{\Delta t})\bigr].
\]

\noindent
We want to solve
\[
\det\!\begin{pmatrix}
\lambda_{1}(T) - \lambda_{i}(T) + B_{1}(\Delta t) - \Delta\lambda
  & \cdots
  & -\,\Delta\lambda\,B_{i1}(\Delta t) \\[6pt]
\vdots
  & \ddots
  & \vdots \\[6pt]
-\,\Delta\lambda\,B_{1i}(\Delta t)
  & \cdots
  & \lambda_{N}(T) - \lambda_{i}(T) + B_{N}(\Delta t) - \Delta\lambda
\end{pmatrix}
\;=\;0.
\]
In this matrix only $N-1$ diagonal elements --- excluding the $(i,i)$ entry ---
are of finite size; the remaining off-diagonal elements are small.
We have
\[
\det
= \prod_{m=1}^{N}\Bigl[\lambda_{m}(T) - \lambda_{i}(T) + B_{m}(\Delta t) - \Delta\lambda\Bigr]
\;-\;
\sum_{j\neq i} \biggl(\,\prod_{\substack{m \neq j \\ m \neq i}}
   \bigl[\lambda_{m}(T) - \lambda_{i}(T) + B_{m}(\Delta t) - \Delta\lambda\bigr]
\biggr)\,\frac{1}{2}\,B_{ji}^{2}(\Delta t)
\;+\;o(\Delta t).
\]
Here, the first product (diagonal part) involves all $N$ diagonal-like terms,
and the sum over $j \neq i$ ($N-1$ diagonal elements) accounts for corrections
from the off-diagonal blocks.
Higher-order terms are $o(\Delta t)$.

Divide by
\(\displaystyle \prod_{m \neq i}\bigl[\lambda_{m}(T) - \lambda_{i}(T)
     + B_{m}(\Delta t) - \Delta\lambda\bigr]\)
to obtain
\[
o(\Delta t)
\;=\;
-\,\Delta\lambda
\;+\; B_{i}(\Delta t)
\;-\;\sum_{j\neq i}\;
\frac{\tfrac12\,B_{ji}^2(\Delta t)}{\lambda_{j}(T) - \lambda_{i}(T)
      + B_{j}(\Delta t) - \Delta\lambda}.
\]
Hence, to leading order in small $\Delta t$, 
we can ignore $\Delta\lambda$ in the denominator, 
replace $B_{ji}^2(\Delta t)$ by $\Delta t$ as its expectation, ignore 
the random correction (as in Itô calculus), and obtain the desired SDE.
We do not go into further details here, but the details are abundant in the literature,
including the original work of Dyson \cite{dyson1962brownian}.
\end{proof}

\begin{definition}[Dyson Brownian Motion]
\label{def:DBM}
Fix $\beta>0$ and initial data $\bigl(\lambda_1(0)\ge \dots \ge \lambda_N(0)\bigr)$. The \emph{Dyson Brownian Motion} is the unique strong solution to the system of SDEs
\begin{equation}
\label{eq:Dyson_SDE}
d\lambda_i(t)
\;=\;
\frac{\beta}{2}\sum_{j\neq i}\frac{dt}{\lambda_i(t)-\lambda_j(t)}
\;+\;
dW_i(t),
\quad
i=1,\dots,N,
\end{equation}
with the $W_i(t)$ being independent real standard Brownian motions. For $\beta=1,2,4$, this coincides with the eigenvalue process of matrix Brownian motion (GOE, GUE, GSE).
\end{definition}

\begin{remark}
	Equation \eqref{eq:Dyson_SDE} succinctly captures the key idea that the eigenvalues repel each other. Note the singular drift term $\frac{1}{\lambda_i-\lambda_j}$ which pushes $\lambda_i$ away from collisions with $\lambda_j$. This repulsion is so strong (for all $\beta>0$)
	that eigenvalues will not cross (and thus remain ordered) with probability one.
\end{remark}



\section{Transition densities}

\subsection{Starting from zero}

If the Dyson Brownian motion starts from zero\footnote{And then the particles immediately
repel each other and stay ordered for the whole time.}
$\lambda_1(0)=\dots=\lambda_N(0)=0$,
we expect that at time $t$, the density of eigenvalues
is G$\beta$E,
\begin{equation*}
	\propto \prod_{i<j}|\lambda_i-\lambda_j|^{\beta}\ssp
	\exp\left\{ -\frac{1}{2t}\sum_{i}\lambda_i^2 \right\}.
\end{equation*}
This is evident for $\beta=1,2,4$, when we have a matrix model, but not so
much for other $\beta$. For other $\beta$, we would like to show that






\section{Determinantal structure for $\beta=2$}









\section{Derivation of the HCIZ integral}

In this section, we give a self-contained derivation of the Harish--Chandra--Itzykson--Zuber (HCIZ) integral from first principles, in a form commonly used in Random Matrix Theory and particularly in the derivation of Dyson Brownian Motion transition densities.

\subsection{Statement of the HCIZ formula}

Let \(A\) and \(B\) be two \(N\times N\) Hermitian matrices with (real) eigenvalues
\[
   \mathrm{Spec}(A) = (a_1,\dots,a_N),
   \quad
   \mathrm{Spec}(B) = (b_1,\dots,b_N).
\]
We want to compute the integral
\[
   \mathcal{I}(A,B)
   \;:=\;
   \int_{U(N)}
   \exp\bigl(\mathrm{Tr}(A\,U\,B\,U^\dagger)\bigr)
   \,dU,
\]
where \(U(N)\) is the group of \(N\times N\) unitary matrices equipped with its normalized Haar measure \(dU\).
The Harish--Chandra--Itzykson--Zuber formula states that
\[
   \int_{U(N)}
   e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}
   \,dU
   \;=\;
   \Bigl(\prod_{k=1}^{N-1} k!\Bigr)\,
   \frac{\det\!\bigl[e^{\,a_i b_j}\bigr]_{i,j=1}^N}{\prod_{1\le i<j\le N}(a_j-a_i)\,\prod_{1\le i<j\le N}(b_j-b_i)},
\]
up to conventions for the normalization of the Haar measure. Many references fix
the normalization constant
as above.

\subsection{Reduction to the diagonal case}

The integrand \(\exp(\mathrm{Tr}(A\,U\,B\,U^\dagger))\) depends on \(U\) only via conjugation.  Exploiting the Haar measure's bi-invariance:

\begin{enumerate}
   \item Diagonalize \(A = V_A\,\mathrm{diag}(a_1,\dots,a_N)\,V_A^\dagger\).
   \item Diagonalize \(B = V_B\,\mathrm{diag}(b_1,\dots,b_N)\,V_B^\dagger\).
   \item Notice
   \[
      \mathrm{Tr}(A\,U\,B\,U^\dagger)
      \;=\;
      \mathrm{Tr}\Bigl(\mathrm{diag}(a)\,\bigl(V_A^\dagger U V_B\bigr)\,\mathrm{diag}(b)\,\bigl(V_B^\dagger U^\dagger V_A\bigr)\Bigr).
   \]
   Setting \(W=V_A^\dagger\,U\,V_B\) preserves the Haar measure.  Thus
   \[
      \int_{U(N)} e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}\,dU
      \;=\;
      \int_{U(N)}
      e^{\,\mathrm{Tr}(\mathrm{diag}(a)\,W\,\mathrm{diag}(b)\,W^\dagger)}\,dW.
   \]
\end{enumerate}
Therefore, we may assume \(A=\mathrm{diag}(a)\) and \(B=\mathrm{diag}(b)\).  In that case,
\[
   \mathrm{Tr}\bigl(A\,U\,B\,U^\dagger\bigr)
   \;=\;
   \sum_{i,j=1}^N a_i\,b_j\,|U_{ij}|^2.
\]
Hence
\[
   \int_{U(N)}
   \exp\Bigl(\mathrm{Tr}(A\,U\,B\,U^\dagger)\Bigr)\,dU
   \;=\;
   \int_{U(N)}
   \exp\Bigl(\sum_{i,j=1}^N a_i\,b_j\,|U_{ij}|^2\Bigr)\,dU.
\]

\subsection{Symmetry}

\begin{itemize}
   \item The expression must be symmetric in the eigenvalues \(\{a_1,\dots,a_N\}\) (since any permutation of those entries can be achieved by a suitable unitary conjugation).
   \item Similarly, it is symmetric in the eigenvalues \(\{b_1,\dots,b_N\}\).
   \item One can argue that if \(a_i=a_j\) for some \(i\neq j\) but \(b_i\neq b_j\), the integral must vanish (or degenerate) appropriately, which implies the presence of Vandermonde factors in the denominator.
\end{itemize}
By general principles of polynomial (or analytic) symmetry, one shows
\[
   \int_{U(N)} e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}\,dU
   \;=\;
   \Phi_N
   \,\frac{\det[e^{\,a_i b_j}]}{\prod_{1\le i<j\le N}(a_j-a_i)\,\prod_{1\le i<j\le N}(b_j-b_i)},
\]
where \(\Phi_N\) is a constant (independent of the specific eigenvalues).

\subsection*{4. Determining the Constant \(\Phi_N\)}

One standard method to fix \(\Phi_N\):
\begin{enumerate}
   \item Set some \(\{a_i\}\) or \(\{b_j\}\) to special values (e.g.\ let one subset go to 0 or let all but one eigenvalue be 0).
   \item Expand both sides in power series, compare coefficients, or use known integrals from representation theory or from the ``Weingarten formula'' approach.
\end{enumerate}
It is well known that the result is
\[
   \Phi_N
   \;=\;
   \prod_{k=1}^{N-1} k!
   \quad\Longrightarrow\quad
   \int_{U(N)} e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}\,dU
   \;=\;
   \Bigl(\prod_{k=1}^{N-1} k!\Bigr)\,
   \frac{\det\!\bigl[e^{\,a_i b_j}\bigr]}{\prod_{1\le i<j\le N}(a_j-a_i)\,\prod_{1\le i<j\le N}(b_j-b_i)}.
\]

\subsection*{5. Connection to Dyson Brownian Motion}

In the context of Dyson Brownian Motion (for \(\beta=2\)), one writes the law of
\(\mathcal{M}(t)=A+\sqrt{t}\,G_{\mathrm{UE}}\) and integrates out the angular parts via
\[
   \int_{U(N)}
   \exp\!\bigl(\mathrm{Tr}(A\,U\,\Lambda\,U^\dagger)\bigr)\,dU,
\]
where \(\Lambda\) is diagonal with eigenvalues \(\{\lambda_1,\dots,\lambda_N\}\).  The HCIZ formula then produces the classic determinant--Vandermonde factor, yielding the transition density for the eigenvalues of \(\mathcal{M}(t)\).  This leads to the ``\(\exp(a_i\lambda_j)\)'' structure in the joint density or correlation kernels discussed in Lecture~10.












\appendix
\setcounter{section}{9}
\section{Problems (due 2025-04-29)}

\subsection{Collisions}

Show that two independent standard 1D Brownian motions, started at $a_1\neq a_2$, almost surely intersect.

\subsection{Estimate on the modulus of continuity}

Let $B(t)$ be a standard 1D Brownian motion with $B(0)=0$,
defined as a process with independent increments and $B(t)-B(s)\sim \mathcal{N}(0,t-s)$,
without any continuity assumptions.

Show that
\begin{equation*}
	\operatorname{\mathbb{E}}|B(t)-B(s)|^2 \;\le\; |t-s|
\end{equation*}
implies that
that one can take an almost
surely continuous modification of the function $t\mapsto B(t)$.


\subsection{}

\begin{enumerate}[1.]



\item \textbf{Generator for the Dyson SDE.} For $\beta>0$, write the generator $\mathcal{L}$ of the Markov process \eqref{eq:Dyson_SDE} in differential form. That is, show that for any $C^2$ test function $f(\lambda_1,\dots,\lambda_N)$,
\[
\frac{d}{dt}\,\mathbb{E}\bigl[f(\lambda(t))\bigr]
\;=\;\mathbb{E}\bigl[\mathcal{L}f(\lambda(t))\bigr],
\]
where
\[
\mathcal{L} \;=\; \sum_{i=1}^N \bigl(2\,\tfrac{\partial^2}{\partial \lambda_i^2}\bigr)
\;+\;2\beta\sum_{i<j}\,\frac{1}{\lambda_i-\lambda_j}\bigl(\tfrac{\partial}{\partial \lambda_i}-\tfrac{\partial}{\partial \lambda_j}\bigr).
\]
Verify formally that the density from \Cref{prop:beta_invariant} is stationary for $\mathcal{L}$ when $\lambda(0)=0$.



\item \textbf{Limit shape for Non-Intersecting Bridges.} Let $b_1(t),\dots,b_N(t)$ be $N$ real Brownian bridges from $a_1<\dots<a_N$ at $t=0$ to $z_1<\dots<z_N$ at $t=1$, conditioned on no intersections. Simulate numerically for $N=10$ and $a_i=i,\,z_i=2i$. Describe visually how the paths arrange themselves and how they may approximate a continuous shape as $N$ grows.

\item \textbf{Determinantal representation of transition densities (general $\beta=2$).} Fill in details for the matrix integral approach that leads to the transition probability formula in \Cref{thm:DBM_transition_beta2}, including the key use of the unitary integral (the HCIZ formula). You may take the HCIZ formula as known or provide a reference. Make explicit the factor by which $P(\lambda(t)=x\,|\,\lambda(0)=a)$ is normalized.

\item \textbf{DBM short time expansions.} Let $\lambda_i(0)=0$ for all $i$, and consider \eqref{eq:Dyson_SDE} for small $t$. Expand the drift and diffusion terms to second order in $t$, and compare with the approach in \Cref{sub:sde_proof_idea}. Why do the $\tfrac{1}{\lambda_i-\lambda_j}$ terms not blow up when $\lambda_i\approx \lambda_j$ for small $t$?
\end{enumerate}







































\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

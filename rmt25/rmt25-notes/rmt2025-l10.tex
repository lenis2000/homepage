\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 10: Dyson Brownian Motion}


\date{Monday, March 24, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/simulations/model/random-matrices/}{\texttt{Live simulations}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l10.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle

\begin{abstract}
	This lecture begins the second half of the course after the Spring Break, and
	here we start with the Dyson Brownian Motion.
	In the later lectures we explore random growth models and their universal scaling limits
	which are closely connected to random matrix theory.
\end{abstract}

\section{Motivations}
\subsection{Why introduce time?}
Our previous lectures dealt with static matrix ensembles (e.g., GUE, GOE, and so on). However, there are both \emph{physical} and \emph{mathematical} reasons to study a dynamical model for random matrices. For instance:
\begin{enumerate}
\item In physics, one often interprets random matrices as Hamiltonians of quantum systems. It is natural to let these Hamiltonians vary in time and to describe how spectra evolve.
\item Such time-dependent models are vital for studying \emph{universality results} in random matrix theory. Rigorous proofs of local eigenvalue correlations often involve coupling or evolving an ensemble toward (or away from) a known reference ensemble.
\item Dynamical extensions yield intriguing connections to 2D statistical mechanics, representation theory, and Markov chain interpretations such as \emph{nonintersecting path ensembles}.
\end{enumerate}

\subsection{Simple example: $1\times1$ case}
When $N=1$, an $N\times N$ Hermitian matrix is just a single real entry. Thus GUE/GOE/GSE distributions each reduce to a real Gaussian variable with mean $0$ and variance $1$. If we allow \emph{time}, the natural time evolution is standard \emph{Brownian motion} $B(t)$ on $\mathbb{R}$.

Recall that a standard one--dimensional Brownian motion \(B(t)\) is a continuous stochastic process with the following key properties:
\begin{enumerate}
    \item \textbf{Continuity:} \(t\mapsto B(t)\) is almost surely continuous.
    \item \textbf{Independent increments:} For any \(0\leq s < t\), the increment \(B(t)-B(s)\) is independent of the past \(\{B(u): 0\le u \le s\}\).
    \item \textbf{Gaussian increments:} \(B(t)-B(s)\) is normally distributed with mean \(0\) and variance \(t-s\); that is,
    \[
    B(t)-B(s) \sim \mathcal{N}(0,\,t-s).
    \]
\end{enumerate}
Thus, if the process starts at \(B(0)=a\), then for any fixed \(t>0\),
\[
B(t)\sim \mathcal{N}(a,\,t).
\]

Our goal is to generalize this to the case of \emph{matrix-valued} Brownian motion and, ultimately, to see how the \emph{eigenvalues} of such a matrix evolve.

\section{Matrix Brownian motion and its eigenvalues}
\label{sec:matrix_BM}

\subsection{Definition}
Let $X(t)$ be an $N\times N$ matrix whose entries are i.i.d.\ real/complex Brownian motions (depending on $\beta=1,2$). For instance:
\begin{itemize}
\item If $\beta=1$: $X(t)$ has entries that are i.i.d.\ real Brownian motions.
\item If $\beta=2$: $X(t)$ has entries that are i.i.d.\ complex Brownian motions (independent real and imaginary parts).
\end{itemize}
Since $X(t)$ may not be Hermitian, define
\[
	\mathcal{M}(t) \;=\; \frac{1}{\sqrt{2}}\bigl(X(t) + X^\dagger(t)\bigr).
\]
Here $X^\dagger(t)$ is the conjugate transpose. Then $\mathcal{M}(t)$ is an \emph{Hermitian} matrix (or real symmetric for $\beta=1$).

\begin{lemma}
\label{lemma:time_fixed_law}
If $\mathcal{M}(0) = A$ is a fixed deterministic matrix, then $\mathcal{M}(t)$ at time $t$ is distributed as
\[
A \;+\;\sqrt{t}\, G_{\beta},
\]
where $G_{\beta}$ is a random Hermitian matrix from the Gaussian ensemble with $\beta=1$ or $2$.
\end{lemma}
\begin{proof}[Sketch of proof]
	Straightforward observation.
\end{proof}

For the one-dimensional case, notice that $a+\sqrt t\ssp Z$, where $Z\sim \mathcal{N}(0,1)$, is a Gaussian random variable with mean $a$ and variance $t$, and every such Gaussian variable can be represented in this form.

\subsection{Eigenvalues as Markov process}
We now focus on $\lambda_i(t)$, the (ordered) eigenvalues of $\mathcal{M}(t)$. Denote
\[
\lambda(t) = \bigl(\lambda_1(t)\ge \dots \ge \lambda_N(t)\bigr).
\]
\begin{theorem}
\label{thm:lambda_is_markov}
As $t$ varies, the process $\lambda(t)$ is a continuous-time Markov process in $\mathbb{R}^N$.
\end{theorem}
\begin{proof}[Sketch of proof]
	Assume $\beta=2$, the case $\beta=1$ is similar.
We need to show that $\lambda(t)$ depends on its future and past only through its instantaneous value. Using the independent increment property of $X(t)$, consider times $0< u< t$. We have
\[
\mathcal{M}(t) \;=\; \mathcal{M}(u)\;+\;\bigl(\mathcal{M}(t)-\mathcal{M}(u)\bigr).
\]
Since $\mathcal{M}(u)$ diagonalizes to $\mathrm{diag}\bigl(\lambda_1(u),\ldots,\lambda_N(u)\bigr)$ by some unitary $U_u$, we can write
\[
U_u^\dagger\,\mathcal{M}(t)\,U_u \;=\;\mathrm{diag}\bigl(\lambda_1(u),\ldots,\lambda_N(u)\bigr)\;+\; U_u^\dagger\bigl(\mathcal{M}(t)-\mathcal{M}(u)\bigr)\,U_u.
\]
The second term again has i.i.d.\ random entries (due to unitary invariance of
GUE), independent of $\mathcal{M}(s)$ for $s\le u$.
Therefore, conditioned on $\mathcal{M}(s)$, $s\le u$, the dependence only
comes through $\lambda(u)$, and the eigenvalues $\lambda_i(s)$ for $s\ge u$ follow
the same dynamics. This proves the Markov property.
\end{proof}

\section{Dyson Brownian Motion}
We now describe the stochastic differential equation (SDE)
for $\lambda(t)$ explicitly, following the classical result due to Dyson
\cite{dyson1962brownian}. Let us first briefly discuss what is an SDE.

\subsection{Stochastic differential equations - an informal introduction}

In order to describe the eigenvalues of a time-dependent Hermitian matrix, we rely on \emph{stochastic differential equations} (SDEs). These are differential equations where one or more of the terms involve \emph{random noise}. For simplicity, we start with the one-dimensional setup and later extend it to systems of equations such as those arising in Dyson Brownian Motion.

In an ordinary differential equation (ODE), a function \(x(t)\) evolves according to a deterministic rule of the form
\[
\frac{dx(t)}{dt} \;=\; b\bigl(x(t)\bigr),
\]
where \(b(\,\cdot\,)\) is a deterministic function called the \emph{drift}. If one imposes an initial condition \(x(0)=x_0\), then classical theorems guarantee that, under mild regularity assumptions, a unique solution exists for all \(t\ge0\).

An SDE generalizes this setup by adding a \emph{stochastic (or noise) term} to the right-hand side. Concretely, suppose \(W(t)\) is a standard one-dimensional Brownian motion. Then the simplest SDE has the form
\[
dx(t) \;=\; \sigma\, dW(t),
\]
where \(\sigma\) is a nonnegative constant. This equation may be formally interpreted as
\[
\frac{dx(t)}{dt} \;=\; \sigma\,\frac{dW(t)}{dt},
\]
but it should be emphasized that \(\tfrac{dW}{dt}\) does not exist in the usual sense of classical calculus (Brownian motion is nowhere differentiable almost surely). Instead, one interprets the equation via the \emph{It\^{o} integral}
\[
x(t) \;=\; x(0)\;+\;\int_0^t \sigma\, dW(s).
\]
This integral is defined carefully through a limit of sums involving the increments \(W(t_{k+1})-W(t_k)\), yielding an \emph{almost sure} continuous stochastic process \(t\mapsto x(t)\).

\medskip
More generally, one allows both \emph{drift} and \emph{diffusion} terms:
\begin{equation}
\label{eq:1D_SDE_general}
dx(t) \;=\; b\bigl(x(t)\bigr)\,dt \;+\; \sigma\bigl(x(t)\bigr)\,dW(t).
\end{equation}
Here,
\begin{itemize}
\item \(b(\cdot)\) is the \emph{drift coefficient}, capturing deterministic motion;
\item \(\sigma(\cdot)\) is the \emph{diffusion coefficient}, encoding how strongly the process is randomized by Brownian motion.
\end{itemize}
Under suitable Lipschitz and growth conditions on \(b\) and \(\sigma\), one can show \emph{existence and pathwise uniqueness} of strong solutions to \eqref{eq:1D_SDE_general}. Concretely, this means there is almost surely a unique process \(x(t)\) satisfying \eqref{eq:1D_SDE_general} for each realization of the Brownian motion \(W(t)\). One constructs such a solution, for example, by an iterative limit of approximations.
The simplest discrete-time approximation, analogous to Euler’s method for ordinary differential equations. Over a small time step \(\Delta t\), one approximates
\[
x_{n+1} \;=\; x_n \;+\; b(x_n)\,\Delta t \;+\; \sigma(x_n)\,\bigl(W(t_{n+1}) - W(t_n)\bigr).
\]
This scheme converges to the true solution pathwise under standard Lipschitz conditions on \(b\) and \(\sigma\).


\medskip
A major utility of SDEs is in performing \emph{It\^{o} calculus}. Suppose \(x(t)\) solves the SDE \eqref{eq:1D_SDE_general} and let \(f\colon\mathbb{R}\to\mathbb{R}\) be a sufficiently smooth function. One might try to apply the usual chain rule to \(f(x(t))\), but must account for the extra "noise" term. The correct extension is the \emph{It\^{o} formula}:
\[
df\bigl(x(t)\bigr)
\;=\;
\frac{\partial f}{\partial x}\bigl(x(t)\bigr)\,dx(t)
\;+\;\frac12\,
\frac{\partial^2 f}{\partial x^2}\bigl(x(t)\bigr)\,\bigl(dW(t)\bigr)^2,
\]
where \((dW(t))^2\) is interpreted as \(dt\) in a formal sense. Substituting \eqref{eq:1D_SDE_general} yields:
\[
df\bigl(x(t)\bigr)
\;=\;
b\bigl(x(t)\bigr)\,\frac{\partial f}{\partial x}\bigl(x(t)\bigr)\,dt
\;+\;
\sigma\bigl(x(t)\bigr)\,\frac{\partial f}{\partial x}\bigl(x(t)\bigr)\,dW(t)
\;+\;
\frac12\,\sigma^2\bigl(x(t)\bigr)\,\frac{\partial^2 f}{\partial x^2}\bigl(x(t)\bigr)\,dt.
\]
This identity is an indispensable tool for analyzing stochastic processes, both in theoretical and applied contexts.

\medskip
To handle matrix-valued processes, one must consider multi-dimensional (or matrix-dimensional) analogs of \eqref{eq:1D_SDE_general}. For instance, if \(X(t)\in\mathbb{R}^n\) is an \(n\)-dimensional stochastic process, the SDE becomes
\[
dX(t)
\;=\;
b\bigl(X(t)\bigr)\,dt
\;+\;
\sigma\bigl(X(t)\bigr)\,dW(t),
\]
where \(b(\cdot)\colon\mathbb{R}^n\to\mathbb{R}^n\) and \(\sigma(\cdot)\colon\mathbb{R}^n\to \mathbb{R}^{n\times n}\). Here \(W(t)\) is an \(n\)-dimensional Brownian motion, and the product \(\sigma\bigl(X(t)\bigr)\,dW(t)\) is understood as a matrix-vector multiplication in each small time increment. Existence, uniqueness, and It\^{o}’s formula all generalize naturally under suitable regularity assumptions.

\paragraph{Summary}
Although SDEs can be introduced rigorously via measure-theoretic tools, the above \emph{informal} derivation and discussion provide a workable framework for many typical computations. The key points are:
\begin{itemize}
\item Brownian motion’s roughness prevents classical differential calculus, so new techniques (It\^{o} integrals) are needed.
\item The It\^{o} formula extends the classical chain rule by adding a second-order correction term.
\item Existence and uniqueness theorems ensure that SDEs define well-posed dynamical systems in a stochastic setting.
\item Extending to matrix-valued (or multi-dimensional) settings is conceptually straightforward but requires careful linear algebraic bookkeeping and additional regularity arguments.
\end{itemize}
Equipped with these ideas, we can rigorously address how the eigenvalues of a random matrix evolve over continuous time, culminating in the Dyson Brownian Motion description of Hermitian ensembles.

\subsection{Heuristic derivation of the SDE for the Dyson Brownian Motion}

Let $\mathcal{M}(t)$ be an $n\times n$ Hermitian matrix evolving as $\mathcal{M}(0)=A$ plus i.i.d.\ Gaussian increments in time. Denote its ordered eigenvalues at time $t$ by
\[
\lambda_1(t)\;\ge\;\dots\;\ge\;\lambda_n(t).
\]
We aim to find an SDE for $\lambda_i(t)$.

For a small increment $\Delta t$, we have
\[
\mathcal{M}(t+\Delta t)
\;=\;
\mathcal{M}(t)\;+\;\Delta \mathcal{M},
\]
where the entries of $\Delta \mathcal{M}$ are (approximately) independent $\mathcal{N}(0,\Delta t)$ random variables (real or complex). Suppose we diagonalize $\mathcal{M}(t)=U\,\mathrm{diag}(\lambda_1(t),\dots,\lambda_n(t))\,U^\dagger$.

\begin{proof}[Sketch of the computation]
Search for the $i$-th eigenvalue of the form
\[
\lambda \;=\; \lambda_i(T)\;+\;\Delta\lambda
\quad\bigl[\text{expect } \Delta\lambda \approx O(\sqrt{\Delta t})\bigr].
\]

\noindent
We want to solve
\[
\det\begin{pmatrix}
\lambda_{1}(T) - \lambda_{i}(T) + B_{11}(\Delta t) - \Delta\lambda
		& \cdots
		& \frac{1}{\sqrt2}\,B_{i1}(\Delta t) \\[6pt]
\vdots
		& \ddots
		& \vdots \\[6pt]
		\frac{1}{\sqrt2}\,B_{1i}(\Delta t)
		& \cdots
		& \lambda_{n}(T) - \lambda_{i}(T) + B_{nn}(\Delta t) - \Delta\lambda
\end{pmatrix}
\;=\;0.
\]
In this matrix only $n-1$ diagonal elements --- excluding the $(i,i)$ entry ---
are bounded away from zero; the remaining $(i,i)$-th off-diagonal element is small.
We have
\[
\det
= \prod_{m=1}^{n}\Bigl[\lambda_{m}(T) - \lambda_{i}(T) + B_{mm}(\Delta t) - \Delta\lambda\Bigr]
\;-\;
\sum_{j\neq i} \biggl(\,\prod_{\substack{m \neq j \\ m \neq i}}
			\bigl[\lambda_{m}(T) - \lambda_{i}(T) + B_{mm}(\Delta t) - \Delta\lambda\bigr]
\biggr)\,\frac{1}{2}\,B_{ji}^{2}(\Delta t)
\;+\;o(\Delta t).
\]
Here, the first product (diagonal part) involves all $n$ diagonal-like terms,
and the sum over $j \neq i$ ($n-1$ diagonal elements) accounts for corrections
from the off-diagonal blocks.
Higher-order terms are $o(\Delta t)$.

Divide by
\(\displaystyle \prod_{m \neq i}\bigl[\lambda_{m}(T) - \lambda_{i}(T)
					+ B_{m}(\Delta t) - \Delta\lambda\bigr]\)
to obtain
\[
o(\Delta t)
\;=\;
-\,\Delta\lambda
\;+\; B_{ii}(\Delta t)
\;-\;\sum_{j\neq i}\;
\frac{\tfrac12\,B_{ji}^2(\Delta t)}{\lambda_{j}(T) - \lambda_{i}(T)
						+ B_{j}(\Delta t) - \Delta\lambda}.
\]
Hence, to leading order in small $\Delta t$,
we can ignore $\Delta\lambda$ in the denominator,
replace $B_{ji}^2(\Delta t)$ by $\Delta t$ as its expectation,\footnote{For
other $\beta$, this will be $\beta \Delta t$, due to the dimensionality of the
Brownian motion on the full rank matrix.}
ignore
the random correction (as in It\^{o} calculus), and obtain the desired SDE.
We do not go into further details here, but the details are abundant in the literature,
including the original work of Dyson \cite{dyson1962brownian}.
\end{proof}

\begin{definition}[Dyson Brownian Motion]
\label{def:DBM}
Fix $\beta>0$ and initial data $\bigl(\lambda_1(0)\ge \dots \ge \lambda_n(0)\bigr)$. The \emph{Dyson Brownian Motion} is the unique strong solution to the system of SDEs
\begin{equation}
\label{eq:Dyson_SDE}
d\lambda_i(t)
\;=\;
\frac{\beta}{2}\sum_{j\neq i}\frac{dt}{\lambda_i(t)-\lambda_j(t)}
\;+\;
dW_i(t),
\quad
i=1,\dots,n,
\end{equation}
with the $W_i(t)$ being independent real standard Brownian motions. For $\beta=1,2,4$, this coincides with the eigenvalue process of matrix Brownian motion (GOE, GUE, GSE).
\end{definition}

\begin{remark}
	Equation \eqref{eq:Dyson_SDE} succinctly captures the key idea that the eigenvalues repel each other. Note the singular drift term $\frac{1}{\lambda_i-\lambda_j}$ which pushes $\lambda_i$ away from collisions with $\lambda_j$. This repulsion is so strong (for all $\beta>0$)
	that eigenvalues will not cross (and thus remain ordered) with probability one.
\end{remark}

\section{Mapping the G$\beta$E densities with the Dyson Brownian Motion}

If the Dyson Brownian motion starts from zero\footnote{And then the particles immediately
repel each other and stay ordered for the whole time.}
$\lambda_1(0)=\dots=\lambda_N(0)=0$,
we expect that at time $t$, the density of eigenvalues
is G$\beta$E,
\begin{equation*}
	\propto \prod_{i<j}|\lambda_i-\lambda_j|^{\beta}\ssp
	\exp\left\{ -\frac{1}{2t}\sum_{i}\lambda_i^2 \right\}.
\end{equation*}
This is evident for $\beta=1,2,4$, when we have a matrix model, but not so
much for other $\beta$. For other $\beta$, we would like to
\begin{itemize}
\item Make sense of the SDE and its solutions. We skip this part in the course.
\item Make a computation checking that the above density is preserved under the SDE
\eqref{eq:Dyson_SDE}.
\end{itemize}

For example,
in the $N=1$ case, $d\lambda = dW(t)$ is a Markov process and one wants to show that
\[
p(t,\lambda) \;=\; \frac{1}{\sqrt{2\pi\,t}}\,
\exp\Bigl(-\frac{\lambda^{2}}{2\,t}\Bigr)
\]
is preserved in the evolution.  To verify this, one computes the generator
of the semigroup, which for Brownian motion is
\[
\frac{1}{2}\,\frac{\partial^{2}}{\partial\lambda^{2}}.
\]
One then checks that
\[
\frac{\partial}{\partial t}\,p(t,\lambda)
\;=\;
\frac{1}{2}\,\frac{\partial^{2}}{\partial\lambda^{2}}\,p(t,\lambda).
\]
This is a direct computation.

For larger $N$, one needs to write down the corresponding generator and check
that the same type of equation is satisfied.
See Problem~\ref{prob:generator}.

\section{Determinantal structure for $\beta=2$}

To understand the determinantal structure of the Dyson Brownian Motion, we 
first need the explicit transition probabilities:

\begin{theorem}[\(\beta=2\) Dyson Brownian Motion transition probabilities]
	\label{thm:dbm-transition}
For \(\beta=2\), let \(\lambda(t)=(\lambda_1(t)\ge \cdots \ge \lambda_N(t))\) follow Dyson Brownian Motion starting at \(\lambda(0)=\mathbf{a}=(a_1\ge \cdots \ge a_N)\).  Then for each fixed time \(t>0\),
\[
P\bigl(\lambda(t) = \mathbf{x}\;\big|\;\lambda(0)=\mathbf{a}\bigr)
\;=\;
N!\,\bigl(\frac{1}{\sqrt{2\pi t}}\bigr)^{N}
\;\prod_{1\le i<j\le N}\frac{x_i - x_j}{a_i - a_j}
\;\det\Bigl[\exp\Bigl(-\frac{(x_i - a_j)^2}{2t}\Bigr)\Bigr]_{i,j=1}^N,
\]
where \(x_1 \ge \dots \ge x_N\).
\end{theorem}

The proof of this theorem is given in the next
\href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l11.pdf}{Lecture 11},
based on the Harish--Chandra--Itzykson--Zuber formula that we
outline next.


\section{Harish-Chandra--Itzykson--Zuber (HCIZ) integral}

In this section, we give a self-contained derivation of the Harish--Chandra--Itzykson--Zuber (HCIZ) integral from first principles, in a form commonly used in Random Matrix Theory and particularly in the derivation of Dyson Brownian Motion transition densities.

\subsection{Statement of the HCIZ formula}

Let \(A\) and \(B\) be two \(N\times N\) Hermitian matrices with (real) eigenvalues
\[
   \mathrm{Spec}(A) = (a_1,\dots,a_N),
   \quad
   \mathrm{Spec}(B) = (b_1,\dots,b_N).
\]
We want to compute the integral
\[
   \mathcal{I}(A,B)
   \;:=\;
   \int_{U(N)}
   \exp\bigl(\mathrm{Tr}(A\,U\,B\,U^\dagger)\bigr)
   \,dU,
\]
where \(U(N)\) is the group of \(N\times N\) unitary matrices equipped with its normalized Haar measure \(dU\).
The Harish--Chandra--Itzykson--Zuber formula states that
\[
   \int_{U(N)}
   e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}
   \,dU
   \;=\;
   \Bigl(\prod_{k=1}^{N-1} k!\Bigr)\,
   \frac{\det\bigl[e^{\,a_i b_j}\bigr]_{i,j=1}^N}{\prod_{1\le i<j\le N}(a_j-a_i)\,\prod_{1\le i<j\le N}(b_j-b_i)},
\]
up to conventions for the normalization of the Haar measure. Many references fix
the normalization constant
as above.

\subsection{Reduction to the diagonal case}

The integrand \(\exp(\mathrm{Tr}(A\,U\,B\,U^\dagger))\) depends on \(U\) only via conjugation.  Exploiting the Haar measure's bi-invariance:

\begin{enumerate}
   \item Diagonalize \(A = V_A\,\mathrm{diag}(a_1,\dots,a_N)\,V_A^\dagger\).
   \item Diagonalize \(B = V_B\,\mathrm{diag}(b_1,\dots,b_N)\,V_B^\dagger\).
   \item Notice
   \[
      \mathrm{Tr}(A\,U\,B\,U^\dagger)
      \;=\;
      \mathrm{Tr}\Bigl(\mathrm{diag}(a)\,\bigl(V_A^\dagger U V_B\bigr)\,\mathrm{diag}(b)\,\bigl(V_B^\dagger U^\dagger V_A\bigr)\Bigr).
   \]
   Setting \(W=V_A^\dagger\,U\,V_B\) preserves the Haar measure.  Thus
   \[
      \int_{U(N)} e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}\,dU
      \;=\;
      \int_{U(N)}
      e^{\,\mathrm{Tr}(\mathrm{diag}(a)\,W\,\mathrm{diag}(b)\,W^\dagger)}\,dW.
   \]
\end{enumerate}
Therefore, we may assume \(A=\mathrm{diag}(a)\) and \(B=\mathrm{diag}(b)\).  In that case,
\[
   \mathrm{Tr}\bigl(A\,U\,B\,U^\dagger\bigr)
   \;=\;
   \sum_{i,j=1}^N a_i\,b_j\,|U_{ij}|^2.
\]
Hence
\begin{equation}
	\label{eq:HCIZ_to_compute}
   \int_{U(N)}
   \exp\Bigl(\mathrm{Tr}(A\,U\,B\,U^\dagger)\Bigr)\,dU
   \;=\;
   \int_{U(N)}
   \exp\Bigl(\sum_{i,j=1}^N a_i\,b_j\,|U_{ij}|^2\Bigr)\,dU.
 \end{equation}

\subsection{Symmetry}

Let \(f(A,B)\) denote the
right-hand side of \eqref{eq:HCIZ_to_compute}.
We have established that $f(A,B)$ must be:
\begin{enumerate}
   \item Symmetric in the eigenvalues $\{a_1,\ldots,a_N\}$ of $A$
   \item Symmetric in the eigenvalues $\{b_1,\ldots,b_N\}$ of $B$
   \item Analytic in all variables when the eigenvalues are distinct
\end{enumerate}

When some eigenvalues coincide, the function must behave appropriately. Specifically:
\begin{lemma}
If $a_i = a_j$ for some $i \neq j$, then $f(A,B)$ must be invariant under permuting the corresponding $b_i$ and $b_j$.
\end{lemma}
\begin{proof}
When eigenvalues coincide, the corresponding eigenvectors can be chosen arbitrarily within the degenerate subspace. This means that when $a_i = a_j$, we can apply a unitary transformation that effectively swaps the roles of $b_i$ and $b_j$ without changing the integral.
\end{proof}

\textit{Remark on rigor.}
To make these symmetry arguments fully rigorous, one notes that $f(A,B)$ can be extended to an analytic function of the eigenvalues (even when they are treated as complex variables close to the real axis). Moreover, if some $a_i = a_j$, the existence of a unitary acting within the degenerate subspace justifies the required symmetry in $(b_i,b_j)$. One also checks that $f(A,B)$ remains finite in the limit $(a_j-a_i)\to 0$ or $(b_j-b_i)\to 0$, enforcing vanishing at a rate that compensates for the factor in the denominator.

This constraint, combined with analyticity, forces $f(A,B)$ to vanish as $(a_j-a_i) \to 0$ or $(b_j-b_i) \to 0$ at a rate that exactly cancels the denominator's singularity. The form of the answer must therefore be:
\[
   f(A,B) = \frac{g(A,B)}{\prod_{1\le i<j\le N}(a_j-a_i)\,\prod_{1\le i<j\le N}(b_j-b_i)},
\]
where $g(A,B)$ is analytic and antisymmetric in the $\{a_i\}$ and in the $\{b_i\}$ variables.

\subsection{Conclusion of the argument}

By the fundamental theorem of antisymmetric polynomials, $g(A,B)$ must be expressible as a product of the Vandermonde determinants and a symmetric function. Moreover, by examining the behavior under the scaling $A \mapsto tA$ and $B \mapsto B/t$, one shows that the only function with the correct analytic properties and scaling behavior is
\[
   g(A,B) = C_N \cdot \det[e^{\,a_i b_j}]_{i,j=1}^N,
\]
where $C_N$ is a constant depending only on $N$. One can alternatively pin this down by checking that $f(A,B)$ satisfies a certain heat equation in $A$ (or $B$), and thus matches the known solution $\det[e^{\,a_i b_j}]$ up to a constant.

Therefore, we have established that
\[
   \int_{U(N)} e^{\,\mathrm{Tr}(A\,U\,B\,U^\dagger)}\,dU
   \;=\;
   \Phi_N
   \,\frac{\det[e^{\,a_i b_j}]_{i,j=1}^N}{\prod_{1\le i<j\le N}(a_j-a_i)\,\prod_{1\le i<j\le N}(b_j-b_i)},
\]
where $\Phi_N = C_N$ is a normalization constant independent
of the eigenvalues. Through a separate calculation
(see Problem~\ref{prob:PhiN}), often
involving either a small-time heat-kernel expansion or a
rank-one reduction, one can determine that
\begin{equation}
	\label{eq:PhiN}
   \Phi_N = \prod_{k=1}^{N-1} k!.
 \end{equation}












\appendix
\setcounter{section}{9}
\section{Problems (due 2025-04-29)}

\subsection{Collisions}

Show that two independent standard 1D Brownian motions, started at $a_1\neq a_2$, almost surely intersect.

\subsection{Estimate on the modulus of continuity}

Let $B(t)$ be a standard 1D Brownian motion with $B(0)=0$,
defined as a process with independent increments and $B(t)-B(s)\sim \mathcal{N}(0,t-s)$,
without any continuity assumptions.

Show that
\begin{equation*}
	\operatorname{\mathbb{E}}|B(t)-B(s)|^2 \;\le\; |t-s|
\end{equation*}
implies that
that one can take an almost
surely continuous modification of the function $t\mapsto B(t)$.

\subsection{Generator for Dyson Brownian Motion}
\label{prob:generator}

Consider the Dyson Brownian Motion(\Cref{def:DBM}) for general $\beta > 0$. The invariant measure for this process when started from zero is expected to be the distribution with density proportional to:
\begin{equation*}
p_{\beta}(\lambda_1,\ldots,\lambda_N) \propto \prod_{i<j}|\lambda_i-\lambda_j|^{\beta} \exp\left\{-\frac{1}{2}\sum_{i=1}^N \lambda_i^2\right\}.
\end{equation*}
Prove that this density is invariant under the Dyson SDE \eqref{eq:Dyson_SDE} by showing
\begin{equation*}
\mathcal{L}p_{\beta} = 0,
\end{equation*}
where $\mathcal{L}$ is the infinitesimal generator of the process. Specifically, compute:
\begin{equation*}
\mathcal{L}\rho = \frac{1}{2}\sum_{i=1}^N
\frac{\partial^2}{\partial \lambda_i^2} \rho-
\frac{\beta}{2}\sum_{i=1}^N\sum_{j\neq
i}
\frac{\partial}{\partial\lambda_i}\left(
\frac{1}{\lambda_i-\lambda_j} \rho
\right),
\end{equation*}
and verify that it indeed annihilates $p_{\beta}$.

\subsection{Constant in the HCIZ formula}
\label{prob:PhiN}

Show that in the Harish--Chandra--Itzykson--Zuber formula, the constant $\Phi_N$ is given by
\[
   \Phi_N = \prod_{k=1}^{N-1} k!,
\]
by \emph{directly} evaluating the left-hand side for the special case
\[
   A=\mathrm{diag}(x,0,\dots,0),
   \quad
   B=\mathrm{diag}(y,0,\dots,0).
\]
In this rank-one case, note that
\[
   \mathrm{Tr}(A\,U\,B\,U^\dagger)
   \;=\;
   x\,y \,\bigl|U_{11}\bigr|^2.
\]
You can then reduce the integral to one over the distribution of the first column of $U$,
which is a vector uniformly distributed on the complex unit sphere $\mathbb{C}^N$ (under the normalized Haar measure).
Use the known Jacobian for this parametrization to perform the integral and match it with
the right-hand side evaluated at $(a_1,b_1)=(x,y)$ and $(a_2=\dots=a_N=b_2=\dots=b_N=0)$.









































\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

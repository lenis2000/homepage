\documentclass[letterpaper,11pt,oneside,reqno]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[pdftex,backref=page,colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage[alphabetic,nobysame]{amsrefs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathtools}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{datetime}
\usepackage{comment}

%paper geometry
\usepackage[DIV=12]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific
\newcommand{\ssp}{\hspace{1pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{example}[proposition]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newenvironment{lnotes}{\section*{Notes for the lecturer}}{}

% \excludecomment{lnotes}

\begin{document}
\title{Lectures on Random Matrices
(Spring 2025)
\\Lecture 1: Moments of random variables and random
matrices}


\date{Monday, January 13, 2025\footnote{\href{https://lpetrov.cc/rmt25/}{\texttt{Course webpage}}
$\bullet$ \href{https://lpetrov.cc/rmt25/rmt25-notes/rmt2025-l01.tex}{\texttt{TeX Source}}
$\bullet$
Updated at \currenttime, \today}}



\author{Leonid Petrov}


\maketitle

\begin{lnotes}

``If you find something nice or beautiful or efficient in
RMT or computing/numerics around RMT, please share with me.''

\medskip


\textbf{Course Meetings:}
\begin{itemize}
				\item Classes are scheduled for 2:00-3:15 PM in Kerchof 326 on Wednesdays and some Mondays.
				\item Lecture notes, including problem sets, will be available online.
\end{itemize}

\textbf{Lecture Schedule:}
\begin{itemize}
				\item (Mon) January 13: Moments of random variables and matrices.
				\item (Wed) January 15: Wigner's semicircle law.
				\item (Wed) January 22: Eigenvalue densities, Dyson Brownian motion.
				\item Additional lectures are scheduled for the following dates:
				\begin{itemize}
								\item (Wed) January 29
								\item (Wed) February 5
								\item (Wed) February 12
								\item (Wed) February 19
								\item (Wed) February 26
								\item (Wed) March 5
								\item (Mon) March 24
								\item (Wed) March 26
								\item (Wed) April 2
								\item (Wed) April 9
								\item (Wed) April 16
								\item (Wed) April 23
				\end{itemize}
\end{itemize}

\textbf{Student Presentations:}
\begin{itemize}
				\item To be held on Mondays in April:
				\begin{itemize}
								\item (Mon) April 7
								\item (Mon) April 14
								\item (Mon) April 21
								\item (Mon) April 28 (tentative - dependent on student enrollment)
				\end{itemize}
\end{itemize}

\textbf{Weekly Individual Meetings:}
\begin{itemize}
				\item The course covers a span of 12 full weeks, excluding the first week, the final week, and the week following Spring break, during which I will be traveling.
				\item Students will have one-on-one meetings at least 8 times during this period, each lasting approximately 45 minutes. While in-person meetings are preferred, a Zoom option is available.
				\item These meetings are vital to the reading course component, allowing discussions on lectures, homework problems, student presentations, and research topics related to random matrices.
\end{itemize}
During the first week, a regular weekly meeting time will be set up for each student. Consistency throughout the semester is highly encouraged, although some flexibility is possible.





\end{lnotes}


\section{Why study random matrices?}

\paragraph{On the history.}
Random matrix theory (RMT) is a fascinating field that
studies
properties of matrices with randomly generated entries,
focusing (at least initially)
on the statistical behavior of their eigenvalues.
This theory finds its roots in the domain of nuclear
physics through the pioneering work of Wigner, Dyson, and
others \cite{wigner1955characteristic},
\cite{dyson1962brownian},
\cite{Dyson1962_III}, who utilized it to analyze the energy levels of complex quantum systems.
Other, earlier roots include statistics \cite{dixon1905generalization}
and classical Lie groups \cite{Hurwitz1897}.
Today, RMT has evolved to span a wide array of disciplines,
from pure mathematics, including areas such as integrable
systems and representation theory, to practical applications
in fields like data science and engineering.

\paragraph{Classical groups and Lie theory.}
Random matrices are deeply connected to \emph{classical Lie groups}, particularly the orthogonal, unitary, and symplectic groups. This connection emerges primarily due to the invariance properties of these groups, such as those derived from the Haar measure.
Random matrices significantly impact representation theory, linking to integrals over matrix groups through character expansions. The symmetry classes of random matrix ensembles, like the Gaussian Orthogonal (GOE), Unitary (GUE), and Symplectic (GSE), correspond to respective symmetry groups.

\paragraph{Toolbox.}
RMT utilizes a broad range of tools ranging across all of mathematics, including probability theory, combinatorics, analysis (classical and modern), algebra, representation theory, and number theory.
The theory of random matrices is a rich source of problems and techniques for all of mathematics.

The main content of this course is to explore the toolbox
around random matrices, including going into discrete models
like dimers and statistical mechanics. Some of this will be included
in the lectures, and some other topics will be covered in the
reading course component, which is individualized.

\paragraph{Applications.}
Random matrix theory finds applications across a diverse set
of fields. In nuclear physics, random matrix ensembles serve
as models for complex quantum Hamiltonians, thereby
explaining the statistics of energy levels. In number
theory, connections have been drawn between random matrices
and the Riemann zeta function, particularly concerning the
distribution of zeros on the critical line. Wireless
communications benefit from random matrix theory through the
analysis of eigenvalue distributions, which helps in
understanding channel capacity in multi-antenna (MIMO) systems. In the burgeoning field of
machine learning, random weight matrices and their spectra
are key to analyzing neural networks and their
generalization capabilities. High-dimensional statistics
and econometrics
also draw on random matrix tools for tasks such as principal
component analysis and covariance estimation in large
datasets. Additionally, combinatorial random processes
exhibit connections to random permutations, random graphs,
and partition theory, all through the lens of matrix
integrals.

\section{Recall Central Limit Theorem}

\subsection{Central Limit Theorem and examples}

We begin by establishing the necessary groundwork for understanding and proving
the Central Limit Theorem. The theorem's power lies in its remarkable universality:
it applies to a wide variety of probability distributions under mild conditions.

\begin{definition}
A sequence of random variables $\{X_i\}_{i=1}^{\infty}$ is said to be
\emph{independent and identically distributed (iid)}
if:

\begin{itemize}
    \item Each $X_i$ has the same probability distribution as every other $X_j$, for all $i, j$.
    \item The variables are mutually independent, meaning that for any finite subset $\{X_1, X_2, \dots, X_n\}$, the joint distribution factors as the product of the individual distributions:
    \[
			\operatorname{\mathbb{P}}(X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n)
			=
			\operatorname{\mathbb{P}}(X_1 \leq x_1)
			\operatorname{\mathbb{P}}(X_2 \leq x_2) \cdots \operatorname{\mathbb{P}}(X_n \leq x_n).
    \]
\end{itemize}
\end{definition}

\begin{theorem}[Classical Central Limit Theorem]
	Let $\{X_i\}_{i=1}^{\infty}$ be a sequence of iid random variables with finite mean $\mu = \operatorname{\mathbb{E}}[X_i]$ and finite
	variance $\sigma^2 = \operatorname{\mathrm{Var}}(X_i)$.
	Define the normalized sum
\[
Z_n = \frac{1}{\sqrt{n}} \sum_{i=1}^n \left(X_i - \mu\right).
\]
Then, as $n \to \infty$, the distribution of $Z_n$ converges in distribution to a normal random variable with mean $0$ and variance $\sigma^2$, i.e.,
\[
Z_n \xrightarrow{d} \mathcal{N}(0, \sigma^2).
\]
\end{theorem}
Convergence in distribution means
\begin{equation}
	\label{eq:conv-in-dist}
	\lim_{n \to \infty} \operatorname{\mathbb{P}}(Z_n \leq x) = \operatorname{\mathbb{P}}(Z \leq x)
		= \int_{-\infty}^x \frac{1}{\sqrt{2\pi \sigma^2}}\ssp e^{-\frac{t^2}{2\sigma^2}} \, dt
	\qquad
	\text{for all } x \in \mathbb{R},
\end{equation}
where $Z \sim \mathcal{N}(0, \sigma^2)$ is the Gaussian random variable.

\begin{remark}
	For a general random variable instead of
	$Z\sim \mathcal{N}(0, \sigma^2)$, the convergence in distribution
	\eqref{eq:conv-in-dist} holds only for $x$ at which the cumulative distribution function of $Z$ is continuous.
	Since the normal distribution is absolutely continuous (has density), the convergence holds for all $x$.
\end{remark}
\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.5\textwidth]{./pictures/uniform_pdfs.pdf}
	\caption{Densities of $U_1$, $U_1+U_2$, $U_1+U_2+U_3$ (where $U_i$ are iid uniform on $[0,1]$),
		and $\mathcal{N}(0,1)$,
		normalized to have the same mean and variance.}
	\label{fig:uniform_pdfs}
\end{figure}

\begin{example}
Let $\{X_i\}_{i=1}^{\infty}$ be a sequence of iid Bernoulli random variables with parameter $p$, meaning that each $X_i$ takes the value $1$ with probability $p$ and $0$ with probability $1 - p$. The mean and variance of each $X_i$ are given by:
\[
	\mu = \operatorname{\mathbb{E}}[X_i] = p, \quad \sigma^2 = \operatorname{\mathrm{Var}}(X_i) = p(1 - p).
\]
We also have the distribution of $X_1+\cdots+X_n$:
\begin{equation*}
	\operatorname{\mathbb{P}}\left( X_1+ \cdots + X_n = k \right) = \binom{n}{k} p^k (1-p)^{n-k},
	\qquad k = 0, 1, \ldots, n.
\end{equation*}

Introduce the normalized quantity
\begin{equation}
	\label{eq:z_normalized_quantity}
	z = \frac{k - np}{\sqrt{np(1-p)}},
\end{equation}
and assume that throughout the asymptotic analysis,
this quantity stays finite.

Our aim is to show that, for $k$ such that $z$ remains bounded as $n\to\infty$, the following holds:
\[
\operatorname{\mathbb{P}}(S_n = k) = \frac{1}{\sqrt{2\pi np(1-p)}} \exp\Bigl( -\frac{z^2}{2} \Bigr) (1+o(1)).
\]

For large $n$, Stirling's formula gives
\[
m! \sim \sqrt{2\pi m}\, m^m e^{-m}, \quad \text{as } m\to\infty.
\]
Apply Stirling's approximation to $n!$, $k!$, and $(n-k)!$:
\[
n! \sim \sqrt{2\pi n}\, n^n e^{-n}, \quad
k! \sim \sqrt{2\pi k}\, k^k e^{-k}, \quad
(n-k)! \sim \sqrt{2\pi (n-k)}\,(n-k)^{n-k} e^{-(n-k)}.
\]
Thus,
\[
\binom{n}{k} \sim \frac{\sqrt{2\pi n}\, n^n e^{-n}}{\sqrt{2\pi k}\, k^k e^{-k}\sqrt{2\pi (n-k)}\,(n-k)^{n-k} e^{-(n-k)}}
= \frac{n^n}{k^k (n-k)^{n-k}}
\frac{1}{\sqrt{2\pi\, k(n-k)/n}}.
\]
More precisely, one often writes
\[
\binom{n}{k} \sim \frac{1}{\sqrt{2\pi np(1-p)}} \exp\Bigl( n\ln n - k\ln k - (n-k)\ln (n-k) \Bigr),
\]
where $p\approx k/n$ thanks to the fact that 
$z$ \eqref{eq:z_normalized_quantity} is assumed to be finite.

We have
\[
k = np+ z\sqrt{np(1-p)}.
\]
Then, consider the second-order Taylor expansion. We have
\[
n\ln n - k\ln k - (n-k)\ln (n-k) \sim n H -\frac{z^2}{2},
\]
where $H=-[p\ln p+(1-p)\ln(1-p)]+c(z;p)/\sqrt n$
(for an explicit function $c(z;p)$)
is the 
``entropy'' term which exactly cancels with the prefactors coming from $p^k (1-p)^{n-k}$.

After combining the approximations from the binomial coefficient and the probability weights, one arrives at
\[
\operatorname{\mathbb{P}}(S_n = k)
\sim \frac{1}{\sqrt{2\pi np(1-p)}} \exp\left( -\frac{z^2}{2} \right),
\]
as desired.


\end{example}


\subsection{Moments of the normal distribution}

\begin{proposition}
The moments of a random variable $Z \sim \mathcal{N}(0, \sigma^2)$ are given by:
\begin{equation}
	\label{eq:normal-moments}
	\operatorname{\mathbb{E}}[Z^k] = \begin{cases}
		0, & \text{if } k \text{ is odd}, \\
		\sigma^k (k-1)!! = \sigma^k \cdot (k-1)(k-3) \cdots 1, & \text{if } k \text{ is even}.
	\end{cases}
\end{equation}
\end{proposition}
\begin{proof}
	We just compute the integrals. Assume $k$ is even (for odd,
	the integral is zero by symmetry). Also assume $\sigma = 1$ for simplicity.
	Then
\begin{equation*}
	\operatorname{\mathbb{E}}[Z^k]
	=
	\frac{1}{\sqrt{2\pi}}
	\int_{-\infty}^{\infty}  z^k e^{-z^2/2} \, dz.
\end{equation*}
Applying integration by parts (putting $ze^{-z^2/2}$ under $d$), we get
\begin{equation*}
	\operatorname{\mathbb{E}}[Z^k] = \frac{1}{\sqrt{2\pi}} \left[-z^{k-1}e^{-z^2/2}\right]_{-\infty}^{\infty} + \frac{k-1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} z^{k-2} e^{-z^2/2} \, dz.
\end{equation*}
The first term vanishes at infinity (you can verify this using L'Hôpital's rule), leaving us with:
\begin{equation*}
	\operatorname{\mathbb{E}}[Z^k] = (k-1)\operatorname{\mathbb{E}}[Z^{k-2}].
\end{equation*}
This gives us a recursive formula, and completes the proof.
\end{proof}


\subsection{Moments of sums of iid random variables}

Let us now show the CLT by moments.
\begin{remark}
	This proof requires an additional assumption
	that all moments of the random variables are finite.
	This is quite a strong assumption, and while the CLT holds
	without it, this proof by moments is more algebraic, and
	will translate to random matrices more directly.
\end{remark}

\subsubsection{Computation of moments}

Denote $Y_i=X_i-\mu$, these are also iid, but have mean $0$.
We consider
\begin{equation*}
	\operatorname{\mathbb{E}}\left[ \left( \sum_{i=1}^n Y_i \right)^k \right].
\end{equation*}
Expanding the $k$-th power using the multinomial theorem, we obtain:
\begin{equation*}
\left( \sum_{i=1}^n Y_i \right)^k = \sum_{j_1 + j_2 + \dots + j_n = k} \frac{k!}{j_1! j_2! \dots j_n!} Y_1^{j_1} Y_2^{j_2} \dots Y_n^{j_n}.
\end{equation*}
Taking the expectation and using linearity, we have:
\begin{align*}
 \mathbb{E}\left[ \left( \sum_{i=1}^n Y_i \right)^k \right]
 &=
 \sum_{j_1 + j_2 + \dots + j_n = k}
 \frac{k!}{j_1! j_2! \dots j_n!} \mathbb{E}\left[Y_1^{j_1}\right]
 \mathbb{E}\left[Y_2^{j_2}\right] \dots
 \mathbb{E}\left[Y_n^{j_n}\right] \\
 &=
 \sum_{j_1 + j_2 + \dots + j_n = k}
 \frac{k!}{j_1! j_2! \dots j_n!}
 \mathsf{m}_{j_1}\ldots \mathsf{m}_{j_n},
\end{align*}
where $\mathsf{m}_j=\mathbb{E}[Y^j]$ (recall the identical distribution of $Y_i$).
Note that 
$\mathsf{m}_0 = 1$ and
$\mathsf{m}_1 = 0$.

The sum over all $j_1, \ldots, j_n$ with $j_1 + \ldots + j_n = k$ is the number of ways to partition $k$ into $n$ non-negative integers.
We can order these integers, and thus 
obtain the sum over all partitions of $k$ into $\le n$ parts.
Since $n$ is large, we simply sum over all partitions of $k$.
For each partition $\lambda$ of $k$
(where $k=\lambda_1+\lambda_2+\ldots+\lambda_n $ and 
$\lambda_1\geq \lambda_2\geq \ldots\geq \lambda_n\geq 0$),
the corresponding term in the sum is
\begin{equation*}
	\frac{k!}{\lambda_1! \lambda_2! \ldots }
	\mathsf{m}_{\lambda_1}\mathsf{m}_{\lambda_2}\ldots.
\end{equation*}
We now continue the proof by ``unordering'' the partitions. In other words, for a given partition 
\[
\lambda=(\lambda_1,\lambda_2,\ldots,\lambda_n)
\]
of the exponent \( k \), we must count the number of distinct multisets of indices \((j_1,j_2,\ldots,j_n)\) that yield the same collection \(\{\lambda_1,\lambda_2,\ldots\}\). Denote by 
\[
b_r = \#\{i : \lambda_i=r\}
\]
the number of parts equal to \( r \) in \(\lambda\). Then there are

\colorbox{yellow}{\parbox{.7\textwidth}{continue here}}

\[
\binom{n}{b_0,b_1,b_2,\ldots}
\]
ways of choosing which of the \( n \) slots correspond to which values. (Strictly speaking, there are only finitely many nonzero \( b_r \), and we have \( b_0 = n-\sum_{r\ge 1} b_r \).) 

Thus, the contribution of all terms corresponding to a given partition \(\lambda\) is
\[
\binom{n}{b_0,b_1,b_2,\ldots} \; \frac{k!}{\lambda_1! \lambda_2! \cdots} \; \mathsf{m}_{\lambda_1}\mathsf{m}_{\lambda_2}\cdots.
\]
Thus,
\[
\mathbb{E}\left[\left( \sum_{i=1}^n Y_i\right)^k\right]= \sum_{\lambda \vdash k} \binom{n}{b_0,b_1,b_2,\ldots} \; \frac{k!}{\lambda_1! \lambda_2! \cdots} \; \prod_{r\ge 0} (\mathsf{m}_r)^{\,b_r},
\]
where the notation $\lambda \vdash k$ means that $\lambda$ is a partition of $k$.



\subsubsection{Asymptotics of moments}

Now we consider the leading term of the asymptotics 
as $n\to+\infty$.


the dominant contribution comes from those partitions for which the number of indices with \(\lambda_i\ge 3\) is as small as possible. (The terms in which some \( j_i \geq 3 \) have factors \(\mathsf{m}_r\) that are asymptotically negligible, when compared with the contributions coming from partitions all of whose nonzero parts are \(2\).)

Since \(\mathsf{m}_1=0\), the only nonzero contributions come from partitions with no parts equal to \( 1 \). Among these, the largest contribution arises from the partition in which exactly \( k/2 \) of the \( j_i \) equal 2 (with the remainder zero) when \( k \) is even (if \( k \) is odd the moment is asymptotically zero, as required by the symmetry of the limit distribution). More precisely, for even \( k \) the dominating term is

\[
\binom{n}{k/2} \; \frac{k!}{(2!)^{k/2}} \; (\mathsf{m}_2)^{k/2}.
\]

Normalizing by the variance, we compare with the moment of a standard normal, using \(\sigma^2 = \mathsf{m}_2\). Recall that for a standard normal random variable \( Z \),
\[
\mathbb{E}[Z^k]=
\begin{cases}
\frac{k!}{2^{k/2}(k/2)!}, & \text{if } k \text{ is even}, \\
0, & \text{if } k \text{ is odd.}
\end{cases}
\]

When we normalize 
\[
S_n=\frac{Y_1+\cdots+Y_n}{\sqrt{n \,\mathsf{m}_2}},
\]
its \( k \)th moment (for even \( k \)) is given by
\[
\mathbb{E}[S_n^k]=\frac{1}{(n\,\mathsf{m}_2)^{k/2}}\,\mathbb{E}\left[\left(\sum_{i=1}^{n}Y_i\right)^k\right].
\]
Substituting the dominant term we have approximately
\[
\mathbb{E}[S_n^k]\approx \frac{1}{(n\,\mathsf{m}_2)^{k/2}} \; \binom{n}{k/2}\; \frac{k!}{(2!)^{k/2}} \; (\mathsf{m}_2)^{k/2}.
\]
Noting that
\[
\binom{n}{k/2}\sim \frac{n^{k/2}}{(k/2)!}
\]
for large \( n \) and fixed \( k \), the powers of \( n \) and \(\mathsf{m}_2\) cancel, yielding
\[
\mathbb{E}[S_n^k]\sim \frac{k!}{2^{k/2}(k/2)!},
\]
which is exactly the \( k \)th moment of the standard normal distribution. A similar (but simpler) argument shows that the moments for odd \( k \) vanish.

Thus, by the method of moments, the distribution of the normalized sum \( S_n \) converges to the standard normal law as \( n\to\infty \). This completes the moment-based proof of the central limit theorem.

\medskip

\textbf{Summary:} We expanded the \( k \)th moment of \(\sum_{i=1}^n Y_i\) by the multinomial theorem, then identified the dominant contributions coming from partitions with as many parts equal to \( 2 \) as possible (since \(\mathsf{m}_1=0\) and higher moments produce negligible contributions after normalization). After “unordering” the partitions correctly by counting multiplicities, we found that the normalized moments converge to those of the standard Gaussian, which by the method of moments implies the central limit theorem.


































\newpage
\appendix
\setcounter{section}{0}

\section{Problems (due 2025-02-13)}

Each problem is a subsection (like \ref{sub:normal-approximation}),
and is may have several parts.

\subsection{Normal approximation}
\label{sub:normal-approximation}

\begin{enumerate}
	\item In \Cref{fig:uniform_pdfs}, which color is
		the normal curve and which is the sum of three uniform random variables?
	\item Show that the sum of 12 iid uniform random variables on $[-1,1]$
		(without normalization) is approximately standard normal.
	\item Find (numerically is okay)
		the maximum discrepancy between the distribution of the sum of 12 iid uniform random variables on $[-1,1]$ and the standard normal distribution:
		\begin{equation*}
			\sup_{x \in \mathbb{R}} \left| \operatorname{\mathbb{P}}\left(  \sum_{i=1}^{12} U_i \leq x \right) - \operatorname{\mathbb{P}}\left( Z \leq x \right) \right|.
		\end{equation*}
\end{enumerate}






\bibliographystyle{alpha}
\bibliography{bib}


\medskip

\textsc{L. Petrov, University of Virginia, Department of Mathematics, 141 Cabell Drive, Kerchof Hall, P.O. Box 400137, Charlottesville, VA 22904, USA}

E-mail: \texttt{lenia.petrov@gmail.com}


\end{document}

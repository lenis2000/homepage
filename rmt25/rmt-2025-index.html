---
title: Random Matrices, Spring 2025
layout: default
permalink: /rmt25/
---

<h1 class="mb-4 mt-4">MATH 8380: Random Matrices, Spring 2025</h1>

E-mail address for communication:
<a href="mailto:lenia.petrov+rmt2025@gmail.com"
    >lenia.petrov+rmt2025@gmail.com</a
>
<br /><br />

<h2>Abstract</h2>
<p class="mb-2">
    This course will explore a rich zoo of probabilistic models, with a focus on
    random matrices, dimer models, and statistical mechanical models of
    algebraic origin. Geometric structures that naturally arise in these
    contexts will also be discussed. The course content will be flexible and
    tailored to the interests of participants, making it ideal for most graduate
    students in the math department, second year and up.
</p>

<p class="mb-2">
    A key feature of the course will be a "reading course" component, where
    weekly meetings provide focused, interactive learning sessions.
</p>

<p class="mb-2">
    Some topics will repeat from the continuous probability-heavy course
    <a href="https://lpetrov.cc/rmt19/">https://lpetrov.cc/rmt19/</a> but most
    of the content will be new.
</p>

<h2 class="mb-3">Course Meetings and notes</h2>

<h3>2:00-3:15pm, Kerchof 326</h3>

<h4 class="mt-4 mb-3">Regular Lectures on Wednesdays and some Mondays</h4>

Live simulations of some random matrix properties are
available at
<a href="{{site.url}}/simulations/model/random-matrices/">
	this link
</a>.

<br><br>

Lecture notes (problem sets are at the end of each lecture):
<ol>
  <!-- Lecture 1 -->
  <li>
    <b>(Mon) January 13</b>.
    <a href="{{site.url}}/rmt25/rmt25-notes/rmt2025-l01.pdf">
      Moments of random variables and random matrices
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Why study random matrices?
          <ul>
            <li>On the history.</li>
            <li>Classical groups and Lie theory.</li>
            <li>Toolbox.</li>
            <li>Applications.</li>
          </ul>
        </li>
        <li>
          2: Recall Central Limit Theorem
          <ul>
            <li>2.1: Central Limit Theorem and examples</li>
            <li>2.2: Moments of the normal distribution</li>
            <li>
              2.3: Moments of sums of iid random variables
              <ul>
                <li>2.3.1: Computation of moments</li>
                <li>2.3.2: \(n\)-dependent factor</li>
                <li>2.3.3: Combinatorial factor</li>
                <li>2.3.4: Putting it all together</li>
              </ul>
            </li>
            <li>2.4: Convergence in distribution</li>
          </ul>
        </li>
        <li>
          3: Random matrices and semicircle law
          <ul>
            <li>3.1: Where can randomness in a matrix come from?</li>
            <li>3.2: Real Wigner matrices</li>
            <li>3.3: Empirical spectral distribution</li>
            <li>3.4: Expected moments of traces of random matrices</li>
            <li>3.5: Immediate next steps</li>
          </ul>
        </li>
        <li>
          A: Problems
          <ul>
            <li>A.1: Normal approximation</li>
            <li>A.2: Convergence in distribution</li>
            <li>A.3: Moments of sum justification</li>
            <li>A.4: Distribution not determined by moments</li>
            <li>A.5: Uniqueness of the normal distribution</li>
            <li>A.6: Quaternions</li>
            <li>A.7: Ensemble \(UD_\lambda U^\dagger\)</li>
            <li>A.8: Invariance of the GOE</li>
            <li>A.9: Counting \(n\)-powers in the real Wigner matrix</li>
            <li>A.10: Counting trees</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 2 -->
  <li>
    <b>(Wed) January 15</b>.
    <a href="{{site.url}}/rmt25/rmt25-notes/rmt2025-l02.pdf">
      Wigner's semicircle law
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>1: Recap</li>
        <li>
          2: Two computations
          <ul>
            <li>2.1: Moments of the semicircle law</li>
            <li>2.2: Counting trees and Catalan numbers</li>
          </ul>
        </li>
        <li>
          3: Analysis steps in the proof
          <ul>
            <li>3.1: The semicircle distribution is determined by its moments</li>
            <li>3.2: Convergence to the semicircle law
              <ul>
                <li>3.2.1: A concentration bound and the Borel--Cantelli lemma</li>
                <li>3.2.2: Tightness of \(\{\nu_n\}\) and subsequential limits</li>
                <li>3.2.3: Characterizing the limit measure</li>
              </ul>
            </li>
          </ul>
        </li>
        <li>
          4: Proof of Proposition (variance bound): bounding the variance
        </li>
        <li>
          5: Remark: Variants of the semicircle law
        </li>
        <li>
          B: Problems
          <ul>
            <li>B.1: Standard formula</li>
            <li>B.2: Tree profiles</li>
            <li>B.3: Ballot problem</li>
            <li>B.4: Reflection principle</li>
            <li>B.5: Bounding probability in the proof</li>
            <li>B.6: Almost sure convergence and convergence in probability</li>
            <li>B.7: Wigner's semicircle law for complex Wigner matrices</li>
            <li>B.8: Semicircle law without the moment condition</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 3 -->
  <li>
    <b>(Wed) January 22</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l03.pdf">Gaussian and tridiagonal matrices</a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>1: Recap</li>
        <li>
          2: Gaussian ensembles
          <ul>
            <li>2.1: Definitions</li>
            <li>2.2: Joint eigenvalue distribution for GOE</li>
            <li>2.3: Step A. Joint density of matrix entries</li>
            <li>2.4: Step B. Spectral decomposition</li>
            <li>2.5: Step C. Jacobian
              <ul>
                <li>Parametrizing \(\delta Q\).</li>
                <li>Computing \(\delta W\).</li>
                <li>Local structure of the map.</li>
              </ul>
            </li>
            <li>2.6: Step D. Final Form of the density</li>
          </ul>
        </li>
        <li>
          3: Other classical ensembles with explicit eigenvalue densities
          <ul>
            <li>
              3.1: Wishart (Laguerre) ensemble
              <ul>
                <li>3.1.1: Definition via SVD</li>
                <li>3.1.2: Joint density of eigenvalues</li>
              </ul>
            </li>
            <li>
              3.2: Jacobi (MANOVA/CCA) ensemble
              <ul>
                <li>3.2.1: Setup</li>
                <li>3.2.2: Jacobi ensemble</li>
              </ul>
            </li>
            <li>3.3: General Pattern and \(\beta\)-Ensembles</li>
          </ul>
        </li>
        <li>
          4: Tridiagonal form for real symmetric matrices
          <ul>
            <li>Step 1: Zeroing out subdiagonal entries in the first column.</li>
            <li>Step 2: Inductive reduction on the trailing principal submatrix.</li>
            <li>Step 3: Repeat for columns (and rows) 3, 4, etc.</li>
          </ul>
        </li>
        <li>
          5: Tridiagonalization of random matrices
          <ul>
            <li>5.1: Dumitriu–Edelman tridiagonal model for GOE</li>
            <li>5.2: Generalization to \(\beta\)-ensembles</li>
          </ul>
        </li>
        <li>
          C: Problems
          <ul>
            <li>C.1: Invariance of GOE and GUE</li>
            <li>C.2: Preimage size for spectral decomposition</li>
            <li>C.3: Distinct eigenvalues</li>
            <li>C.4: Testing distinctness of eigenvalues via rank-1 perturbations</li>
            <li>C.5: Jacobian for GUE</li>
            <li>C.6: Normalization for GOE</li>
            <li>C.7: Wishart eigenvalue density</li>
            <li>C.8: Householder reflection properties</li>
            <li>C.9: Distribution of the Householder vector in random tridiagonalization</li>
            <li>C.10: Householder reflection for GUE</li>
            <li>C.11: Jacobi ensemble is related to two Wisharts</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 4 -->
  <li>
    <b>(Wed) January 29</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l04.pdf">
      Semicircle law for G$\beta$E via tridiagonalization. Beginning determinantal processes
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>1: Recap
          <ul>
            <li>1.1: Gaussian ensembles</li>
            <li>1.2: Tridiagonalization</li>
          </ul>
        </li>
        <li>
          2: Tridiagonal random matrices
          <ul>
            <li>2.1: Distribution of the tridiagonal form of the GOE</li>
            <li>2.2: Dumitriu--Edelman G\(\beta\)E tridiagonal random matrices</li>
            <li>2.3: The case \(\beta = 2\)</li>
          </ul>
        </li>
        <li>
          3: Wigner semicircle law via tridiagonalization
          <ul>
            <li>3.1: Moments for tridiagonal matrices</li>
            <li>3.2: Asymptotics of chi random variables</li>
            <li>3.3: Completing the proof: global semicircle behavior</li>
          </ul>
        </li>
        <li>
          4: Wigner semicircle law via Stieltjes transform
          <ul>
            <li>4.1: Tridiagonal structure and characteristic polynomials
              <ul>
                <li>4.1.1: Three-term recurrence for the characteristic polynomial</li>
                <li>4.1.2: Spectral connection and eigenvalues</li>
              </ul>
            </li>
            <li>4.2: Stieltjes transform / resolvent</li>
            <li>4.3: Approach via continued fractions</li>
          </ul>
        </li>
        <li>5: Determinantal point processes (discrete)</li>
        <li>
          6: Application of determinantal processes to random matrices at \(\beta = 2\)
          <ul>
            <li>6.1: Local eigenvalue statistics (bulk and edge scaling limits)</li>
            <li>6.2: Correlation functions and densities</li>
            <li>6.3: Poisson process example</li>
          </ul>
        </li>
        <li>
          D: Problems
          <ul>
            <li>D.1: Eigenvalue density of G\(\beta\)E</li>
            <li>D.2: Chi-square mean and variance</li>
            <li>D.3: Edge contributions in the tridiagonal moment computation</li>
            <li>D.4: Hermite polynomials and three-term recurrence</li>
            <li>D.5 (unnumbered title)</li>
            <li>D.6: Gap probabilities</li>
            <li>D.7: Stieltjes transform approach for tridiagonal matrices</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 5 -->
  <li>
    <b>(Wed) February 5</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l05.pdf">
      Determinantal Point Processes and the GUE
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>1: Recap</li>
        <li>
          2: Discrete determinantal point processes
          <ul>
            <li>2.1: Definition and basic properties</li>
          </ul>
        </li>
        <li>
          3: Determinantal structure in the GUE
          <ul>
            <li>3.1: Correlation functions as densities with respect to Lebesgue measure</li>
            <li>
              3.2: The GUE eigenvalues as DPP
              <ul>
                <li>3.2.1: Setup</li>
                <li>3.2.2: Writing the Vandermonde as a determinant</li>
                <li>3.2.3: Orthogonalization by linear operations</li>
                <li>3.2.4: Rewriting the density in determinantal form</li>
              </ul>
            </li>
            <li>3.3: Christoffel--Darboux formula</li>
          </ul>
        </li>
        <li>
          E: Problems
          <ul>
            <li>E.1: Gap Probability for Discrete DPPs</li>
            <li>E.2: Generating Functions for Multiplicative Statistics</li>
            <li>E.3: Variance</li>
            <li>E.4: Formula for the Hermite polynomials</li>
            <li>E.5: Generating function for the Hermite polynomials</li>
            <li>E.6: Projection Property of the GUE Kernel</li>
            <li>E.7: Recurrence Relation for the Hermite Polynomials</li>
            <li>E.8: Differential Equation for the Hermite Polynomials</li>
            <li>E.9: Norm of the Hermite Polynomials</li>
            <li>E.10: Existence of Determinantal Point Processes with a Given Kernel</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 6 -->
  <li>
    <b>(Wed) February 19</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l06.pdf">
      Double contour integral kernel. Steepest descent and semicircle law
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>1: Recap: Determinantal structure of the GUE</li>
        <li>
          2: Double Contour Integral Representation for the GUE Kernel
          <ul>
            <li>2.1: One contour integral representation for Hermite polynomials</li>
            <li>2.2: Another contour integral representation for Hermite polynomials</li>
            <li>2.3: Normalization of Hermite polynomials</li>
            <li>2.4: Double contour integral representation for the GUE kernel</li>
            <li>2.5: Conjugation of the kernel</li>
            <li>2.6: Extensions</li>
          </ul>
        </li>
        <li>
          3: Steepest descent — generalities for single integrals
          <ul>
            <li>3.1: Setup</li>
            <li>3.2: Saddle points and steepest descent paths</li>
            <li>3.3: Local asymptotic evaluation near a saddle point</li>
          </ul>
        </li>
        <li>
          4: Steepest descent for the GUE kernel
          <ul>
            <li>4.1: Scaling</li>
            <li>4.2: Critical points</li>
            <li>4.3: Imaginary critical points: \(\lvert X\rvert < 2\), "bulk"</li>
          </ul>
        </li>
        <li>
          F: Problems
          <ul>
            <li>F.1: Different global positions</li>
            <li>F.2: Sine kernel</li>
            <li>F.3: Discrete sine process</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 7 -->
  <li>
    <b>(Mon) February 24</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l07.pdf">
      Steepest descent and local statistics. Cutting corners
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Steepest descent for the GUE kernel
          <ul>
            <li>1.1: Recap</li>
            <li>1.2: Scaling</li>
            <li>1.3: Critical points</li>
            <li>1.4: Imaginary critical points: \(\lvert X\rvert < 2\), "bulk"</li>
            <li>1.5: Real critical points: \(\lvert X\rvert > 2\), "large deviations"</li>
            <li>1.6: Double critical point: \(\lvert X\rvert=2\), "edge"</li>
            <li>1.7: Airy kernel, Tracy–Widom distribution, and convergence of the maximal eigenvalue</li>
            <li>1.8: Remark: what happens for general \(\beta\)?</li>
          </ul>
        </li>
        <li>2: Cutting corners: setup</li>
        <li>
          3: Corners of Hermitian matrices
          <ul>
            <li>3.1: Principal corners</li>
            <li>3.2: Interlacing</li>
            <li>3.3: Orbital measure</li>
          </ul>
        </li>
        <li>
          4: Polynomial equation and joint distribution
          <ul>
            <li>4.1: Derivation</li>
            <li>4.2: Inductive nature of the transition</li>
            <li>4.3: Case \(\beta = \infty\)</li>
          </ul>
        </li>
        <li>
          G: Problems
          <ul>
            <li>G.1: General bulk case</li>
            <li>G.2: Large deviations</li>
            <li>G.3: Airy kernel</li>
            <li>G.4: Interlacing proof</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 8 -->
  <li>
    <b>(Wed) February 26</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l08.pdf">
      Cutting corners and loop equations
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Cutting corners: polynomial equation and distribution
          <ul>
            <li>1.1: Recap: polynomial equation</li>
            <li>1.2: Extension to general \(\beta\)</li>
            <li>1.3: Distribution of the eigenvalues of the corners</li>
          </ul>
        </li>
        <li>
          2: Loop equations
          <ul>
            <li>2.1: Formulation</li>
            <li>2.2: Proof of Theorem (loop equation) for \(\beta > 2\)</li>
          </ul>
        </li>
        <li>
          3: Applications of loop equations
          <ul>
            <li>3.1: Stieltjes transform equations</li>
            <li>3.2: Asymptotic behavior</li>
            <li>3.3: Example: G\(\beta\)E and the semicircle law</li>
          </ul>
        </li>
        <li>
          H: Problems
          <ul>
            <li>H.1: Cauchy determinant</li>
            <li>H.2: Jacobian from \(n-1\) to \(n\) dependent variables</li>
            <li>H.3: Dirichlet density</li>
            <li>H.4: General beta Gaussian density and cutting corners</li>
            <li>H.5: General \(\beta\) Corners Process Simulation</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 9 -->
  <li>
    <b>(Wed) March 5</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l09.pdf">
      Loop equations and asymptotics to Gaussian Free Field
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Recap
          <ul>
            <li>(Dynamical) loop equations</li>
            <li>Loop equations for \(W=0\)</li>
            <li>The full corners process</li>
            <li>Example: G\(\beta\)E and the semicircle law</li>
          </ul>
        </li>
        <li>
          2: Gaussian Free Field
          <ul>
            <li>2.1: Gaussian correlated vectors and random fields</li>
            <li>2.2: Gaussian fields as random generalized functions</li>
            <li>2.3: Concrete treatment via orthogonal functions</li>
            <li>2.4: Connection to Brownian bridge</li>
            <li>2.5: Covariance structure and Green's function</li>
            <li>2.6: The GFF on the upper half-plane</li>
          </ul>
        </li>
        <li>
          3: Fluctuations
          <ul>
            <li>3.1: Height function and related definitions</li>
            <li>3.2: Main results on Gaussian fluctuations</li>
            <li>3.3: Deformed ensemble</li>
            <li>3.4: Wiener-Hopf like factorization</li>
            <li>3.5: First order asymptotics of \(\mathcal{A}(z)\)</li>
            <li>3.6: Outlook of further steps</li>
          </ul>
        </li>
        <li>
          I: Problems
          <ul>
            <li>I.1: Brownian bridge</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 10 -->
  <li>
    <b>(Mon) March 24</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l10.pdf">
      Dyson Brownian Motion
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Motivations
          <ul>
            <li>1.1: Why introduce time?</li>
            <li>1.2: Simple example: 1×1 case</li>
          </ul>
        </li>
        <li>
          2: Matrix Brownian motion and its eigenvalues
          <ul>
            <li>2.1: Definition</li>
            <li>2.2: Eigenvalues as Markov process</li>
          </ul>
        </li>
        <li>
          3: Dyson Brownian Motion
          <ul>
            <li>3.1: Stochastic differential equations — an informal introduction
              <ul>
                <li>Summary</li>
              </ul>
            </li>
            <li>3.2: Heuristic derivation of the SDE for the Dyson Brownian Motion</li>
          </ul>
        </li>
        <li>
          4: Mapping the G\(\beta\)E densities with the Dyson Brownian Motion
        </li>
        <li>5: Determinantal structure for \(\beta = 2\)</li>
        <li>
          6: Harish-Chandra–Itzykson–Zuber (HCIZ) integral
          <ul>
            <li>6.1: Statement of the HCIZ formula</li>
            <li>6.2: Reduction to the diagonal case</li>
            <li>6.3: Symmetry</li>
            <li>6.4: Conclusion of the argument</li>
          </ul>
        </li>
        <li>
          J: Problems
          <ul>
            <li>J.1: Collisions</li>
            <li>J.2: Estimate on the modulus of continuity</li>
            <li>J.3: Generator for Dyson Brownian Motion</li>
            <li>J.4: Constant in the HCIZ formula</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <!-- Lecture 11 -->
  <li>
    <b>(Wed) March 26</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l11.pdf">
      Asymptotics of Dyson Brownian Motion with an outlier
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Recap
          <ul>
            <li>1.1: Dyson Brownian Motion (DBM)</li>
            <li>1.2: Eigenvalue SDE</li>
            <li>1.3: Preservation of G\(\boldsymbol{\beta}\)E density</li>
            <li>1.4: Harish–Chandra–Itzykson–Zuber (HCIZ) integral</li>
          </ul>
        </li>
        <li>
          2: Optional: proof of HCIZ integral via representation theory
        </li>
        <li>
          3: Determinantal structure for \(\beta = 2\)
          <ul>
            <li>3.1: Transition density</li>
            <li>3.2: Determinantal correlations</li>
            <li>3.3: On the proof of determinantal structure</li>
          </ul>
        </li>
        <li>
          4: Asymptotic analysis: signal plus noise
          <ul>
            <li>4.1: Setup</li>
            <li>4.2: Outline of the steepest descent approach</li>
            <li>4.3: Asymptotics</li>
            <li>4.4: Airy kernel</li>
            <li>4.5: BBP transition and the deformed Airy kernel</li>
            <li>4.6: Gaussian regime</li>
            <li>4.7: Matching Fredholm determinant to the Gaussian distribution</li>
          </ul>
        </li>
        <li>
          K: Problems
          <ul>
            <li>K.1: Biorthogonal ensembles</li>
            <li>K.2: Scaling of the kernel</li>
            <li>K.3: Gaussian regime and integration contours</li>
            <li>K.4: Gaussian kernel</li>
            <li>K.5: GUE kernel</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

  <li>
    <b>(Thu) March 27</b>.
    <a href="/rmt25/rmt25-notes/rmt2025-l12.pdf">
        Random Growth Models
    </a>
    <details>
      <summary>Table of Contents</summary>
      <ul>
        <li>
          1: Recap
          <ul>
            <li>1.1: Dyson Brownian Motion with Determinantal Structure</li>
            <li>1.2: The BBP Phase Transition</li>
            <li>1.3: Remark: Corners process with outliers</li>
            <li>1.4: Goal today</li>
          </ul>
        </li>
        <li>
          2: A window into universality: Airy line ensemble
        </li>
        <li>
          3: KPZ universality class: Scaling and fluctuations
          <ul>
            <li>3.1: Universality of random growth</li>
            <li>3.2: KPZ equation</li>
            <li>3.3: First discoveries</li>
            <li>3.4: Effect of initial conditions</li>
            <li>3.5: Remark: Gaussian Free Field in KPZ universality</li>
          </ul>
        </li>
        <li>
          4: Polynuclear Growth and Last Passage Percolation
          <ul>
            <li>4.1: Definition and single-layer PNG</li>
            <li>4.2: Multiline PNG</li>
            <li>4.3: KPZ mechanisms in the PNG growth</li>
            <li>4.4: Last Passage Percolation (LPP)</li>
            <li>4.5: Topics to continue</li>
          </ul>
        </li>
        <li>
          L: Problems
          <ul>
            <li>L.1: PNG ordering</li>
            <li>L.2: PNG and last passage percolation</li>
          </ul>
        </li>
      </ul>
    </details>
  </li>

<li>
  <b>(Wed) April 9</b>.
  <a href="/rmt25/rmt25-notes/rmt2025-l13.pdf">Title TBD</a>
  <details>
    <summary>Table of Contents</summary>
    <ul>
      <li>Content to be added</li>
    </ul>
  </details>
</li>
<li>
  <b>(Wed) April 16</b>.
  <a href="/rmt25/rmt25-notes/rmt2025-l14.pdf">Title TBD</a>
  <details>
    <summary>Table of Contents</summary>
    <ul>
      <li>Content to be added</li>
    </ul>
  </details>
</li>
<li>
  <b>(Wed) April 23</b>.
  <a href="/rmt25/rmt25-notes/rmt2025-l15.pdf">Random matrices and topology</a>
  <details>
    <summary>Table of Contents</summary>
    <ul>
      <li>Content to be added</li>
    </ul>
  </details>
</li>
</ol>

<ul>
    <li><a href="/rmt25/rmt25-notes/rmt2025-lectures-combined.pdf">Combined lecture notes in a single PDF</a> (updated with some delay)</li>
</ul>



<h4>Student Presentations on Mondays in April</h4>
<ol>
    <li><b>(Mon) April 7</b> (2:05-3:15pm, Kerchof 326)
        <ul>
            <li>Suren Kyurumyan</li>
            <li>Declan Stacy</li>
        </ul>
    </li>
    <li><b>(Mon) April 14</b> (2:05-3:15pm, Kerchof 326)
        <ul>
            <li>Annika Kelly</li>
            <li>Jun Park</li>
        </ul>
    </li>
    <li><b>(Mon) April 21</b> (2:05-3:15pm, Kerchof 326)
        <ul>
            <li>Mikhail Tikhonov</li>
            <li>Yizhen Li</li>
        </ul>
    </li>
    <li><b>(Mon) April 28</b> (2:05-3:15pm, Kerchof 326)
        <ul>
            <li>Connor MacMahon</li>
            <li>Ryland Wilson</li>
        </ul>
    </li>
</ol>

<h4>Weekly individual meetings</h4>

<p>
    The course spans 12 full weeks, excluding the first week, final week, and the week following Spring break when I will be traveling. Students are required to meet with me at least 8 times during these 12 weeks, with each meeting lasting approximately 45 minutes. (In-person is strongly preferred, but a zoom option is available.)
    These meetings are an integral part of the reading course, during which we can discuss lectures, homework problem solutions, presentation, explore research topics, or dig into any other relevant topics related to random matrices.
</p>

<p class="mb-2" />
During the first week of class, we will establish a regular weekly meeting time for each student. While some flexibility is possible, the goal is to maintain a consistent weekly schedule throughout the semester.</p>

<!-- <h2 class="mt-4">Ideas for presentations or projects</h2> -->
<!--  -->
<!-- <ol> -->
<!--     <li>Bisi, E., and Cunden, F.D. <em>"λ-shaped random matrices, λ-plane trees, and λ-Dyck paths"</em>. -->
<!--     <a href="https://arxiv.org/abs/2403.07418">arXiv:2403.07418 [math.PR]</a>. -->
<!--  -->
<!--     Investigates spectral properties of random matrices shaped by self-conjugate Young diagrams, introducing combinatorial structures such as λ-plane trees and λ-Dyck paths, with connections to free probability theory. -->
<!--     </li> -->
<!--     <li> -->
<!--     Reference hunting: Check (and add) references to theorems and statements in the lecture notes -->
<!--     </li> -->
<!-- </ol> -->


<h2 class="mt-4">Books</h2>
<ol>
    <li>Mehta, M.L. <em>"Random Matrices"</em>. A first textbook that approaches the subject through the lens of theoretical physics.</li>
    <li>Anderson, G.W., Guionnet, A. and Zeitouni, O. <em>"An Introduction to Random Matrices"</em>. A comprehensive treatment that emphasizes probabilistic methods and stochastic analysis.</li>
    <li>Pastur, L. and Shcherbina, M. <em>"Eigenvalue Distribution of Large Random Matrices"</em>. An analytical perspective on random matrix theory, focusing on mathematical techniques.</li>
    <li>Muirhead, R.J. <em>"Aspects of Multivariate Statistical Theory"</em>. A statistical approach to random matrices.</li>
    <li>Vershynin, R. <em>"High-dimensional probability: An introduction with applications in data science"</em>. A contemporary work bridging random matrix theory with modern data science applications.</li>
    <li>Baik, J., Deift, P., and Suidan, T. <em>"Combinatorics and Random Matrix Theory"</em>. Explores the connections between random matrices and combinatorial probability, particularly in asymptotic problems.</li>
    <li>Forrester, P.J. <em>"Log-Gases and Random Matrices"</em>. A comprehensive reference work containing extensive collections of explicit formulas and results.</li>
    <li>Akemann, G., Baik, J., and Di Francesco, P. (editors). <em>"The Oxford Handbook of Random Matrix Theory"</em>. A curated collection featuring diverse perspectives from experts across the field's many applications.</li>
    <li>Tao, T. <em>"Topics in Random Matrix Theory"</em>. A pedagogical approach derived from graduate-level teaching and academic blog content.</li>
    <li>Potters, M., and Bouchaud, J.P. <em>"A First Course in Random Matrix Theory for Physicists, Engineers and Data Scientists"</em>. An accessible introduction prioritizing intuition and applications over rigorous proofs.</li>
    <li>Bai, Z.D., and Silverstein, J.W. <em>"Spectral Analysis of Large Dimensional Random Matrices"</em>. Focuses on the spectral properties of high-dimensional random matrices.</li>
</ol>

<h2 class="mt-4">Lecture notes by various authors</h2>
<ol>
<li><a href="https://bcourses.berkeley.edu/courses/1522038">V. Gorin</a></li>
<li><a href="https://people.math.wisc.edu/~valko/courses/833/2009f/833.html">B. Valkó</a></li>
<li><a href="https://terrytao.wordpress.com/category/teaching/254a-random-matrices/">T. Tao</a></li>
<li><a href="https://math.berkeley.edu/%7Erezakhan/randommatrix.pdf">F. Rezakhanlou</a></li>
<li><a href="https://math.iisc.ac.in/%7Emanju/RMT17/RMT_2017.pdf">M. Krishnapur</a></li>
</ol>

Additional references are included in lecture notes.
<br>




<details class="mt-4">
    <summary>
        <h2>Assessment <span>&#9660;</span></h2>
    </summary>
    <p>The course grade is based roughly equally on three components:</p>

    <h4>1. Homework Problems</h4>
    <ul>
        <li>Problems are included and mentioned in lectures, posted separately as weekly problem sets</li>
        <li>Each problem has a deadline of a month or until the course ends. Late assignments are not accepted. If you have special needs, emergency, or unavoidable conflicts, please let me know as soon as possible, so we can arrange a workaround.</li>
        <li>
            Problems range from easy to very difficult, not all problems need to
            be solved
        </li>
        <li>Submit problems by email, or communicate orally during meetings</li>
        <li>It is expected that you need to solve about 2 problems in each problem set</li>
    </ul>

    <h4>2. Presentation</h4>
    <ul>
        <li>
            Each student gives one presentation (30 min) during April Monday sessions
        </li>
    </ul>

    <h4>3. Weekly Meetings</h4>
    <ul>
        <li>Participation in 8 individual meetings required, see "Course Meetings" section above</li>
    </ul>
</details>


<details class="mt-4">
    <summary>
        <h2>Policies <span>&#9660;</span></h2>
    </summary>

    <h4 class="mt-3">Approved accommodations</h4>

    All students with special needs requiring accommodations should present the
    appropriate paperwork from the Student Disability Access Center (SDAC). It is
    the student's responsibility to present this paperwork in a timely fashion and
    follow up with the instructor about the accommodations being offered.
    Accommodations for midterms or final exams (e.g., extended time) should be arranged at
    least 5 days before an exam.

    <h4 class="mt-3">Honor code</h4>

    The University of Virginia Honor Code applies to this
    class and is taken seriously.
    Any honor code violations
    will be referred to the
    Honor Committee.

    <h4 class="mt-3">Collaboration on homework assignments</h4>

    Group work on homework problems is allowed and strongly encouraged. Discussions are in general very helpful and inspiring. However, before talking to others, get well started on the problems, and contribute your fair share to the process. When completing the written homework assignments, everyone must write up his or her own solutions in their own words, and cite any reference (other than the textbook and class notes) that you use. Quotations and citations are part of the Honor Code for both UVA and the whole academic community.
</details>
